name: XGBoost pipeline
metadata:
  annotations:
    sdk: https://cloud-pipelines.github.io/pipeline-editor/
implementation:
  graph:
    tasks:
      dataset:
        componentRef:
          url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/57f780b15922061e59833541b71f3d099e710177/components/datasets/Chicago_Taxi_Trips/quick_start_version/component.yaml
          digest: 42030acab16b71bbc3ad018563b5aedb94d373e72f7cb2b322bc27dbbee75e1b
          text: |
            name: Chicago Taxi Trips dataset
            description: |
              City of Chicago Taxi Trips dataset: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew

              The input parameters configure the SQL query to the database.
              The dataset is pretty big, so limit the number of results using the `Limit` or `Where` parameters.
              Read [Socrata dev](https://dev.socrata.com/docs/queries/) for the advanced query syntax
            metadata:
              annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
                canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/datasets/Chicago_Taxi_Trips/quick_start_version/component.yaml'
            inputs:
            - {name: Where, type: String, default: 'trip_start_timestamp>="1900-01-01" AND trip_start_timestamp<"2100-01-01"'}
            - {name: Limit, type: Integer, default: '1000', description: 'Number of rows to return. The rows are randomly sampled.'}
            - {name: Select, type: String, default: 'tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total'}
            - {name: Format, type: String, default: 'csv', description: 'Output data format. Supports csv,tsv,cml,rdf,json'}
            outputs:
            - {name: Table, description: 'Result type depends on format. CSV and TSV have header.'}
            implementation:
              container:
                # image: curlimages/curl  # Sets a non-root user which cannot write to mounted volumes. See https://github.com/curl/curl-docker/issues/22
                image: alpine/curl:8.14.1
                command:
                - sh
                - -c
                - |
                  set -e -x -o pipefail
                  output_path="$0"
                  select="$1"
                  where="$2"
                  limit="$3"
                  format="$4"
                  mkdir -p "$(dirname "$output_path")"
                  curl --get 'https://data.cityofchicago.org/resource/wrvz-psew.'"${format}" \
                      --data-urlencode '$limit='"${limit}" \
                      --data-urlencode '$where='"${where}" \
                      --data-urlencode '$select='"${select}" \
                      | sed -E 's/"([^",\]*)"/\1/g' > "$output_path"  # Removing unneeded quotes around all numbers
                - {outputPath: Table}
                - {inputValue: Select}
                - {inputValue: Where}
                - {inputValue: Limit}
                - {inputValue: Format}
        annotations:
          editor.position: '{"x":-100,"y":150}'
        arguments:
          Select: tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total
          Where: trip_start_timestamp >= "2019-01-01" AND trip_start_timestamp < "2019-02-01"
      train:
        componentRef:
          url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Train/component.yaml
          digest: 5b8bec6716337d7bd8ecafd6dd46bc44527783bc05950cfe03a60206b43c7654
          text: |
            name: Train XGBoost model on CSV
            description: Trains an XGBoost model.
            metadata:
              annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train/component.yaml'}
            inputs:
            - {name: training_data, type: CSV, description: Training data in CSV format.}
            - {name: label_column_name, type: String, description: Name of the column containing
                the label data.}
            - {name: starting_model, type: XGBoostModel, description: Existing trained model to
                start from (in the binary XGBoost format)., optional: true}
            - {name: num_iterations, type: Integer, description: Number of boosting iterations.,
              default: '10', optional: true}
            - name: objective
              type: String
              description: |-
                The learning task and the corresponding learning objective.
                See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                The most common values are:
                "reg:squarederror" - Regression with squared loss (default).
                "reg:logistic" - Logistic regression.
                "binary:logistic" - Logistic regression for binary classification, output probability.
                "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
              default: reg:squarederror
              optional: true
            - {name: booster, type: String, description: 'The booster to use. Can be `gbtree`,
                `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear`
                uses linear functions.', default: gbtree, optional: true}
            - {name: learning_rate, type: Float, description: 'Step size shrinkage used in update
                to prevents overfitting. Range: [0,1].', default: '0.3', optional: true}
            - name: min_split_loss
              type: Float
              description: |-
                Minimum loss reduction required to make a further partition on a leaf node of the tree.
                The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
              default: '0'
              optional: true
            - name: max_depth
              type: Integer
              description: |-
                Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
                0 indicates no limit on depth. Range: [0,Inf].
              default: '6'
              optional: true
            - {name: booster_params, type: JsonObject, description: 'Parameters for the booster.
                See https://xgboost.readthedocs.io/en/latest/parameter.html', optional: true}
            outputs:
            - {name: model, type: XGBoostModel, description: Trained model in the binary XGBoost
                format.}
            - {name: model_config, type: XGBoostModelConfig, description: The internal parameter
                configuration of Booster as a JSON string.}
            implementation:
              container:
                image: python:3.10
                command:
                - sh
                - -c
                - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                  'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
                  -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
                  --user) && "$0" "$@"
                - sh
                - -ec
                - |
                  program_path=$(mktemp)
                  printf "%s" "$0" > "$program_path"
                  python3 -u "$program_path" "$@"
                - |
                  def _make_parent_dirs_and_return_path(file_path: str):
                      import os
                      os.makedirs(os.path.dirname(file_path), exist_ok=True)
                      return file_path

                  def train_XGBoost_model_on_CSV(
                      training_data_path,
                      model_path,
                      model_config_path,
                      label_column_name,
                      starting_model_path = None,
                      num_iterations = 10,
                      # Booster parameters
                      objective = "reg:squarederror",
                      booster = "gbtree",
                      learning_rate = 0.3,
                      min_split_loss = 0,
                      max_depth = 6,
                      booster_params = None,
                  ):
                      """Trains an XGBoost model.

                      Args:
                          training_data_path: Training data in CSV format.
                          model_path: Trained model in the binary XGBoost format.
                          model_config_path: The internal parameter configuration of Booster as a JSON string.
                          starting_model_path: Existing trained model to start from (in the binary XGBoost format).
                          label_column_name: Name of the column containing the label data.
                          num_iterations: Number of boosting iterations.
                          booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                          objective: The learning task and the corresponding learning objective.
                              See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                              The most common values are:
                              "reg:squarederror" - Regression with squared loss (default).
                              "reg:logistic" - Logistic regression.
                              "binary:logistic" - Logistic regression for binary classification, output probability.
                              "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                              "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                              "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized
                          booster: The booster to use. Can be `gbtree`, `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear` uses linear functions.
                          learning_rate: Step size shrinkage used in update to prevents overfitting. Range: [0,1].
                          min_split_loss: Minimum loss reduction required to make a further partition on a leaf node of the tree.
                              The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].
                          max_depth: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.
                              0 indicates no limit on depth. Range: [0,Inf].

                      Annotations:
                          author: Alexey Volkov <alexey.volkov@ark-kun.com>
                      """
                      import pandas
                      import xgboost

                      df = pandas.read_csv(
                          training_data_path,
                      ).convert_dtypes()
                      print("Training data information:")
                      df.info(verbose=True)
                      # Converting column types that XGBoost does not support
                      for column_name, dtype in df.dtypes.items():
                          if dtype in ["string", "object"]:
                              print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                              df[column_name] = df[column_name].astype("category")
                              print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                          # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                          if pandas.api.types.is_float_dtype(dtype):
                              # Converting from "Float64" to "float64"
                              df[column_name] = df[column_name].astype(dtype.name.lower())
                      print()
                      print("Final training data information:")
                      df.info(verbose=True)

                      training_data = xgboost.DMatrix(
                          data=df.drop(columns=[label_column_name]),
                          label=df[[label_column_name]],
                          enable_categorical=True,
                      )

                      booster_params = booster_params or {}
                      booster_params.setdefault("objective", objective)
                      booster_params.setdefault("booster", booster)
                      booster_params.setdefault("learning_rate", learning_rate)
                      booster_params.setdefault("min_split_loss", min_split_loss)
                      booster_params.setdefault("max_depth", max_depth)

                      starting_model = None
                      if starting_model_path:
                          starting_model = xgboost.Booster(model_file=starting_model_path)

                      print()
                      print("Training the model:")
                      model = xgboost.train(
                          params=booster_params,
                          dtrain=training_data,
                          num_boost_round=num_iterations,
                          xgb_model=starting_model,
                          evals=[(training_data, "training_data")],
                      )

                      # Saving the model in binary format
                      model.save_model(model_path)

                      model_config_str = model.save_config()
                      with open(model_config_path, "w") as model_config_file:
                          model_config_file.write(model_config_str)

                  import json
                  import argparse
                  _parser = argparse.ArgumentParser(prog='Train XGBoost model on CSV', description='Trains an XGBoost model.')
                  _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
                  _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=True, default=argparse.SUPPRESS)
                  _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
                  _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
                  _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
                  _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
                  _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
                  _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
                  _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
                  _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
                  _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                  _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                  _parsed_args = vars(_parser.parse_args())

                  _outputs = train_XGBoost_model_on_CSV(**_parsed_args)
                args:
                - --training-data
                - {inputPath: training_data}
                - --label-column-name
                - {inputValue: label_column_name}
                - if:
                    cond: {isPresent: starting_model}
                    then:
                    - --starting-model
                    - {inputPath: starting_model}
                - if:
                    cond: {isPresent: num_iterations}
                    then:
                    - --num-iterations
                    - {inputValue: num_iterations}
                - if:
                    cond: {isPresent: objective}
                    then:
                    - --objective
                    - {inputValue: objective}
                - if:
                    cond: {isPresent: booster}
                    then:
                    - --booster
                    - {inputValue: booster}
                - if:
                    cond: {isPresent: learning_rate}
                    then:
                    - --learning-rate
                    - {inputValue: learning_rate}
                - if:
                    cond: {isPresent: min_split_loss}
                    then:
                    - --min-split-loss
                    - {inputValue: min_split_loss}
                - if:
                    cond: {isPresent: max_depth}
                    then:
                    - --max-depth
                    - {inputValue: max_depth}
                - if:
                    cond: {isPresent: booster_params}
                    then:
                    - --booster-params
                    - {inputValue: booster_params}
                - --model
                - {outputPath: model}
                - --model-config
                - {outputPath: model_config}
        annotations:
          editor.position: '{"x":720,"y":600}'
        arguments:
          label_column_name: tips
          training_data:
            taskOutput:
              taskId: Fill all missing values using Pandas on CSV data
              outputName: transformed_table
      predict:
        componentRef:
          url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Predict/component.yaml
          digest: 6fd7196d2061e6f49ec98459c90f4b1e4e63bca21170c70b23f7679921ce01d7
          text: |
            name: Xgboost predict on CSV
            description: Makes predictions using a trained XGBoost model.
            metadata:
              annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Predict/component.yaml'}
            inputs:
            - {name: data, type: CSV, description: Feature data in Apache Parquet format.}
            - {name: model, type: XGBoostModel, description: Trained model in binary XGBoost format.}
            - {name: label_column_name, type: String, description: Optional. Name of the column
                containing the label data that is excluded during the prediction., optional: true}
            outputs:
            - {name: predictions, description: Model predictions.}
            implementation:
              container:
                image: python:3.10
                command:
                - sh
                - -c
                - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                  'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
                  -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'
                  --user) && "$0" "$@"
                - sh
                - -ec
                - |
                  program_path=$(mktemp)
                  printf "%s" "$0" > "$program_path"
                  python3 -u "$program_path" "$@"
                - |
                  def _make_parent_dirs_and_return_path(file_path: str):
                      import os
                      os.makedirs(os.path.dirname(file_path), exist_ok=True)
                      return file_path

                  def xgboost_predict_on_CSV(
                      data_path,
                      model_path,
                      predictions_path,
                      label_column_name = None,
                  ):
                      """Makes predictions using a trained XGBoost model.

                      Args:
                          data_path: Feature data in Apache Parquet format.
                          model_path: Trained model in binary XGBoost format.
                          predictions_path: Model predictions.
                          label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.

                      Annotations:
                          author: Alexey Volkov <alexey.volkov@ark-kun.com>
                      """
                      from pathlib import Path

                      import numpy
                      import pandas
                      import xgboost

                      df = pandas.read_csv(
                          data_path,
                      ).convert_dtypes()
                      print("Evaluation data information:")
                      df.info(verbose=True)
                      # Converting column types that XGBoost does not support
                      for column_name, dtype in df.dtypes.items():
                          if dtype in ["string", "object"]:
                              print(f"Treating the {dtype.name} column '{column_name}' as categorical.")
                              df[column_name] = df[column_name].astype("category")
                              print(f"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.")
                          # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213
                          if pandas.api.types.is_float_dtype(dtype):
                              # Converting from "Float64" to "float64"
                              df[column_name] = df[column_name].astype(dtype.name.lower())
                      print("Final evaluation data information:")
                      df.info(verbose=True)

                      if label_column_name is not None:
                          df = df.drop(columns=[label_column_name])

                      testing_data = xgboost.DMatrix(
                          data=df,
                          enable_categorical=True,
                      )

                      model = xgboost.Booster(model_file=model_path)

                      predictions = model.predict(testing_data)

                      Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
                      numpy.savetxt(predictions_path, predictions)

                  import argparse
                  _parser = argparse.ArgumentParser(prog='Xgboost predict on CSV', description='Makes predictions using a trained XGBoost model.')
                  _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
                  _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
                  _parser.add_argument("--label-column-name", dest="label_column_name", type=str, required=False, default=argparse.SUPPRESS)
                  _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                  _parsed_args = vars(_parser.parse_args())

                  _outputs = xgboost_predict_on_CSV(**_parsed_args)
                args:
                - --data
                - {inputPath: data}
                - --model
                - {inputPath: model}
                - if:
                    cond: {isPresent: label_column_name}
                    then:
                    - --label-column-name
                    - {inputValue: label_column_name}
                - --predictions
                - {outputPath: predictions}
        annotations:
          editor.position: '{"x":1060,"y":200}'
        arguments:
          model:
            taskOutput:
              taskId: train
              outputName: model
          label_column_name: "tips"
          data:
            taskOutput:
              taskId: Fill all missing values using Pandas on CSV data
              outputName: transformed_table
      Fill all missing values using Pandas on CSV data:
        componentRef:
          url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml
          digest: c873de1f3a95bb7fb17efbbc5a72c4a13a1ef581dc162054707441ddce2c06d2
          text: |
            name: Fill all missing values using Pandas on CSV data
            description: Fills the missing column items with the specified replacement value.
            metadata:
              annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml'}
            inputs:
            - {name: table, type: CSV, description: Input data table.}
            - {name: replacement_value, type: String, description: The value to use when replacing
                the missing items., default: '0', optional: true}
            - {name: column_names, type: JsonArray, description: Names of the columns where to
                perform the replacement., optional: true}
            outputs:
            - {name: transformed_table, type: CSV, description: Transformed data table where missing
                values are filed.}
            implementation:
              container:
                image: python:3.9
                command:
                - sh
                - -c
                - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
                  'pandas==1.4.1' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
                  --no-warn-script-location 'pandas==1.4.1' 'numpy<2' --user) && "$0" "$@"
                - sh
                - -ec
                - |
                  program_path=$(mktemp)
                  printf "%s" "$0" > "$program_path"
                  python3 -u "$program_path" "$@"
                - |
                  def _make_parent_dirs_and_return_path(file_path: str):
                      import os
                      os.makedirs(os.path.dirname(file_path), exist_ok=True)
                      return file_path

                  def fill_all_missing_values_using_Pandas_on_CSV_data(
                      table_path,
                      transformed_table_path,
                      replacement_value = "0",
                      column_names = None,
                  ):
                      """Fills the missing column items with the specified replacement value.

                      Args:
                          table_path: Input data table.
                          transformed_table_path: Transformed data table where missing values are filed.
                          replacement_value: The value to use when replacing the missing items.
                          column_names: Names of the columns where to perform the replacement.
                      """
                      import pandas

                      df = pandas.read_csv(
                          table_path,
                          dtype="string",
                      )

                      for column_name in column_names or df.columns:
                          df[column_name] = df[column_name].fillna(value=replacement_value)

                      df.to_csv(
                          transformed_table_path, index=False,
                      )

                  import json
                  import argparse
                  _parser = argparse.ArgumentParser(prog='Fill all missing values using Pandas on CSV data', description='Fills the missing column items with the specified replacement value.')
                  _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
                  _parser.add_argument("--replacement-value", dest="replacement_value", type=str, required=False, default=argparse.SUPPRESS)
                  _parser.add_argument("--column-names", dest="column_names", type=json.loads, required=False, default=argparse.SUPPRESS)
                  _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                  _parsed_args = vars(_parser.parse_args())

                  _outputs = fill_all_missing_values_using_Pandas_on_CSV_data(**_parsed_args)
                args:
                - --table
                - {inputPath: table}
                - if:
                    cond: {isPresent: replacement_value}
                    then:
                    - --replacement-value
                    - {inputValue: replacement_value}
                - if:
                    cond: {isPresent: column_names}
                    then:
                    - --column-names
                    - {inputValue: column_names}
                - --transformed-table
                - {outputPath: transformed_table}
        annotations:
          editor.position: '{"x":350,"y":150}'
        arguments:
          replacement_value: "0"
          table:
            taskOutput:
              taskId: dataset
              outputName: Table
