{
  "name": "Common components",
  "root_folder": {
    "name": "Common components",
    "folders": [
      {
        "components": [
          {
            "text": "name: Chicago Taxi Trips dataset\ndescription: |\n  City of Chicago Taxi Trips dataset: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew\n\n  The input parameters configure the SQL query to the database.\n  The dataset is pretty big, so limit the number of results using the `Limit` or `Where` parameters.\n  Read [Socrata dev](https://dev.socrata.com/docs/queries/) for the advanced query syntax\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: Where, type: String, default: 'trip_start_timestamp>=\"1900-01-01\" AND trip_start_timestamp<\"2100-01-01\"'}\n- {name: Limit, type: Integer, default: '1000', description: 'Number of rows to return. The rows are randomly sampled.'}\n- {name: Select, type: String, default: 'tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total'}\n- {name: Format, type: String, default: 'csv', description: 'Output data format. Suports csv,tsv,cml,rdf,json'}\noutputs:\n- {name: Table, description: 'Result type depends on format. CSV and TSV have header.'}\nimplementation:\n  container:\n    # image: curlimages/curl  # Sets a non-root user which cannot write to mounted volumes. See https://github.com/curl/curl-docker/issues/22\n    image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x -o pipefail\n      output_path=\"$0\"\n      select=\"$1\"\n      where=\"$2\"\n      limit=\"$3\"\n      format=\"$4\"\n      mkdir -p \"$(dirname \"$output_path\")\"\n      curl --get 'https://data.cityofchicago.org/resource/wrvz-psew.'\"${format}\" \\\n          --data-urlencode '$limit='\"${limit}\" \\\n          --data-urlencode '$where='\"${where}\" \\\n          --data-urlencode '$select='\"${select}\" \\\n          | tr -d '\"' > \"$output_path\"  # Removing unneeded quotes around all numbers\n    - {outputPath: Table}\n    - {inputValue: Select}\n    - {inputValue: Where}\n    - {inputValue: Limit}\n    - {inputValue: Format}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipelines/2463ecda532517462590d75e6e14a8af6b55869a/components/datasets/Chicago_Taxi_Trips/component.yaml"
          },
          {
            "text": "name: Train XGBoost model on CSV\ndescription: Trains an XGBoost model.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train/component.yaml'}\ninputs:\n- {name: training_data, type: CSV, description: Training data in CSV format.}\n- {name: label_column_name, type: String, description: Name of the column containing\n    the label data.}\n- {name: starting_model, type: XGBoostModel, description: Existing trained model to\n    start from (in the binary XGBoost format)., optional: true}\n- {name: num_iterations, type: Integer, description: Number of boosting iterations.,\n  default: '10', optional: true}\n- name: objective\n  type: String\n  description: |-\n    The learning task and the corresponding learning objective.\n    See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n    The most common values are:\n    \"reg:squarederror\" - Regression with squared loss (default).\n    \"reg:logistic\" - Logistic regression.\n    \"binary:logistic\" - Logistic regression for binary classification, output probability.\n    \"binary:logitraw\" - Logistic regression for binary classification, output score before logistic transformation\n    \"rank:pairwise\" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n    \"rank:ndcg\" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n  default: reg:squarederror\n  optional: true\n- {name: booster, type: String, description: 'The booster to use. Can be `gbtree`,\n    `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear`\n    uses linear functions.', default: gbtree, optional: true}\n- {name: learning_rate, type: Float, description: 'Step size shrinkage used in update\n    to prevents overfitting. Range: [0,1].', default: '0.3', optional: true}\n- name: min_split_loss\n  type: Float\n  description: |-\n    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n    The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].\n  default: '0'\n  optional: true\n- name: max_depth\n  type: Integer\n  description: |-\n    Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n    0 indicates no limit on depth. Range: [0,Inf].\n  default: '6'\n  optional: true\n- {name: booster_params, type: JsonObject, description: 'Parameters for the booster.\n    See https://xgboost.readthedocs.io/en/latest/parameter.html', optional: true}\noutputs:\n- {name: model, type: XGBoostModel, description: Trained model in the binary XGBoost\n    format.}\n- {name: model_config, type: XGBoostModelConfig, description: The internal parameter\n    configuration of Booster as a JSON string.}\nimplementation:\n  container:\n    image: python:3.10\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def train_XGBoost_model_on_CSV(\n          training_data_path,\n          model_path,\n          model_config_path,\n          label_column_name,\n          starting_model_path = None,\n          num_iterations = 10,\n          # Booster parameters\n          objective = \"reg:squarederror\",\n          booster = \"gbtree\",\n          learning_rate = 0.3,\n          min_split_loss = 0,\n          max_depth = 6,\n          booster_params = None,\n      ):\n          \"\"\"Trains an XGBoost model.\n\n          Args:\n              training_data_path: Training data in CSV format.\n              model_path: Trained model in the binary XGBoost format.\n              model_config_path: The internal parameter configuration of Booster as a JSON string.\n              starting_model_path: Existing trained model to start from (in the binary XGBoost format).\n              label_column_name: Name of the column containing the label data.\n              num_iterations: Number of boosting iterations.\n              booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html\n              objective: The learning task and the corresponding learning objective.\n                  See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n                  The most common values are:\n                  \"reg:squarederror\" - Regression with squared loss (default).\n                  \"reg:logistic\" - Logistic regression.\n                  \"binary:logistic\" - Logistic regression for binary classification, output probability.\n                  \"binary:logitraw\" - Logistic regression for binary classification, output score before logistic transformation\n                  \"rank:pairwise\" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n                  \"rank:ndcg\" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n              booster: The booster to use. Can be `gbtree`, `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear` uses linear functions.\n              learning_rate: Step size shrinkage used in update to prevents overfitting. Range: [0,1].\n              min_split_loss: Minimum loss reduction required to make a further partition on a leaf node of the tree.\n                  The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].\n              max_depth: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n                  0 indicates no limit on depth. Range: [0,Inf].\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          \"\"\"\n          import pandas\n          import xgboost\n\n          df = pandas.read_csv(\n              training_data_path,\n          ).convert_dtypes()\n          print(\"Training data information:\")\n          df.info(verbose=True)\n          # Converting column types that XGBoost does not support\n          for column_name, dtype in df.dtypes.items():\n              if dtype in [\"string\", \"object\"]:\n                  print(f\"Treating the {dtype.name} column '{column_name}' as categorical.\")\n                  df[column_name] = df[column_name].astype(\"category\")\n                  print(f\"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.\")\n              # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213\n              if pandas.api.types.is_float_dtype(dtype):\n                  # Converting from \"Float64\" to \"float64\"\n                  df[column_name] = df[column_name].astype(dtype.name.lower())\n          print()\n          print(\"Final training data information:\")\n          df.info(verbose=True)\n\n          training_data = xgboost.DMatrix(\n              data=df.drop(columns=[label_column_name]),\n              label=df[[label_column_name]],\n              enable_categorical=True,\n          )\n\n          booster_params = booster_params or {}\n          booster_params.setdefault(\"objective\", objective)\n          booster_params.setdefault(\"booster\", booster)\n          booster_params.setdefault(\"learning_rate\", learning_rate)\n          booster_params.setdefault(\"min_split_loss\", min_split_loss)\n          booster_params.setdefault(\"max_depth\", max_depth)\n\n          starting_model = None\n          if starting_model_path:\n              starting_model = xgboost.Booster(model_file=starting_model_path)\n\n          print()\n          print(\"Training the model:\")\n          model = xgboost.train(\n              params=booster_params,\n              dtrain=training_data,\n              num_boost_round=num_iterations,\n              xgb_model=starting_model,\n              evals=[(training_data, \"training_data\")],\n          )\n\n          # Saving the model in binary format\n          model.save_model(model_path)\n\n          model_config_str = model.save_config()\n          with open(model_config_path, \"w\") as model_config_file:\n              model_config_file.write(model_config_str)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Train XGBoost model on CSV', description='Trains an XGBoost model.')\n      _parser.add_argument(\"--training-data\", dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--starting-model\", dest=\"starting_model_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--num-iterations\", dest=\"num_iterations\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--objective\", dest=\"objective\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--booster\", dest=\"booster\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--learning-rate\", dest=\"learning_rate\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--min-split-loss\", dest=\"min_split_loss\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--max-depth\", dest=\"max_depth\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--booster-params\", dest=\"booster_params\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model-config\", dest=\"model_config_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = train_XGBoost_model_on_CSV(**_parsed_args)\n    args:\n    - --training-data\n    - {inputPath: training_data}\n    - --label-column-name\n    - {inputValue: label_column_name}\n    - if:\n        cond: {isPresent: starting_model}\n        then:\n        - --starting-model\n        - {inputPath: starting_model}\n    - if:\n        cond: {isPresent: num_iterations}\n        then:\n        - --num-iterations\n        - {inputValue: num_iterations}\n    - if:\n        cond: {isPresent: objective}\n        then:\n        - --objective\n        - {inputValue: objective}\n    - if:\n        cond: {isPresent: booster}\n        then:\n        - --booster\n        - {inputValue: booster}\n    - if:\n        cond: {isPresent: learning_rate}\n        then:\n        - --learning-rate\n        - {inputValue: learning_rate}\n    - if:\n        cond: {isPresent: min_split_loss}\n        then:\n        - --min-split-loss\n        - {inputValue: min_split_loss}\n    - if:\n        cond: {isPresent: max_depth}\n        then:\n        - --max-depth\n        - {inputValue: max_depth}\n    - if:\n        cond: {isPresent: booster_params}\n        then:\n        - --booster-params\n        - {inputValue: booster_params}\n    - --model\n    - {outputPath: model}\n    - --model-config\n    - {outputPath: model_config}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Train/component.yaml"
          },
          {
            "text": "name: Xgboost predict on CSV\ndescription: Makes predictions using a trained XGBoost model.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Predict/component.yaml'}\ninputs:\n- {name: data, type: CSV, description: Feature data in Apache Parquet format.}\n- {name: model, type: XGBoostModel, description: Trained model in binary XGBoost format.}\n- {name: label_column_name, type: String, description: Optional. Name of the column\n    containing the label data that is excluded during the prediction., optional: true}\noutputs:\n- {name: predictions, description: Model predictions.}\nimplementation:\n  container:\n    image: python:3.10\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def xgboost_predict_on_CSV(\n          data_path,\n          model_path,\n          predictions_path,\n          label_column_name = None,\n      ):\n          \"\"\"Makes predictions using a trained XGBoost model.\n\n          Args:\n              data_path: Feature data in Apache Parquet format.\n              model_path: Trained model in binary XGBoost format.\n              predictions_path: Model predictions.\n              label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          \"\"\"\n          from pathlib import Path\n\n          import numpy\n          import pandas\n          import xgboost\n\n          df = pandas.read_csv(\n              data_path,\n          ).convert_dtypes()\n          print(\"Evaluation data information:\")\n          df.info(verbose=True)\n          # Converting column types that XGBoost does not support\n          for column_name, dtype in df.dtypes.items():\n              if dtype in [\"string\", \"object\"]:\n                  print(f\"Treating the {dtype.name} column '{column_name}' as categorical.\")\n                  df[column_name] = df[column_name].astype(\"category\")\n                  print(f\"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.\")\n              # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213\n              if pandas.api.types.is_float_dtype(dtype):\n                  # Converting from \"Float64\" to \"float64\"\n                  df[column_name] = df[column_name].astype(dtype.name.lower())\n          print(\"Final evaluation data information:\")\n          df.info(verbose=True)\n\n          if label_column_name is not None:\n              df = df.drop(columns=[label_column_name])\n\n          testing_data = xgboost.DMatrix(\n              data=df,\n              enable_categorical=True,\n          )\n\n          model = xgboost.Booster(model_file=model_path)\n\n          predictions = model.predict(testing_data)\n\n          Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)\n          numpy.savetxt(predictions_path, predictions)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Xgboost predict on CSV', description='Makes predictions using a trained XGBoost model.')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--predictions\", dest=\"predictions_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = xgboost_predict_on_CSV(**_parsed_args)\n    args:\n    - --data\n    - {inputPath: data}\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: label_column_name}\n        then:\n        - --label-column-name\n        - {inputValue: label_column_name}\n    - --predictions\n    - {outputPath: predictions}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Predict/component.yaml"
          }
        ],
        "name": "Quick start"
      },
      {
        "components": [
          {
            "text": "name: Chicago Taxi Trips dataset\ndescription: |\n  City of Chicago Taxi Trips dataset: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew\n\n  The input parameters configure the SQL query to the database.\n  The dataset is pretty big, so limit the number of results using the `Limit` or `Where` parameters.\n  Read [Socrata dev](https://dev.socrata.com/docs/queries/) for the advanced query syntax\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: Where, type: String, default: 'trip_start_timestamp>=\"1900-01-01\" AND trip_start_timestamp<\"2100-01-01\"'}\n- {name: Limit, type: Integer, default: '1000', description: 'Number of rows to return. The rows are randomly sampled.'}\n- {name: Select, type: String, default: 'tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total'}\n- {name: Format, type: String, default: 'csv', description: 'Output data format. Suports csv,tsv,cml,rdf,json'}\noutputs:\n- {name: Table, description: 'Result type depends on format. CSV and TSV have header.'}\nimplementation:\n  container:\n    # image: curlimages/curl  # Sets a non-root user which cannot write to mounted volumes. See https://github.com/curl/curl-docker/issues/22\n    image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342\n    command:\n    - sh\n    - -c\n    - |\n      set -e -x -o pipefail\n      output_path=\"$0\"\n      select=\"$1\"\n      where=\"$2\"\n      limit=\"$3\"\n      format=\"$4\"\n      mkdir -p \"$(dirname \"$output_path\")\"\n      curl --get 'https://data.cityofchicago.org/resource/wrvz-psew.'\"${format}\" \\\n          --data-urlencode '$limit='\"${limit}\" \\\n          --data-urlencode '$where='\"${where}\" \\\n          --data-urlencode '$select='\"${select}\" \\\n          | tr -d '\"' > \"$output_path\"  # Removing unneeded quotes around all numbers\n    - {outputPath: Table}\n    - {inputValue: Select}\n    - {inputValue: Where}\n    - {inputValue: Limit}\n    - {inputValue: Format}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipelines/2463ecda532517462590d75e6e14a8af6b55869a/components/datasets/Chicago_Taxi_Trips/component.yaml"
          }
        ],
        "name": "Datasets"
      },
      {
        "folders": [
          {
            "components": [
              {
                "text": "name: Select columns using Pandas on ApacheParquet data\ndescription: Selects columns from a data table.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Select_columns/in_ApacheParquet_format/component.yaml'}\ninputs:\n- {name: table, type: ApacheParquet, description: Input data table.}\n- {name: column_names, type: JsonArray, description: Names of the columns to select\n    from the table.}\noutputs:\n- {name: transformed_table, type: ApacheParquet, description: Transformed data table\n    that only has the chosen columns.}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pandas==1.4.1' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'pandas==1.4.1' 'pyarrow==9.0.0' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def select_columns_using_Pandas_on_ApacheParquet_data(\n          table_path,\n          transformed_table_path,\n          column_names,\n      ):\n          \"\"\"Selects columns from a data table.\n\n          Args:\n              table_path: Input data table.\n              transformed_table_path: Transformed data table that only has the chosen columns.\n              column_names: Names of the columns to select from the table.\n          \"\"\"\n          import pandas\n\n          df = pandas.read_parquet(table_path)\n          df = df[column_names]\n          df.to_parquet(transformed_table_path, index=False)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Select columns using Pandas on ApacheParquet data', description='Selects columns from a data table.')\n      _parser.add_argument(\"--table\", dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--column-names\", dest=\"column_names\", type=json.loads, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transformed-table\", dest=\"transformed_table_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = select_columns_using_Pandas_on_ApacheParquet_data(**_parsed_args)\n    args:\n    - --table\n    - {inputPath: table}\n    - --column-names\n    - {inputValue: column_names}\n    - --transformed-table\n    - {outputPath: transformed_table}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Select_columns/in_ApacheParquet_format/component.yaml"
              },
              {
                "text": "name: Fill all missing values using Pandas on ApacheParquet data\ndescription: Fills the missing column items with the specified replacement value.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Fill_all_missing_values/in_ApacheParquet_format/component.yaml'}\ninputs:\n- {name: table, type: ApacheParquet, description: Input data table.}\n- {name: replacement_value, type: String, description: The value to use when replacing\n    the missing items., default: '0', optional: true}\n- {name: column_names, type: JsonArray, description: Names of the columns where to\n    perform the replacement., optional: true}\noutputs:\n- {name: transformed_table, type: ApacheParquet, description: Transformed data table\n    where missing values are filed.}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pandas==1.4.1' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'pandas==1.4.1' 'pyarrow==9.0.0' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def fill_all_missing_values_using_Pandas_on_ApacheParquet_data(\n          table_path,\n          transformed_table_path,\n          replacement_value = \"0\",\n          column_names = None,\n      ):\n          \"\"\"Fills the missing column items with the specified replacement value.\n\n          Args:\n              table_path: Input data table.\n              transformed_table_path: Transformed data table where missing values are filed.\n              replacement_value: The value to use when replacing the missing items.\n              column_names: Names of the columns where to perform the replacement.\n          \"\"\"\n          import pandas\n\n          df = pandas.read_parquet(path=table_path)\n          for column_name in column_names or df.columns:\n              column = df[column_name]\n              # The `.astype` method does not work correctly on booleans\n              # So we need to special-case them\n              if pandas.api.types.is_bool_dtype(column.dtype):\n                  if replacement_value.lower() in (\"true\", \"1\"):\n                      converted_replacement_value = True\n                  elif replacement_value.lower() in (\"false\", \"0\"):\n                      converted_replacement_value = False\n                  else:\n                      raise ValueError(\n                          f\"Cannot convert value '{replacement_value}' to boolean for column {column_name}.\"\n                      )\n              else:\n                  # Using Pandas to convert the replacement_value to column.dtype.\n                  converted_replacement_value = pandas.Series(\n                      replacement_value, dtype=column.dtype\n                  ).tolist()[0]\n\n              print(\n                  f\"Filling missing values in column '{column_name}' with '{converted_replacement_value}'\"\n              )\n              column.fillna(value=converted_replacement_value)\n\n          df.to_parquet(\n              path=transformed_table_path,\n              index=False,\n          )\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Fill all missing values using Pandas on ApacheParquet data', description='Fills the missing column items with the specified replacement value.')\n      _parser.add_argument(\"--table\", dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--replacement-value\", dest=\"replacement_value\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--column-names\", dest=\"column_names\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transformed-table\", dest=\"transformed_table_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = fill_all_missing_values_using_Pandas_on_ApacheParquet_data(**_parsed_args)\n    args:\n    - --table\n    - {inputPath: table}\n    - if:\n        cond: {isPresent: replacement_value}\n        then:\n        - --replacement-value\n        - {inputValue: replacement_value}\n    - if:\n        cond: {isPresent: column_names}\n        then:\n        - --column-names\n        - {inputValue: column_names}\n    - --transformed-table\n    - {outputPath: transformed_table}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Fill_all_missing_values/in_ApacheParquet_format/component.yaml"
              },
              {
                "text": "name: Binarize column using Pandas on ApacheParquet data\ndescription: Transforms a table column into a binary class column using a predicate.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Binarize_column/in_ApacheParquet_format/component.yaml'}\ninputs:\n- {name: table, type: ApacheParquet, description: Input data table.}\n- {name: column_name, type: String, description: Name of the column to transform to\n    binary class.}\n- {name: predicate, type: String, description: Expression that determines whether\n    the column value is mapped to class 0 (false) or class 1 (true)., default: '>\n    0', optional: true}\n- {name: new_column_name, type: String, description: Name for the new class column.\n    Equals column_name by default., optional: true}\n- name: keep_original_column\n  type: Boolean\n  description: Whether to keep the original column (column_name) in the table.\n  default: \"False\"\n  optional: true\noutputs:\n- {name: transformed_table, type: ApacheParquet, description: Transformed data table\n    with the binary class column.}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pandas==1.4.3' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'pandas==1.4.3' 'pyarrow==9.0.0' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def binarize_column_using_Pandas_on_ApacheParquet_data(\n          table_path,\n          transformed_table_path,\n          column_name,\n          predicate = \"> 0\",\n          new_column_name = None,\n          keep_original_column = False,\n      ):\n          \"\"\"Transforms a table column into a binary class column using a predicate.\n\n          Args:\n              table_path: Input data table.\n              transformed_table_path: Transformed data table with the binary class column.\n              column_name: Name of the column to transform to binary class.\n              predicate: Expression that determines whether the column value is mapped to class 0 (false) or class 1 (true).\n              new_column_name: Name for the new class column. Equals column_name by default.\n              keep_original_column: Whether to keep the original column (column_name) in the table.\n          \"\"\"\n          import pandas\n\n          df = pandas.read_parquet(path=table_path)\n          original_series = df[column_name]\n\n          # Dynamically executing the predicate code\n          # Variable namespace for code execution\n          namespace = dict(x=original_series)\n          # I though that there should be no space before `predicate` so that \"dot\" predicate methods like \".between(min, max)\" work.\n          # However Python allows spaces before dot: `df .isna()`.\n          # So having a space is not a problem\n          transform_code = f\"\"\"new_series_boolean = x {predicate}\"\"\"\n          # Note: exec() takes no keyword arguments\n          # exec(__source=transform_code, __globals=namespace)\n          exec(transform_code, namespace)\n          new_series_boolean = namespace[\"new_series_boolean\"]\n\n          # There are multiple ways to convert boolean column to integer.\n          # .apply(int) might be faster. https://stackoverflow.com/a/49804868/1497385\n          # TODO: Do a proper benchmark.\n          new_series = new_series_boolean.apply(int)\n          # new_series = new_series_boolean.astype(int)\n          # new_series = new_series_boolean.replace({False: 0, True: 1})\n\n          if new_column_name:\n              df.insert(loc=0, column=new_column_name, value=new_series)\n              if not keep_original_column:\n                  df = df.drop(columns=[column_name])\n          else:\n              df[column_name] = new_series\n\n          df.to_parquet(path=transformed_table_path)\n\n      def _deserialize_bool(s) -> bool:\n          from distutils.util import strtobool\n          return strtobool(s) == 1\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Binarize column using Pandas on ApacheParquet data', description='Transforms a table column into a binary class column using a predicate.')\n      _parser.add_argument(\"--table\", dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--column-name\", dest=\"column_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--predicate\", dest=\"predicate\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--new-column-name\", dest=\"new_column_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--keep-original-column\", dest=\"keep_original_column\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transformed-table\", dest=\"transformed_table_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = binarize_column_using_Pandas_on_ApacheParquet_data(**_parsed_args)\n    args:\n    - --table\n    - {inputPath: table}\n    - --column-name\n    - {inputValue: column_name}\n    - if:\n        cond: {isPresent: predicate}\n        then:\n        - --predicate\n        - {inputValue: predicate}\n    - if:\n        cond: {isPresent: new_column_name}\n        then:\n        - --new-column-name\n        - {inputValue: new_column_name}\n    - if:\n        cond: {isPresent: keep_original_column}\n        then:\n        - --keep-original-column\n        - {inputValue: keep_original_column}\n    - --transformed-table\n    - {outputPath: transformed_table}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Binarize_column/in_ApacheParquet_format/component.yaml"
              },
              {
                "text": "name: Pandas Transform DataFrame in ApacheParquet format\ndescription: |-\n  Transform DataFrame loaded from an ApacheParquet file.\n\n      Inputs:\n          table: DataFrame to transform.\n          transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n              The DataFrame variable is called \"df\".\n              Examples:\n              - `df['prod'] = df['X'] * df['Y']`\n              - `df = df[['X', 'prod']]`\n              - `df.insert(0, \"is_positive\", df[\"X\"] > 0)`\n\n      Outputs:\n          transformed_table: Transformed DataFrame.\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: table, type: ApacheParquet}\n- {name: transform_code, type: PythonCode}\noutputs:\n- {name: transformed_table, type: ApacheParquet}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_ApacheParquet_format/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pandas==1.0.4' 'pyarrow==0.14.1' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' 'pyarrow==0.14.1' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def Pandas_Transform_DataFrame_in_ApacheParquet_format(\n          table_path,\n          transformed_table_path,\n          transform_code,\n      ):\n          '''Transform DataFrame loaded from an ApacheParquet file.\n\n          Inputs:\n              table: DataFrame to transform.\n              transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n                  The DataFrame variable is called \"df\".\n                  Examples:\n                  - `df['prod'] = df['X'] * df['Y']`\n                  - `df = df[['X', 'prod']]`\n                  - `df.insert(0, \"is_positive\", df[\"X\"] > 0)`\n\n          Outputs:\n              transformed_table: Transformed DataFrame.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          import pandas\n\n          df = pandas.read_parquet(table_path)\n          # The namespace is needed so that the code can replace `df`. For example df = df[['X']]\n          namespace = locals()\n          exec(transform_code, namespace)\n          namespace['df'].to_parquet(transformed_table_path)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in ApacheParquet format', description='Transform DataFrame loaded from an ApacheParquet file.\\n\\n    Inputs:\\n        table: DataFrame to transform.\\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\\n            The DataFrame variable is called \"df\".\\n            Examples:\\n            - `df[\\'prod\\'] = df[\\'X\\'] * df[\\'Y\\']`\\n            - `df = df[[\\'X\\', \\'prod\\']]`\\n            - `df.insert(0, \"is_positive\", df[\"X\"] > 0)`\\n\\n    Outputs:\\n        transformed_table: Transformed DataFrame.\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n      _parser.add_argument(\"--table\", dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transform-code\", dest=\"transform_code\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transformed-table\", dest=\"transformed_table_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = Pandas_Transform_DataFrame_in_ApacheParquet_format(**_parsed_args)\n    args:\n    - --table\n    - {inputPath: table}\n    - --transform-code\n    - {inputValue: transform_code}\n    - --transformed-table\n    - {outputPath: transformed_table}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Transform_DataFrame/in_ApacheParquet_format/component.yaml"
              },
              {
                "text": "name: Split table into folds\ndescription: |-\n  Splits the data table into the specified number of folds.\n\n      The data is split into the specified number of folds k (default: 5).\n      Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.\n      Each training subsample has (k-1)/k fraction of samples.\n      The train_i subsample is produced by excluding test_i subsample form all samples.\n\n      Inputs:\n          table: The data to split by rows\n          number_of_folds: Number of folds to split data into\n          random_seed: Random seed for reproducible splitting\n\n      Outputs:\n          train_i: The i-th training subsample\n          test_i: The i-th testing subsample\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml'\ninputs:\n- {name: table, type: CSV}\n- {name: number_of_folds, type: Integer, default: '5', optional: true}\n- {name: random_seed, type: Integer, default: '0', optional: true}\noutputs:\n- {name: train_1, type: CSV}\n- {name: train_2, type: CSV}\n- {name: train_3, type: CSV}\n- {name: train_4, type: CSV}\n- {name: train_5, type: CSV}\n- {name: test_1, type: CSV}\n- {name: test_2, type: CSV}\n- {name: test_3, type: CSV}\n- {name: test_4, type: CSV}\n- {name: test_5, type: CSV}\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'scikit-learn==0.23.1' 'pandas==1.0.5' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'scikit-learn==0.23.1' 'pandas==1.0.5' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def split_table_into_folds(\n          table_path,\n\n          train_1_path,\n          train_2_path,\n          train_3_path,\n          train_4_path,\n          train_5_path,\n\n          test_1_path,\n          test_2_path,\n          test_3_path,\n          test_4_path,\n          test_5_path,\n\n          number_of_folds = 5,\n          random_seed = 0,\n      ):\n          \"\"\"Splits the data table into the specified number of folds.\n\n          The data is split into the specified number of folds k (default: 5).\n          Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.\n          Each training subsample has (k-1)/k fraction of samples.\n          The train_i subsample is produced by excluding test_i subsample form all samples.\n\n          Inputs:\n              table: The data to split by rows\n              number_of_folds: Number of folds to split data into\n              random_seed: Random seed for reproducible splitting\n\n          Outputs:\n              train_i: The i-th training subsample\n              test_i: The i-th testing subsample\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n\n          \"\"\"\n          import pandas\n          from sklearn import model_selection\n\n          max_number_of_folds = 5\n\n          if number_of_folds < 1 or number_of_folds > max_number_of_folds:\n              raise ValueError('Number of folds must be between 1 and {}.'.format(max_number_of_folds))\n\n          df = pandas.read_csv(\n              table_path,\n              dtype=\"string\",\n          )\n          splitter = model_selection.KFold(\n              n_splits=number_of_folds,\n              shuffle=True,\n              random_state=random_seed,\n          )\n          folds = list(splitter.split(df))\n\n          fold_paths = [\n              (train_1_path, test_1_path),\n              (train_2_path, test_2_path),\n              (train_3_path, test_3_path),\n              (train_4_path, test_4_path),\n              (train_5_path, test_5_path),\n          ]\n\n          for i in range(max_number_of_folds):\n              (train_path, test_path) = fold_paths[i]\n              if i < len(folds):\n                  (train_indices, test_indices) = folds[i]\n                  train_fold = df.iloc[train_indices]\n                  test_fold = df.iloc[test_indices]\n              else:\n                  train_fold = df.iloc[0:0]\n                  test_fold = df.iloc[0:0]\n              train_fold.to_csv(train_path, index=False)\n              test_fold.to_csv(test_path, index=False)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Split table into folds', description='Splits the data table into the specified number of folds.')\n      _parser.add_argument(\"--table\", dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--number-of-folds\", dest=\"number_of_folds\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--random-seed\", dest=\"random_seed\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--train-1\", dest=\"train_1_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--train-2\", dest=\"train_2_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--train-3\", dest=\"train_3_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--train-4\", dest=\"train_4_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--train-5\", dest=\"train_5_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--test-1\", dest=\"test_1_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--test-2\", dest=\"test_2_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--test-3\", dest=\"test_3_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--test-4\", dest=\"test_4_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--test-5\", dest=\"test_5_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = split_table_into_folds(**_parsed_args)\n    args:\n    - --table\n    - {inputPath: table}\n    - if:\n        cond: {isPresent: number_of_folds}\n        then:\n        - --number-of-folds\n        - {inputValue: number_of_folds}\n    - if:\n        cond: {isPresent: random_seed}\n        then:\n        - --random-seed\n        - {inputValue: random_seed}\n    - --train-1\n    - {outputPath: train_1}\n    - --train-2\n    - {outputPath: train_2}\n    - --train-3\n    - {outputPath: train_3}\n    - --train-4\n    - {outputPath: train_4}\n    - --train-5\n    - {outputPath: train_5}\n    - --test-1\n    - {outputPath: test_1}\n    - --test-2\n    - {outputPath: test_2}\n    - --test-3\n    - {outputPath: test_3}\n    - --test-4\n    - {outputPath: test_4}\n    - --test-5\n    - {outputPath: test_5}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml"
              }
            ],
            "name": "Parquet"
          },
          {
            "components": [
              {
                "text": "name: Select columns using Pandas on CSV data\ndescription: Selects columns from a data table.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Select_columns/in_CSV_format/component.yaml'}\ninputs:\n- {name: table, type: CSV, description: Input data table.}\n- {name: column_names, type: JsonArray, description: Names of the columns to select\n    from the table.}\noutputs:\n- {name: transformed_table, type: CSV, description: Transformed data table that only\n    has the chosen columns.}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pandas==1.4.2' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'pandas==1.4.2' 'numpy<2' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def select_columns_using_Pandas_on_CSV_data(\n          table_path,\n          transformed_table_path,\n          column_names,\n      ):\n          \"\"\"Selects columns from a data table.\n\n          Args:\n              table_path: Input data table.\n              transformed_table_path: Transformed data table that only has the chosen columns.\n              column_names: Names of the columns to select from the table.\n          \"\"\"\n          import pandas\n\n          df = pandas.read_csv(\n              table_path,\n              dtype=\"string\",\n          )\n          df = df[column_names]\n          df.to_csv(transformed_table_path, index=False)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Select columns using Pandas on CSV data', description='Selects columns from a data table.')\n      _parser.add_argument(\"--table\", dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--column-names\", dest=\"column_names\", type=json.loads, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transformed-table\", dest=\"transformed_table_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = select_columns_using_Pandas_on_CSV_data(**_parsed_args)\n    args:\n    - --table\n    - {inputPath: table}\n    - --column-names\n    - {inputValue: column_names}\n    - --transformed-table\n    - {outputPath: transformed_table}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Select_columns/in_CSV_format/component.yaml"
              },
              {
                "text": "name: Fill all missing values using Pandas on CSV data\ndescription: Fills the missing column items with the specified replacement value.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml'}\ninputs:\n- {name: table, type: CSV, description: Input data table.}\n- {name: replacement_value, type: String, description: The value to use when replacing\n    the missing items., default: '0', optional: true}\n- {name: column_names, type: JsonArray, description: Names of the columns where to\n    perform the replacement., optional: true}\noutputs:\n- {name: transformed_table, type: CSV, description: Transformed data table where missing\n    values are filed.}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pandas==1.4.1' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'pandas==1.4.1' 'numpy<2' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def fill_all_missing_values_using_Pandas_on_CSV_data(\n          table_path,\n          transformed_table_path,\n          replacement_value = \"0\",\n          column_names = None,\n      ):\n          \"\"\"Fills the missing column items with the specified replacement value.\n\n          Args:\n              table_path: Input data table.\n              transformed_table_path: Transformed data table where missing values are filed.\n              replacement_value: The value to use when replacing the missing items.\n              column_names: Names of the columns where to perform the replacement.\n          \"\"\"\n          import pandas\n\n          df = pandas.read_csv(\n              table_path,\n              dtype=\"string\",\n          )\n\n          for column_name in column_names or df.columns:\n              df[column_name] = df[column_name].fillna(value=replacement_value)\n\n          df.to_csv(\n              transformed_table_path, index=False,\n          )\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Fill all missing values using Pandas on CSV data', description='Fills the missing column items with the specified replacement value.')\n      _parser.add_argument(\"--table\", dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--replacement-value\", dest=\"replacement_value\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--column-names\", dest=\"column_names\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transformed-table\", dest=\"transformed_table_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = fill_all_missing_values_using_Pandas_on_CSV_data(**_parsed_args)\n    args:\n    - --table\n    - {inputPath: table}\n    - if:\n        cond: {isPresent: replacement_value}\n        then:\n        - --replacement-value\n        - {inputValue: replacement_value}\n    - if:\n        cond: {isPresent: column_names}\n        then:\n        - --column-names\n        - {inputValue: column_names}\n    - --transformed-table\n    - {outputPath: transformed_table}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Fill_all_missing_values/in_CSV_format/component.yaml"
              },
              {
                "text": "name: Binarize column using Pandas on CSV data\ndescription: Transforms a table column into a binary class column using a predicate.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Binarize_column/in_CSV_format/component.yaml'}\ninputs:\n- {name: table, type: CSV, description: Input data table.}\n- {name: column_name, type: String, description: Name of the column to transform to\n    binary class.}\n- {name: predicate, type: String, description: Expression that determines whether\n    the column value is mapped to class 0 (false) or class 1 (true)., default: '>\n    0', optional: true}\n- {name: new_column_name, type: String, description: Name for the new class column.\n    Equals column_name by default., optional: true}\n- name: keep_original_column\n  type: Boolean\n  description: Whether to keep the original column (column_name) in the table.\n  default: \"False\"\n  optional: true\noutputs:\n- {name: transformed_table, type: CSV, description: Transformed data table with the\n    binary class column.}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def binarize_column_using_Pandas_on_CSV_data(\n          table_path,\n          transformed_table_path,\n          column_name,\n          predicate = \"> 0\",\n          new_column_name = None,\n          keep_original_column = False,\n      ):\n          \"\"\"Transforms a table column into a binary class column using a predicate.\n\n          Args:\n              table_path: Input data table.\n              transformed_table_path: Transformed data table with the binary class column.\n              column_name: Name of the column to transform to binary class.\n              predicate: Expression that determines whether the column value is mapped to class 0 (false) or class 1 (true).\n              new_column_name: Name for the new class column. Equals column_name by default.\n              keep_original_column: Whether to keep the original column (column_name) in the table.\n          \"\"\"\n          import pandas\n\n          df = pandas.read_csv(table_path).convert_dtypes()\n          original_series = df[column_name]\n\n          # Dynamically executing the predicate code\n          # Variable namespace for code execution\n          namespace = dict(x=original_series)\n          # I though that there should be no space before `predicate` so that \"dot\" predicate methods like \".between(min, max)\" work.\n          # However Python allows spaces before dot: `df .isna()`.\n          # So having a space is not a problem\n          transform_code = f\"\"\"new_series_boolean = x {predicate}\"\"\"\n          # Note: exec() takes no keyword arguments\n          # exec(__source=transform_code, __globals=namespace)\n          exec(transform_code, namespace)\n          new_series_boolean = namespace[\"new_series_boolean\"]\n\n          # There are multiple ways to convert boolean column to integer.\n          # .apply(int) might be faster. https://stackoverflow.com/a/49804868/1497385\n          # TODO: Do a proper benchmark.\n          new_series = new_series_boolean.apply(int)\n          # new_series = new_series_boolean.astype(int)\n          # new_series = new_series_boolean.replace({False: 0, True: 1})\n\n          if new_column_name:\n              df.insert(loc=0, column=new_column_name, value=new_series)\n              if not keep_original_column:\n                  df = df.drop(columns=[column_name])\n          else:\n              df[column_name] = new_series\n\n          df.to_csv(transformed_table_path, index=False)\n\n      def _deserialize_bool(s) -> bool:\n          from distutils.util import strtobool\n          return strtobool(s) == 1\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Binarize column using Pandas on CSV data', description='Transforms a table column into a binary class column using a predicate.')\n      _parser.add_argument(\"--table\", dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--column-name\", dest=\"column_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--predicate\", dest=\"predicate\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--new-column-name\", dest=\"new_column_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--keep-original-column\", dest=\"keep_original_column\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transformed-table\", dest=\"transformed_table_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = binarize_column_using_Pandas_on_CSV_data(**_parsed_args)\n    args:\n    - --table\n    - {inputPath: table}\n    - --column-name\n    - {inputValue: column_name}\n    - if:\n        cond: {isPresent: predicate}\n        then:\n        - --predicate\n        - {inputValue: predicate}\n    - if:\n        cond: {isPresent: new_column_name}\n        then:\n        - --new-column-name\n        - {inputValue: new_column_name}\n    - if:\n        cond: {isPresent: keep_original_column}\n        then:\n        - --keep-original-column\n        - {inputValue: keep_original_column}\n    - --transformed-table\n    - {outputPath: transformed_table}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Binarize_column/in_CSV_format/component.yaml"
              },
              {
                "text": "name: Pandas Transform DataFrame in CSV format\ndescription: |-\n  Transform DataFrame loaded from a CSV file.\n\n      Inputs:\n          table: Table to transform.\n          transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n              The DataFrame variable is called \"df\".\n              Examples:\n              - `df['prod'] = df['X'] * df['Y']`\n              - `df = df[['X', 'prod']]`\n              - `df.insert(0, \"is_positive\", df[\"X\"] > 0)`\n\n      Outputs:\n          transformed_table: Transformed table.\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: table, type: CSV}\n- {name: transform_code, type: PythonCode}\noutputs:\n- {name: transformed_table, type: CSV}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def Pandas_Transform_DataFrame_in_CSV_format(\n          table_path,\n          transformed_table_path,\n          transform_code,\n      ):\n          '''Transform DataFrame loaded from a CSV file.\n\n          Inputs:\n              table: Table to transform.\n              transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n                  The DataFrame variable is called \"df\".\n                  Examples:\n                  - `df['prod'] = df['X'] * df['Y']`\n                  - `df = df[['X', 'prod']]`\n                  - `df.insert(0, \"is_positive\", df[\"X\"] > 0)`\n\n          Outputs:\n              transformed_table: Transformed table.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          import pandas\n\n          df = pandas.read_csv(\n              table_path,\n          ).convert_dtypes()\n          # The namespace is needed so that the code can replace `df`. For example df = df[['X']]\n          namespace = locals()\n          exec(transform_code, namespace)\n          namespace['df'].to_csv(\n              transformed_table_path,\n              index=False,\n          )\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in CSV format', description='Transform DataFrame loaded from a CSV file.\\n\\n    Inputs:\\n        table: Table to transform.\\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\\n            The DataFrame variable is called \"df\".\\n            Examples:\\n            - `df[\\'prod\\'] = df[\\'X\\'] * df[\\'Y\\']`\\n            - `df = df[[\\'X\\', \\'prod\\']]`\\n            - `df.insert(0, \"is_positive\", df[\"X\"] > 0)`\\n\\n    Outputs:\\n        transformed_table: Transformed table.\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n      _parser.add_argument(\"--table\", dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transform-code\", dest=\"transform_code\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transformed-table\", dest=\"transformed_table_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)\n    args:\n    - --table\n    - {inputPath: table}\n    - --transform-code\n    - {inputValue: transform_code}\n    - --transformed-table\n    - {outputPath: transformed_table}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml"
              },
              {
                "text": "name: Split table into folds\ndescription: |-\n  Splits the data table into the specified number of folds.\n\n      The data is split into the specified number of folds k (default: 5).\n      Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.\n      Each training subsample has (k-1)/k fraction of samples.\n      The train_i subsample is produced by excluding test_i subsample form all samples.\n\n      Inputs:\n          table: The data to split by rows\n          number_of_folds: Number of folds to split data into\n          random_seed: Random seed for reproducible splitting\n\n      Outputs:\n          train_i: The i-th training subsample\n          test_i: The i-th testing subsample\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml'\ninputs:\n- {name: table, type: CSV}\n- {name: number_of_folds, type: Integer, default: '5', optional: true}\n- {name: random_seed, type: Integer, default: '0', optional: true}\noutputs:\n- {name: train_1, type: CSV}\n- {name: train_2, type: CSV}\n- {name: train_3, type: CSV}\n- {name: train_4, type: CSV}\n- {name: train_5, type: CSV}\n- {name: test_1, type: CSV}\n- {name: test_2, type: CSV}\n- {name: test_3, type: CSV}\n- {name: test_4, type: CSV}\n- {name: test_5, type: CSV}\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'scikit-learn==0.23.1' 'pandas==1.0.5' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'scikit-learn==0.23.1' 'pandas==1.0.5' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def split_table_into_folds(\n          table_path,\n\n          train_1_path,\n          train_2_path,\n          train_3_path,\n          train_4_path,\n          train_5_path,\n\n          test_1_path,\n          test_2_path,\n          test_3_path,\n          test_4_path,\n          test_5_path,\n\n          number_of_folds = 5,\n          random_seed = 0,\n      ):\n          \"\"\"Splits the data table into the specified number of folds.\n\n          The data is split into the specified number of folds k (default: 5).\n          Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.\n          Each training subsample has (k-1)/k fraction of samples.\n          The train_i subsample is produced by excluding test_i subsample form all samples.\n\n          Inputs:\n              table: The data to split by rows\n              number_of_folds: Number of folds to split data into\n              random_seed: Random seed for reproducible splitting\n\n          Outputs:\n              train_i: The i-th training subsample\n              test_i: The i-th testing subsample\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n\n          \"\"\"\n          import pandas\n          from sklearn import model_selection\n\n          max_number_of_folds = 5\n\n          if number_of_folds < 1 or number_of_folds > max_number_of_folds:\n              raise ValueError('Number of folds must be between 1 and {}.'.format(max_number_of_folds))\n\n          df = pandas.read_csv(\n              table_path,\n              dtype=\"string\",\n          )\n          splitter = model_selection.KFold(\n              n_splits=number_of_folds,\n              shuffle=True,\n              random_state=random_seed,\n          )\n          folds = list(splitter.split(df))\n\n          fold_paths = [\n              (train_1_path, test_1_path),\n              (train_2_path, test_2_path),\n              (train_3_path, test_3_path),\n              (train_4_path, test_4_path),\n              (train_5_path, test_5_path),\n          ]\n\n          for i in range(max_number_of_folds):\n              (train_path, test_path) = fold_paths[i]\n              if i < len(folds):\n                  (train_indices, test_indices) = folds[i]\n                  train_fold = df.iloc[train_indices]\n                  test_fold = df.iloc[test_indices]\n              else:\n                  train_fold = df.iloc[0:0]\n                  test_fold = df.iloc[0:0]\n              train_fold.to_csv(train_path, index=False)\n              test_fold.to_csv(test_path, index=False)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Split table into folds', description='Splits the data table into the specified number of folds.')\n      _parser.add_argument(\"--table\", dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--number-of-folds\", dest=\"number_of_folds\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--random-seed\", dest=\"random_seed\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--train-1\", dest=\"train_1_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--train-2\", dest=\"train_2_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--train-3\", dest=\"train_3_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--train-4\", dest=\"train_4_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--train-5\", dest=\"train_5_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--test-1\", dest=\"test_1_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--test-2\", dest=\"test_2_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--test-3\", dest=\"test_3_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--test-4\", dest=\"test_4_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--test-5\", dest=\"test_5_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = split_table_into_folds(**_parsed_args)\n    args:\n    - --table\n    - {inputPath: table}\n    - if:\n        cond: {isPresent: number_of_folds}\n        then:\n        - --number-of-folds\n        - {inputValue: number_of_folds}\n    - if:\n        cond: {isPresent: random_seed}\n        then:\n        - --random-seed\n        - {inputValue: random_seed}\n    - --train-1\n    - {outputPath: train_1}\n    - --train-2\n    - {outputPath: train_2}\n    - --train-3\n    - {outputPath: train_3}\n    - --train-4\n    - {outputPath: train_4}\n    - --train-5\n    - {outputPath: train_5}\n    - --test-1\n    - {outputPath: test_1}\n    - --test-2\n    - {outputPath: test_2}\n    - --test-3\n    - {outputPath: test_3}\n    - --test-4\n    - {outputPath: test_4}\n    - --test-5\n    - {outputPath: test_5}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml"
              },
              {
                "text": "name: Split rows into subsets\ndescription: Splits the data table according to the split fractions.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml'}\ninputs:\n- {name: table, type: CSV, description: Input data table.}\n- {name: fraction_1, type: Float, description: 'The proportion of the lines to put\n    into the 1st split. Range: [0, 1]'}\n- name: fraction_2\n  type: Float\n  description: |-\n    The proportion of the lines to put into the 2nd split. Range: [0, 1]\n    If fraction_2 is not specified, then fraction_2 = 1 - fraction_1.\n    The remaining lines go to the 3rd split (if any).\n  optional: true\n- {name: random_seed, type: Integer, description: Controls the seed of the random\n    processes., default: '0', optional: true}\noutputs:\n- {name: split_1, type: CSV, description: Subset of the data table.}\n- {name: split_2, type: CSV, description: Subset of the data table.}\n- {name: split_3, type: CSV, description: Subset of the data table.}\n- {name: split_1_count, type: Integer}\n- {name: split_2_count, type: Integer}\n- {name: split_3_count, type: Integer}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def split_rows_into_subsets(\n          table_path,\n          split_1_path,\n          split_2_path,\n          split_3_path,\n          fraction_1,\n          fraction_2 = None,\n          random_seed = 0,\n      ):\n          \"\"\"Splits the data table according to the split fractions.\n\n          Args:\n              table_path: Input data table.\n              split_1_path: Subset of the data table.\n              split_2_path: Subset of the data table.\n              split_3_path: Subset of the data table.\n              fraction_1: The proportion of the lines to put into the 1st split. Range: [0, 1]\n              fraction_2: The proportion of the lines to put into the 2nd split. Range: [0, 1]\n                  If fraction_2 is not specified, then fraction_2 = 1 - fraction_1.\n                  The remaining lines go to the 3rd split (if any).\n              random_seed: Controls the seed of the random processes.\n          \"\"\"\n          import random\n\n          random.seed(random_seed)\n\n          SHUFFLE_BUFFER_SIZE = 10000\n\n          num_splits = 3\n\n          if fraction_1 < 0 or fraction_1 > 1:\n              raise ValueError(\"fraction_1 must be in between 0 and 1.\")\n\n          if fraction_2 is None:\n              fraction_2 = 1 - fraction_1\n          if fraction_2 < 0 or fraction_2 > 1:\n              raise ValueError(\"fraction_2 must be in between 0 and 1.\")\n\n          fraction_3 = 1 - fraction_1 - fraction_2\n\n          fractions = [\n              fraction_1,\n              fraction_2,\n              fraction_3,\n          ]\n\n          assert sum(fractions) == 1\n\n          written_line_counts = [0] * num_splits\n\n          output_files = [\n              open(split_1_path, \"wb\"),\n              open(split_2_path, \"wb\"),\n              open(split_3_path, \"wb\"),\n          ]\n\n          with open(table_path, \"rb\") as input_file:\n              # Writing the headers\n              header_line = input_file.readline()\n              for output_file in output_files:\n                  output_file.write(header_line)\n\n              while True:\n                  line_buffer = []\n                  for i in range(SHUFFLE_BUFFER_SIZE):\n                      line = input_file.readline()\n                      if not line:\n                          break\n                      line_buffer.append(line)\n\n                  # We need to exactly partition the lines between the output files\n                  # To overcome possible systematic bias, we could calculate the total numbers\n                  # of lines written to each file and take that into account.\n                  num_read_lines = len(line_buffer)\n                  number_of_lines_for_files = [0] * num_splits\n                  # List that will have the index of the destination file for each line\n                  file_index_for_line = []\n                  remaining_lines = num_read_lines\n                  remaining_fraction = 1\n                  for i in range(num_splits):\n                      number_of_lines_for_file = (\n                          round(remaining_lines * (fractions[i] / remaining_fraction))\n                          if remaining_fraction > 0\n                          else 0\n                      )\n                      number_of_lines_for_files[i] = number_of_lines_for_file\n                      remaining_lines -= number_of_lines_for_file\n                      remaining_fraction -= fractions[i]\n                      file_index_for_line.extend([i] * number_of_lines_for_file)\n\n                  assert remaining_lines == 0, f\"{remaining_lines}\"\n                  assert len(file_index_for_line) == num_read_lines\n\n                  random.shuffle(file_index_for_line)\n\n                  for i in range(num_read_lines):\n                      output_files[file_index_for_line[i]].write(line_buffer[i])\n                      written_line_counts[file_index_for_line[i]] += 1\n\n                  # Exit if the file ended before we were able to fully fill the buffer\n                  if len(line_buffer) != SHUFFLE_BUFFER_SIZE:\n                      break\n\n          for output_file in output_files:\n              output_file.close()\n\n          return written_line_counts\n\n      def _serialize_int(int_value: int) -> str:\n          if isinstance(int_value, str):\n              return int_value\n          if not isinstance(int_value, int):\n              raise TypeError('Value \"{}\" has type \"{}\" instead of int.'.format(str(int_value), str(type(int_value))))\n          return str(int_value)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Split rows into subsets', description='Splits the data table according to the split fractions.')\n      _parser.add_argument(\"--table\", dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--fraction-1\", dest=\"fraction_1\", type=float, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--fraction-2\", dest=\"fraction_2\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--random-seed\", dest=\"random_seed\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--split-1\", dest=\"split_1_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--split-2\", dest=\"split_2_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--split-3\", dest=\"split_3_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=3)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = split_rows_into_subsets(**_parsed_args)\n\n      _output_serializers = [\n          _serialize_int,\n          _serialize_int,\n          _serialize_int,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --table\n    - {inputPath: table}\n    - --fraction-1\n    - {inputValue: fraction_1}\n    - if:\n        cond: {isPresent: fraction_2}\n        then:\n        - --fraction-2\n        - {inputValue: fraction_2}\n    - if:\n        cond: {isPresent: random_seed}\n        then:\n        - --random-seed\n        - {inputValue: random_seed}\n    - --split-1\n    - {outputPath: split_1}\n    - --split-2\n    - {outputPath: split_2}\n    - --split-3\n    - {outputPath: split_3}\n    - '----output-paths'\n    - {outputPath: split_1_count}\n    - {outputPath: split_2_count}\n    - {outputPath: split_3_count}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/92aa941c738e5b2fe957f987925053bf70996264/components/dataset_manipulation/Split_rows_into_subsets/in_CSV/component.yaml"
              }
            ],
            "name": "CSV"
          },
          {
            "components": [
              {
                "text": "name: Transform using Pandas DataFrame on JsonLines data\ndescription: Transform DataFrame loaded from an ApacheParquet file.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/pandas/Transform_DataFrame/in_JsonLines_format/component.yaml'}\ninputs:\n- {name: table, type: JsonLines, description: DataFrame to transform.}\n- name: transform_code\n  type: PythonCode\n  description: |-\n    Transformation code. Code is written in Python and can consist of multiple lines.\n    The DataFrame variable is called \"df\".\n    Examples:\n    - `df['prod'] = df['X'] * df['Y']`\n    - `df = df[['X', 'prod']]`\n    - `df.insert(0, \"is_positive\", df[\"X\"] > 0)`\noutputs:\n- {name: transformed_table, type: JsonLines, description: Transformed DataFrame.}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def transform_using_Pandas_DataFrame_on_JsonLines_data(\n          table_path,\n          transformed_table_path,\n          transform_code,\n      ):\n          \"\"\"Transform DataFrame loaded from an ApacheParquet file.\n\n          Args:\n              table_path: DataFrame to transform.\n              transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n                  The DataFrame variable is called \"df\".\n                  Examples:\n                  - `df['prod'] = df['X'] * df['Y']`\n                  - `df = df[['X', 'prod']]`\n                  - `df.insert(0, \"is_positive\", df[\"X\"] > 0)`\n              transformed_table_path: Transformed DataFrame.\n          \"\"\"\n          import pandas\n\n          df = pandas.read_json(path_or_buf=table_path, lines=True)\n          # The namespace is needed so that the code can replace `df`. For example df = df[['X']]\n          namespace = dict(df=df)\n          exec(transform_code, namespace)\n          df = namespace[\"df\"]\n          df.to_json(path_or_buf=transformed_table_path, orient=\"records\", lines=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Transform using Pandas DataFrame on JsonLines data', description='Transform DataFrame loaded from an ApacheParquet file.')\n      _parser.add_argument(\"--table\", dest=\"table_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transform-code\", dest=\"transform_code\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transformed-table\", dest=\"transformed_table_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = transform_using_Pandas_DataFrame_on_JsonLines_data(**_parsed_args)\n    args:\n    - --table\n    - {inputPath: table}\n    - --transform-code\n    - {inputValue: transform_code}\n    - --transformed-table\n    - {outputPath: transformed_table}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/pandas/Transform_DataFrame/in_JsonLines_format/component.yaml"
              }
            ],
            "name": "JSON lines"
          },
          {
            "components": [
              {
                "text": "name: Get element by index from JSON\ninputs:\n- {name: Json}\n- {name: Index, type: Integer}\noutputs:\n- {name: Output}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Get_element_by_index/component.yaml'\nimplementation:\n  container:\n    image: stedolan/jq:latest\n    command:\n    - sh\n    - -exc\n    - |\n      input_path=$0\n      output_path=$1\n      index=$2\n      mkdir -p \"$(dirname \"$output_path\")\"\n      < \"$input_path\" jq --raw-output --join-output .[\"$index\"] > \"$output_path\"\n    - {inputPath: Json}\n    - {outputPath: Output}\n    - {inputValue: Index}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a/components/json/Get_element_by_index/component.yaml"
              },
              {
                "text": "name: Get element by key from JSON\ninputs:\n- {name: Json}\n- {name: Key, type: String}\noutputs:\n- {name: Output}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Get_element_by_key/component.yaml'\nimplementation:\n  container:\n    image: stedolan/jq:latest\n    command:\n    - sh\n    - -exc\n    - |\n      input_path=$0\n      output_path=$1\n      key=$2\n      mkdir -p \"$(dirname \"$output_path\")\"\n      < \"$input_path\" jq --raw-output --join-output '.[\"'\"$key\"'\"]' > \"$output_path\"\n    - {inputPath: Json}\n    - {outputPath: Output}\n    - {inputValue: Key}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/dcf4fdde4876e8d76aa0131ad4d67c47b2b5591a/components/json/Get_element_by_key/component.yaml"
              },
              {
                "text": "name: Query JSON using JQ\ninputs:\n- {name: Json}\n- {name: Query, type: String}\n- {name: Options, type: String, default: '--raw-output'}\noutputs:\n- {name: Output}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Query/component.yaml'\nimplementation:\n  container:\n    image: stedolan/jq:latest\n    command:\n    - sh\n    - -exc\n    - |\n      input_path=$0\n      output_path=$1\n      query=$2\n      options=$3\n      mkdir -p \"$(dirname \"$output_path\")\"\n      < \"$input_path\" jq $options \"$query\" > \"$output_path\"\n    - {inputPath: Json}\n    - {outputPath: Output}\n    - {inputValue: Query}\n    - {inputValue: Options}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Query/component.yaml"
              },
              {
                "text": "name: Build dict\ndescription: Creates a JSON object from multiple key and value pairs.\ninputs:\n- {name: key_1, type: String, optional: true}\n- {name: value_1, type: JsonObject, optional: true}\n- {name: key_2, type: String, optional: true}\n- {name: value_2, type: JsonObject, optional: true}\n- {name: key_3, type: String, optional: true}\n- {name: value_3, type: JsonObject, optional: true}\n- {name: key_4, type: String, optional: true}\n- {name: value_4, type: JsonObject, optional: true}\n- {name: key_5, type: String, optional: true}\n- {name: value_5, type: JsonObject, optional: true}\noutputs:\n- {name: Output, type: JsonObject}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_dict/component.yaml'\nimplementation:\n  container:\n    image: python:3.8\n    command:\n    - python3\n    - -u\n    - -c\n    - |\n      def build_dict(\n          key_1 = None,\n          value_1 = None,\n          key_2 = None,\n          value_2 = None,\n          key_3 = None,\n          value_3 = None,\n          key_4 = None,\n          value_4 = None,\n          key_5 = None,\n          value_5 = None,\n      ):\n          \"\"\"Creates a JSON object from multiple key and value pairs.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          \"\"\"\n          result = dict([\n              (key_1, value_1),\n              (key_2, value_2),\n              (key_3, value_3),\n              (key_4, value_4),\n              (key_5, value_5),\n          ])\n          if None in result:\n              del result[None]\n          return result\n\n      import json\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Build dict', description='Creates a JSON object from multiple key and value pairs.')\n      _parser.add_argument(\"--key-1\", dest=\"key_1\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--value-1\", dest=\"value_1\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--key-2\", dest=\"key_2\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--value-2\", dest=\"value_2\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--key-3\", dest=\"key_3\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--value-3\", dest=\"value_3\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--key-4\", dest=\"key_4\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--value-4\", dest=\"value_4\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--key-5\", dest=\"key_5\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--value-5\", dest=\"value_5\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = build_dict(**_parsed_args)\n\n      _outputs = [_outputs]\n\n      _output_serializers = [\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - if:\n        cond: {isPresent: key_1}\n        then:\n        - --key-1\n        - {inputValue: key_1}\n    - if:\n        cond: {isPresent: value_1}\n        then:\n        - --value-1\n        - {inputValue: value_1}\n    - if:\n        cond: {isPresent: key_2}\n        then:\n        - --key-2\n        - {inputValue: key_2}\n    - if:\n        cond: {isPresent: value_2}\n        then:\n        - --value-2\n        - {inputValue: value_2}\n    - if:\n        cond: {isPresent: key_3}\n        then:\n        - --key-3\n        - {inputValue: key_3}\n    - if:\n        cond: {isPresent: value_3}\n        then:\n        - --value-3\n        - {inputValue: value_3}\n    - if:\n        cond: {isPresent: key_4}\n        then:\n        - --key-4\n        - {inputValue: key_4}\n    - if:\n        cond: {isPresent: value_4}\n        then:\n        - --value-4\n        - {inputValue: value_4}\n    - if:\n        cond: {isPresent: key_5}\n        then:\n        - --key-5\n        - {inputValue: key_5}\n    - if:\n        cond: {isPresent: value_5}\n        then:\n        - --value-5\n        - {inputValue: value_5}\n    - '----output-paths'\n    - {outputPath: Output}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Build_dict/component.yaml"
              },
              {
                "text": "name: Build list\ndescription: Creates a JSON array from multiple items.\ninputs:\n- {name: item_1, type: JsonObject, optional: true}\n- {name: item_2, type: JsonObject, optional: true}\n- {name: item_3, type: JsonObject, optional: true}\n- {name: item_4, type: JsonObject, optional: true}\n- {name: item_5, type: JsonObject, optional: true}\noutputs:\n- {name: Output, type: JsonArray}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list/component.yaml'\nimplementation:\n  container:\n    image: python:3.8\n    command:\n    - python3\n    - -u\n    - -c\n    - |\n      def build_list(\n          item_1 = None,\n          item_2 = None,\n          item_3 = None,\n          item_4 = None,\n          item_5 = None,\n      ):\n          \"\"\"Creates a JSON array from multiple items.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          \"\"\"\n          result = []\n          for item in [item_1, item_2, item_3, item_4, item_5]:\n              if item is not None:\n                  result.append(item)\n          return result\n\n      import json\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Build list', description='Creates a JSON array from multiple items.')\n      _parser.add_argument(\"--item-1\", dest=\"item_1\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-2\", dest=\"item_2\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-3\", dest=\"item_3\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-4\", dest=\"item_4\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-5\", dest=\"item_5\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = build_list(**_parsed_args)\n\n      _outputs = [_outputs]\n\n      _output_serializers = [\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - if:\n        cond: {isPresent: item_1}\n        then:\n        - --item-1\n        - {inputValue: item_1}\n    - if:\n        cond: {isPresent: item_2}\n        then:\n        - --item-2\n        - {inputValue: item_2}\n    - if:\n        cond: {isPresent: item_3}\n        then:\n        - --item-3\n        - {inputValue: item_3}\n    - if:\n        cond: {isPresent: item_4}\n        then:\n        - --item-4\n        - {inputValue: item_4}\n    - if:\n        cond: {isPresent: item_5}\n        then:\n        - --item-5\n        - {inputValue: item_5}\n    - '----output-paths'\n    - {outputPath: Output}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Build_list/component.yaml"
              },
              {
                "text": "name: Build list of strings\ndescription: Creates a JSON array from multiple strings.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list_of_strings/component.yaml'}\ninputs:\n- {name: item_1, type: String, optional: true}\n- {name: item_2, type: String, optional: true}\n- {name: item_3, type: String, optional: true}\n- {name: item_4, type: String, optional: true}\n- {name: item_5, type: String, optional: true}\noutputs:\n- {name: Output, type: JsonArray}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def build_list_of_strings(\n          item_1 = None,\n          item_2 = None,\n          item_3 = None,\n          item_4 = None,\n          item_5 = None,\n      ):\n          \"\"\"Creates a JSON array from multiple strings.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          \"\"\"\n          result = []\n          for item in [item_1, item_2, item_3, item_4, item_5]:\n              if item is not None:\n                  result.append(item)\n          return result\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Build list of strings', description='Creates a JSON array from multiple strings.')\n      _parser.add_argument(\"--item-1\", dest=\"item_1\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-2\", dest=\"item_2\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-3\", dest=\"item_3\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-4\", dest=\"item_4\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-5\", dest=\"item_5\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = build_list_of_strings(**_parsed_args)\n\n      _outputs = [_outputs]\n\n      _output_serializers = [\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - if:\n        cond: {isPresent: item_1}\n        then:\n        - --item-1\n        - {inputValue: item_1}\n    - if:\n        cond: {isPresent: item_2}\n        then:\n        - --item-2\n        - {inputValue: item_2}\n    - if:\n        cond: {isPresent: item_3}\n        then:\n        - --item-3\n        - {inputValue: item_3}\n    - if:\n        cond: {isPresent: item_4}\n        then:\n        - --item-4\n        - {inputValue: item_4}\n    - if:\n        cond: {isPresent: item_5}\n        then:\n        - --item-5\n        - {inputValue: item_5}\n    - '----output-paths'\n    - {outputPath: Output}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/aecac18d4023c73c561d7f21192253e9593b9932/components/json/Build_list_of_strings/component.yaml"
              },
              {
                "text": "name: Build list of integers\ndescription: Creates a JSON array from multiple integer numbers.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list_of_integers/component.yaml'}\ninputs:\n- {name: item_1, type: Integer, optional: true}\n- {name: item_2, type: Integer, optional: true}\n- {name: item_3, type: Integer, optional: true}\n- {name: item_4, type: Integer, optional: true}\n- {name: item_5, type: Integer, optional: true}\noutputs:\n- {name: Output, type: JsonArray}\nimplementation:\n  container:\n    image: python:3.8\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def build_list_of_integers(\n          item_1 = None,\n          item_2 = None,\n          item_3 = None,\n          item_4 = None,\n          item_5 = None,\n      ):\n          \"\"\"Creates a JSON array from multiple integer numbers.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          \"\"\"\n          result = []\n          for item in [item_1, item_2, item_3, item_4, item_5]:\n              if item is not None:\n                  result.append(item)\n          return result\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Build list of integers', description='Creates a JSON array from multiple integer numbers.')\n      _parser.add_argument(\"--item-1\", dest=\"item_1\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-2\", dest=\"item_2\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-3\", dest=\"item_3\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-4\", dest=\"item_4\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-5\", dest=\"item_5\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = build_list_of_integers(**_parsed_args)\n\n      _outputs = [_outputs]\n\n      _output_serializers = [\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - if:\n        cond: {isPresent: item_1}\n        then:\n        - --item-1\n        - {inputValue: item_1}\n    - if:\n        cond: {isPresent: item_2}\n        then:\n        - --item-2\n        - {inputValue: item_2}\n    - if:\n        cond: {isPresent: item_3}\n        then:\n        - --item-3\n        - {inputValue: item_3}\n    - if:\n        cond: {isPresent: item_4}\n        then:\n        - --item-4\n        - {inputValue: item_4}\n    - if:\n        cond: {isPresent: item_5}\n        then:\n        - --item-5\n        - {inputValue: item_5}\n    - '----output-paths'\n    - {outputPath: Output}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/bb9d7518b3a23e945c8cc1663942063c6b92c20f/components/json/Build_list_of_integers/component.yaml"
              },
              {
                "text": "name: Build list of floats\ndescription: Creates a JSON array from multiple floating-point numbers.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Build_list_of_floats/component.yaml'}\ninputs:\n- {name: item_1, type: Float, optional: true}\n- {name: item_2, type: Float, optional: true}\n- {name: item_3, type: Float, optional: true}\n- {name: item_4, type: Float, optional: true}\n- {name: item_5, type: Float, optional: true}\noutputs:\n- {name: Output, type: JsonArray}\nimplementation:\n  container:\n    image: python:3.8\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def build_list_of_floats(\n          item_1 = None,\n          item_2 = None,\n          item_3 = None,\n          item_4 = None,\n          item_5 = None,\n      ):\n          \"\"\"Creates a JSON array from multiple floating-point numbers.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          \"\"\"\n          result = []\n          for item in [item_1, item_2, item_3, item_4, item_5]:\n              if item is not None:\n                  result.append(item)\n          return result\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Build list of floats', description='Creates a JSON array from multiple floating-point numbers.')\n      _parser.add_argument(\"--item-1\", dest=\"item_1\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-2\", dest=\"item_2\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-3\", dest=\"item_3\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-4\", dest=\"item_4\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--item-5\", dest=\"item_5\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = build_list_of_floats(**_parsed_args)\n\n      _outputs = [_outputs]\n\n      _output_serializers = [\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - if:\n        cond: {isPresent: item_1}\n        then:\n        - --item-1\n        - {inputValue: item_1}\n    - if:\n        cond: {isPresent: item_2}\n        then:\n        - --item-2\n        - {inputValue: item_2}\n    - if:\n        cond: {isPresent: item_3}\n        then:\n        - --item-3\n        - {inputValue: item_3}\n    - if:\n        cond: {isPresent: item_4}\n        then:\n        - --item-4\n        - {inputValue: item_4}\n    - if:\n        cond: {isPresent: item_5}\n        then:\n        - --item-5\n        - {inputValue: item_5}\n    - '----output-paths'\n    - {outputPath: Output}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/bb9d7518b3a23e945c8cc1663942063c6b92c20f/components/json/Build_list_of_floats/component.yaml"
              },
              {
                "text": "name: Combine lists\ndescription: Combines multiple JSON arrays into one.\ninputs:\n- {name: list_1, type: JsonArray, optional: true}\n- {name: list_2, type: JsonArray, optional: true}\n- {name: list_3, type: JsonArray, optional: true}\n- {name: list_4, type: JsonArray, optional: true}\n- {name: list_5, type: JsonArray, optional: true}\noutputs:\n- {name: Output, type: JsonArray}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/json/Combine_lists/component.yaml'\nimplementation:\n  container:\n    image: python:3.8\n    command:\n    - python3\n    - -u\n    - -c\n    - |\n      def combine_lists(\n          list_1 = None,\n          list_2 = None,\n          list_3 = None,\n          list_4 = None,\n          list_5 = None,\n      ):\n          \"\"\"Combines multiple JSON arrays into one.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          \"\"\"\n          result = []\n          for list in [list_1, list_2, list_3, list_4, list_5]:\n              if list is not None:\n                  result.extend(list)\n          return result\n\n      import json\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Combine lists', description='Combines multiple JSON arrays into one.')\n      _parser.add_argument(\"--list-1\", dest=\"list_1\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--list-2\", dest=\"list_2\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--list-3\", dest=\"list_3\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--list-4\", dest=\"list_4\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--list-5\", dest=\"list_5\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = combine_lists(**_parsed_args)\n\n      _outputs = [_outputs]\n\n      _output_serializers = [\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - if:\n        cond: {isPresent: list_1}\n        then:\n        - --list-1\n        - {inputValue: list_1}\n    - if:\n        cond: {isPresent: list_2}\n        then:\n        - --list-2\n        - {inputValue: list_2}\n    - if:\n        cond: {isPresent: list_3}\n        then:\n        - --list-3\n        - {inputValue: list_3}\n    - if:\n        cond: {isPresent: list_4}\n        then:\n        - --list-4\n        - {inputValue: list_4}\n    - if:\n        cond: {isPresent: list_5}\n        then:\n        - --list-5\n        - {inputValue: list_5}\n    - '----output-paths'\n    - {outputPath: Output}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/json/Combine_lists/component.yaml"
              }
            ],
            "name": "JSON"
          }
        ],
        "name": "Data manipulation"
      },
      {
        "components": [
          {
            "text": "name: Download data\ninputs:\n- {name: Url, type: URI}\n- {name: curl options, type: string, default: '--location', description: 'Additional options given to the curl bprogram. See https://curl.haxx.se/docs/manpage.html'}\noutputs:\n- {name: Data}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/web/Download/component.yaml'\nimplementation:\n  container:\n    # image: curlimages/curl  # Sets a non-root user which cannot write to mounted volumes. See https://github.com/curl/curl-docker/issues/22\n    image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342\n    command:\n    - sh\n    - -exc\n    - |\n      url=\"$0\"\n      output_path=\"$1\"\n      curl_options=\"$2\"\n\n      mkdir -p \"$(dirname \"$output_path\")\"\n      curl --get \"$url\" --output \"$output_path\" $curl_options\n    - inputValue: Url\n    - outputPath: Data\n    - inputValue: curl options\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/web/Download/component.yaml"
          },
          {
            "text": "name: Download from GCS\ninputs:\n- {name: GCS path, type: URI}\noutputs:\n- {name: Data}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/download/component.yaml'\nimplementation:\n    container:\n        image: google/cloud-sdk\n        command:\n        - bash # Pattern comparison only works in Bash\n        - -ex\n        - -c\n        - |\n            if [ -n \"${GOOGLE_APPLICATION_CREDENTIALS}\" ]; then\n                gcloud auth activate-service-account --key-file=\"${GOOGLE_APPLICATION_CREDENTIALS}\"\n            fi\n\n            uri=\"$0\"\n            output_path=\"$1\"\n\n            # Checking whether the URI points to a single blob, a directory or a URI pattern\n            # URI points to a blob when that URI does not end with slash and listing that URI only yields the same URI\n            if [[ \"$uri\" != */ ]] && (gsutil ls \"$uri\" | grep --fixed-strings --line-regexp \"$uri\"); then\n                mkdir -p \"$(dirname \"$output_path\")\"\n                gsutil -m cp -r \"$uri\" \"$output_path\"\n            else\n                mkdir -p \"$output_path\" # When source path is a directory, gsutil requires the destination to also be a directory\n                gsutil -m rsync -r \"$uri\" \"$output_path\" # gsutil cp has different path handling than Linux cp. It always puts the source directory (name) inside the destination directory. gsutil rsync does not have that problem.\n            fi\n        - inputValue: GCS path\n        - outputPath: Data\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/download/component.yaml"
          },
          {
            "text": "name: Upload to GCS with unique name\ndescription: Upload to GCS with unique URI suffix\ninputs:\n- {name: Data}\n- {name: GCS path prefix, type: URI}\noutputs:\n- {name: GCS path, type: String}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/upload_to_unique_uri/component.yaml'\nimplementation:\n    container:\n        image: google/cloud-sdk\n        command:\n        - sh\n        - -ex\n        - -c\n        - |\n            data_path=\"$0\"\n            url_prefix=\"$1\"\n            output_path=\"$2\"\n            random_string=$(< dev/urandom tr -dc A-Za-z0-9 | head -c 64)\n            uri=\"${url_prefix}${random_string}\"\n            if [ -n \"${GOOGLE_APPLICATION_CREDENTIALS}\" ]; then\n                gcloud auth activate-service-account --key-file=\"${GOOGLE_APPLICATION_CREDENTIALS}\"\n            fi\n            gsutil cp -r \"$data_path\" \"$uri\"\n            mkdir -p \"$(dirname \"$output_path\")\"\n            printf \"%s\" \"$uri\" > \"$output_path\"\n        - inputPath: Data\n        - {inputValue: GCS path prefix}\n        - outputPath: GCS path",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_unique_uri/component.yaml"
          },
          {
            "text": "name: Upload to GCS\ninputs:\n- {name: Data}\n- {name: GCS path, type: URI}\noutputs:\n- {name: GCS path, type: String}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/upload_to_explicit_uri/component.yaml'\nimplementation:\n    container:\n        image: google/cloud-sdk\n        command:\n        - sh\n        - -ex\n        - -c\n        - |\n            if [ -n \"${GOOGLE_APPLICATION_CREDENTIALS}\" ]; then\n                gcloud auth activate-service-account --key-file=\"${GOOGLE_APPLICATION_CREDENTIALS}\"\n            fi\n            gsutil cp -r \"$0\" \"$1\"\n            mkdir -p \"$(dirname \"$2\")\"\n            printf \"%s\" \"$1\" > \"$2\"\n        - inputPath: Data\n        - inputValue: GCS path\n        - outputPath: GCS path\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_explicit_uri/component.yaml"
          },
          {
            "text": "name: Download data from IPFS\ndescription: |\n  Download data from IPFS using ipget.\n  See https://ipfs.io/\n  See https://github.com/ipfs/ipget\ninputs:\n- {name: Path, type: String, description: \"IPFS Path. Example: /ipfs/QmWATWQ7fVPP2EFGu71UkfnqhYXDYH566qy47CnJDgvs8u\"}\noutputs:\n- {name: Data}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: \"https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/Download_and_upload/IPFS/Download/component.yaml\"\nimplementation:\n  container:\n    image: alpine\n    command:\n    - sh\n    - -exc\n    - |\n      path=$0\n      output_data_path=$1\n      mkdir -p \"$(dirname \"$output_data_path\")\"\n\n      wget https://dist.ipfs.io/ipget/v0.7.0/ipget_v0.7.0_linux-amd64.tar.gz\n      tar -xzf ipget_v0.7.0_linux-amd64.tar.gz\n      cd ipget\n\n      # Fixing IPFS on Alpine issue:\n      # https://discuss.ipfs.io/t/why-go-ipfs-cant-run-in-alphine-linux/6625\n      mkdir /lib64 && ln -s /lib/libc.musl-x86_64.so.1 /lib64/ld-linux-x86-64.so.2\n\n      ./ipget --output \"$output_data_path\" \"$path\"\n    - {inputValue: Path}\n    - {outputPath: Data}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/cca2d8569d01b527df10c629258be04d52eacc43/components/Download_and_upload/IPFS/Download/component.yaml"
          }
        ],
        "name": "Upload/Download"
      },
      {
        "folders": [
          {
            "components": [
              {
                "text": "name: Train linear regression model using scikit learn from CSV\ndescription: Trains linear regression model using Scikit-learn.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Scikit_learn/Train_linear_regression_model/from_CSV/component.yaml'}\ninputs:\n- {name: dataset, type: CSV, description: Tabular dataset for training.}\n- {name: label_column_name, type: String, description: Name of the table column to\n    use as label.}\noutputs:\n- {name: model, type: ScikitLearnPickleModel, description: Trained model in Scikit-learn\n    pickle format.}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'scikit-learn==1.0.2' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'scikit-learn==1.0.2' 'pandas==1.4.3' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def train_linear_regression_model_using_scikit_learn_from_CSV(\n          dataset_path,\n          model_path,\n          label_column_name,\n      ):\n          \"\"\"Trains linear regression model using Scikit-learn.\n\n          See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\n\n          Args:\n              dataset_path: Tabular dataset for training.\n              model_path: Trained model in Scikit-learn pickle format.\n              label_column_name: Name of the table column to use as label.\n          \"\"\"\n          import pandas\n          import pickle\n          from sklearn import linear_model\n\n          df = pandas.read_csv(dataset_path)\n          model = linear_model.LinearRegression()\n          model.fit(\n              X=df.drop(columns=label_column_name),\n              y=df[label_column_name],\n          )\n\n          with open(model_path, \"wb\") as f:\n              pickle.dump(model, f)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Train linear regression model using scikit learn from CSV', description='Trains linear regression model using Scikit-learn.')\n      _parser.add_argument(\"--dataset\", dest=\"dataset_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = train_linear_regression_model_using_scikit_learn_from_CSV(**_parsed_args)\n    args:\n    - --dataset\n    - {inputPath: dataset}\n    - --label-column-name\n    - {inputValue: label_column_name}\n    - --model\n    - {outputPath: model}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/ML_frameworks/Scikit_learn/Train_linear_regression_model/from_CSV/component.yaml"
              },
              {
                "text": "name: Train logistic regression model using scikit learn from CSV\ndescription: Trains logistic regression model using Scikit-learn.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Scikit_learn/Train_logistic_regression_model/from_CSV/component.yaml'}\ninputs:\n- {name: dataset, type: CSV, description: Tabular dataset for training.}\n- {name: label_column_name, type: String, description: Name of the data column to\n    use as label.}\n- name: penalty\n  type: String\n  description: |-\n    Used to specify the norm used in the penalization.\n    Possible values: {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n    The 'newton-cg',\n    'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n    only supported by the 'saga' solver. If 'none' (not supported by the\n    liblinear solver), no regularization is applied.\n  default: l2\n  optional: true\n- name: solver\n  type: String\n  description: |-\n    Algorithm to use in the optimization problem.\n    Possible values: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'\n\n    - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n    'saga' are faster for large ones.\n    - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n    handle multinomial loss; 'liblinear' is limited to one-versus-rest\n    schemes.\n    - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n    - 'liblinear' and 'saga' also handle L1 penalty\n    - 'saga' also supports 'elasticnet' penalty\n    - 'liblinear' does not support setting ``penalty='none'``\n\n    Note that 'sag' and 'saga' fast convergence is only guaranteed on\n    features with approximately the same scale. You can\n    preprocess the data with a scaler from sklearn.preprocessing.\n  default: lbfgs\n  optional: true\n- {name: max_iterations, type: Integer, description: Maximum number of iterations\n    taken for the solvers to converge., default: '100', optional: true}\n- name: multi_class_mode\n  type: String\n  description: |-\n    Possible values: {'auto', 'ovr', 'multinomial'}, default='auto'\n    If the option chosen is 'ovr', then a binary problem is fit for each\n    label. For 'multinomial' the loss minimised is the multinomial loss fit\n    across the entire probability distribution, *even when the data is\n    binary*. 'multinomial' is unavailable when solver='liblinear'.\n    'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n    and otherwise selects 'multinomial'.\n  default: auto\n  optional: true\n- {name: random_seed, type: Integer, description: Controls the seed of the random\n    processes., default: '0', optional: true}\noutputs:\n- {name: model, type: ScikitLearnPickleModel, description: Trained model in Scikit-learn\n    pickle format.}\n- {name: model_parameters, type: JsonObject}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'scikit-learn==1.0.2' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'scikit-learn==1.0.2' 'pandas==1.4.3' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def train_logistic_regression_model_using_scikit_learn_from_CSV(\n          dataset_path,\n          model_path,\n          label_column_name,\n          penalty = \"l2\", # l1, l2, elasticnet, none\n          solver = \"lbfgs\", # newton-cg, lbfgs, liblinear, sag, saga\n          max_iterations = 100,\n          multi_class_mode = \"auto\", # auto, ovr, multinomial\n          random_seed = 0,\n      ):\n          \"\"\"Trains logistic regression model using Scikit-learn.\n\n          See https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n\n          Args:\n              dataset_path: Tabular dataset for training.\n              model_path: Trained model in Scikit-learn pickle format.\n              label_column_name: Name of the data column to use as label.\n              penalty: Used to specify the norm used in the penalization.\n                  Possible values: {'l1', 'l2', 'elasticnet', 'none'}, default='l2'\n                  The 'newton-cg',\n                  'sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' is\n                  only supported by the 'saga' solver. If 'none' (not supported by the\n                  liblinear solver), no regularization is applied.\n              solver: Algorithm to use in the optimization problem.\n                  Possible values: {'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'\n\n                  - For small datasets, 'liblinear' is a good choice, whereas 'sag' and\n                  'saga' are faster for large ones.\n                  - For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n                  handle multinomial loss; 'liblinear' is limited to one-versus-rest\n                  schemes.\n                  - 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty\n                  - 'liblinear' and 'saga' also handle L1 penalty\n                  - 'saga' also supports 'elasticnet' penalty\n                  - 'liblinear' does not support setting ``penalty='none'``\n\n                  Note that 'sag' and 'saga' fast convergence is only guaranteed on\n                  features with approximately the same scale. You can\n                  preprocess the data with a scaler from sklearn.preprocessing.\n              max_iterations: Maximum number of iterations taken for the solvers to converge.\n              multi_class_mode: Possible values: {'auto', 'ovr', 'multinomial'}, default='auto'\n                  If the option chosen is 'ovr', then a binary problem is fit for each\n                  label. For 'multinomial' the loss minimised is the multinomial loss fit\n                  across the entire probability distribution, *even when the data is\n                  binary*. 'multinomial' is unavailable when solver='liblinear'.\n                  'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n                  and otherwise selects 'multinomial'.\n              random_seed: Controls the seed of the random processes.\n          \"\"\"\n          import json\n          import pandas\n          import pickle\n          from sklearn import linear_model\n\n          df = pandas.read_csv(dataset_path)\n          model = linear_model.LogisticRegression(\n              penalty=penalty,\n              #dual=False,\n              #tol=1e-4,\n              #C=1.0,\n              #fit_intercept=True,\n              #intercept_scaling=1,\n              #class_weight=None,\n              random_state=random_seed,\n              solver=solver,\n              max_iter=max_iterations,\n              multi_class=multi_class_mode,\n              #l1_ratio=None,\n              verbose=1,\n          )\n\n          model_parameters = model.get_params()\n          model_parameters_json = json.dumps(model_parameters, indent=2)\n          print(\"Model parameters:\")\n          print(model_parameters_json)\n          print()\n\n          model.fit(\n              X=df.drop(columns=label_column_name),\n              y=df[label_column_name],\n          )\n\n          with open(model_path, \"wb\") as f:\n              pickle.dump(model, f)\n\n          return (model_parameters_json,)\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Train logistic regression model using scikit learn from CSV', description='Trains logistic regression model using Scikit-learn.')\n      _parser.add_argument(\"--dataset\", dest=\"dataset_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--penalty\", dest=\"penalty\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--solver\", dest=\"solver\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--max-iterations\", dest=\"max_iterations\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--multi-class-mode\", dest=\"multi_class_mode\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--random-seed\", dest=\"random_seed\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = train_logistic_regression_model_using_scikit_learn_from_CSV(**_parsed_args)\n\n      _output_serializers = [\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --dataset\n    - {inputPath: dataset}\n    - --label-column-name\n    - {inputValue: label_column_name}\n    - if:\n        cond: {isPresent: penalty}\n        then:\n        - --penalty\n        - {inputValue: penalty}\n    - if:\n        cond: {isPresent: solver}\n        then:\n        - --solver\n        - {inputValue: solver}\n    - if:\n        cond: {isPresent: max_iterations}\n        then:\n        - --max-iterations\n        - {inputValue: max_iterations}\n    - if:\n        cond: {isPresent: multi_class_mode}\n        then:\n        - --multi-class-mode\n        - {inputValue: multi_class_mode}\n    - if:\n        cond: {isPresent: random_seed}\n        then:\n        - --random-seed\n        - {inputValue: random_seed}\n    - --model\n    - {outputPath: model}\n    - '----output-paths'\n    - {outputPath: model_parameters}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/ML_frameworks/Scikit_learn/Train_logistic_regression_model/from_CSV/component.yaml"
              }
            ],
            "name": "Scikit learn"
          },
          {
            "components": [
              {
                "text": "name: Train XGBoost model on CSV\ndescription: Trains an XGBoost model.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train/component.yaml'}\ninputs:\n- {name: training_data, type: CSV, description: Training data in CSV format.}\n- {name: label_column_name, type: String, description: Name of the column containing\n    the label data.}\n- {name: starting_model, type: XGBoostModel, description: Existing trained model to\n    start from (in the binary XGBoost format)., optional: true}\n- {name: num_iterations, type: Integer, description: Number of boosting iterations.,\n  default: '10', optional: true}\n- name: objective\n  type: String\n  description: |-\n    The learning task and the corresponding learning objective.\n    See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n    The most common values are:\n    \"reg:squarederror\" - Regression with squared loss (default).\n    \"reg:logistic\" - Logistic regression.\n    \"binary:logistic\" - Logistic regression for binary classification, output probability.\n    \"binary:logitraw\" - Logistic regression for binary classification, output score before logistic transformation\n    \"rank:pairwise\" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n    \"rank:ndcg\" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n  default: reg:squarederror\n  optional: true\n- {name: booster, type: String, description: 'The booster to use. Can be `gbtree`,\n    `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear`\n    uses linear functions.', default: gbtree, optional: true}\n- {name: learning_rate, type: Float, description: 'Step size shrinkage used in update\n    to prevents overfitting. Range: [0,1].', default: '0.3', optional: true}\n- name: min_split_loss\n  type: Float\n  description: |-\n    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n    The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].\n  default: '0'\n  optional: true\n- name: max_depth\n  type: Integer\n  description: |-\n    Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n    0 indicates no limit on depth. Range: [0,Inf].\n  default: '6'\n  optional: true\n- {name: booster_params, type: JsonObject, description: 'Parameters for the booster.\n    See https://xgboost.readthedocs.io/en/latest/parameter.html', optional: true}\noutputs:\n- {name: model, type: XGBoostModel, description: Trained model in the binary XGBoost\n    format.}\n- {name: model_config, type: XGBoostModelConfig, description: The internal parameter\n    configuration of Booster as a JSON string.}\nimplementation:\n  container:\n    image: python:3.10\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def train_XGBoost_model_on_CSV(\n          training_data_path,\n          model_path,\n          model_config_path,\n          label_column_name,\n          starting_model_path = None,\n          num_iterations = 10,\n          # Booster parameters\n          objective = \"reg:squarederror\",\n          booster = \"gbtree\",\n          learning_rate = 0.3,\n          min_split_loss = 0,\n          max_depth = 6,\n          booster_params = None,\n      ):\n          \"\"\"Trains an XGBoost model.\n\n          Args:\n              training_data_path: Training data in CSV format.\n              model_path: Trained model in the binary XGBoost format.\n              model_config_path: The internal parameter configuration of Booster as a JSON string.\n              starting_model_path: Existing trained model to start from (in the binary XGBoost format).\n              label_column_name: Name of the column containing the label data.\n              num_iterations: Number of boosting iterations.\n              booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html\n              objective: The learning task and the corresponding learning objective.\n                  See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n                  The most common values are:\n                  \"reg:squarederror\" - Regression with squared loss (default).\n                  \"reg:logistic\" - Logistic regression.\n                  \"binary:logistic\" - Logistic regression for binary classification, output probability.\n                  \"binary:logitraw\" - Logistic regression for binary classification, output score before logistic transformation\n                  \"rank:pairwise\" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n                  \"rank:ndcg\" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n              booster: The booster to use. Can be `gbtree`, `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear` uses linear functions.\n              learning_rate: Step size shrinkage used in update to prevents overfitting. Range: [0,1].\n              min_split_loss: Minimum loss reduction required to make a further partition on a leaf node of the tree.\n                  The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].\n              max_depth: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n                  0 indicates no limit on depth. Range: [0,Inf].\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          \"\"\"\n          import pandas\n          import xgboost\n\n          df = pandas.read_csv(\n              training_data_path,\n          ).convert_dtypes()\n          print(\"Training data information:\")\n          df.info(verbose=True)\n          # Converting column types that XGBoost does not support\n          for column_name, dtype in df.dtypes.items():\n              if dtype in [\"string\", \"object\"]:\n                  print(f\"Treating the {dtype.name} column '{column_name}' as categorical.\")\n                  df[column_name] = df[column_name].astype(\"category\")\n                  print(f\"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.\")\n              # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213\n              if pandas.api.types.is_float_dtype(dtype):\n                  # Converting from \"Float64\" to \"float64\"\n                  df[column_name] = df[column_name].astype(dtype.name.lower())\n          print()\n          print(\"Final training data information:\")\n          df.info(verbose=True)\n\n          training_data = xgboost.DMatrix(\n              data=df.drop(columns=[label_column_name]),\n              label=df[[label_column_name]],\n              enable_categorical=True,\n          )\n\n          booster_params = booster_params or {}\n          booster_params.setdefault(\"objective\", objective)\n          booster_params.setdefault(\"booster\", booster)\n          booster_params.setdefault(\"learning_rate\", learning_rate)\n          booster_params.setdefault(\"min_split_loss\", min_split_loss)\n          booster_params.setdefault(\"max_depth\", max_depth)\n\n          starting_model = None\n          if starting_model_path:\n              starting_model = xgboost.Booster(model_file=starting_model_path)\n\n          print()\n          print(\"Training the model:\")\n          model = xgboost.train(\n              params=booster_params,\n              dtrain=training_data,\n              num_boost_round=num_iterations,\n              xgb_model=starting_model,\n              evals=[(training_data, \"training_data\")],\n          )\n\n          # Saving the model in binary format\n          model.save_model(model_path)\n\n          model_config_str = model.save_config()\n          with open(model_config_path, \"w\") as model_config_file:\n              model_config_file.write(model_config_str)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Train XGBoost model on CSV', description='Trains an XGBoost model.')\n      _parser.add_argument(\"--training-data\", dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--starting-model\", dest=\"starting_model_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--num-iterations\", dest=\"num_iterations\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--objective\", dest=\"objective\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--booster\", dest=\"booster\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--learning-rate\", dest=\"learning_rate\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--min-split-loss\", dest=\"min_split_loss\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--max-depth\", dest=\"max_depth\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--booster-params\", dest=\"booster_params\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model-config\", dest=\"model_config_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = train_XGBoost_model_on_CSV(**_parsed_args)\n    args:\n    - --training-data\n    - {inputPath: training_data}\n    - --label-column-name\n    - {inputValue: label_column_name}\n    - if:\n        cond: {isPresent: starting_model}\n        then:\n        - --starting-model\n        - {inputPath: starting_model}\n    - if:\n        cond: {isPresent: num_iterations}\n        then:\n        - --num-iterations\n        - {inputValue: num_iterations}\n    - if:\n        cond: {isPresent: objective}\n        then:\n        - --objective\n        - {inputValue: objective}\n    - if:\n        cond: {isPresent: booster}\n        then:\n        - --booster\n        - {inputValue: booster}\n    - if:\n        cond: {isPresent: learning_rate}\n        then:\n        - --learning-rate\n        - {inputValue: learning_rate}\n    - if:\n        cond: {isPresent: min_split_loss}\n        then:\n        - --min-split-loss\n        - {inputValue: min_split_loss}\n    - if:\n        cond: {isPresent: max_depth}\n        then:\n        - --max-depth\n        - {inputValue: max_depth}\n    - if:\n        cond: {isPresent: booster_params}\n        then:\n        - --booster-params\n        - {inputValue: booster_params}\n    - --model\n    - {outputPath: model}\n    - --model-config\n    - {outputPath: model_config}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Train/component.yaml"
              },
              {
                "text": "name: Xgboost predict on CSV\ndescription: Makes predictions using a trained XGBoost model.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Predict/component.yaml'}\ninputs:\n- {name: data, type: CSV, description: Feature data in Apache Parquet format.}\n- {name: model, type: XGBoostModel, description: Trained model in binary XGBoost format.}\n- {name: label_column_name, type: String, description: Optional. Name of the column\n    containing the label data that is excluded during the prediction., optional: true}\noutputs:\n- {name: predictions, description: Model predictions.}\nimplementation:\n  container:\n    image: python:3.10\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def xgboost_predict_on_CSV(\n          data_path,\n          model_path,\n          predictions_path,\n          label_column_name = None,\n      ):\n          \"\"\"Makes predictions using a trained XGBoost model.\n\n          Args:\n              data_path: Feature data in Apache Parquet format.\n              model_path: Trained model in binary XGBoost format.\n              predictions_path: Model predictions.\n              label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          \"\"\"\n          from pathlib import Path\n\n          import numpy\n          import pandas\n          import xgboost\n\n          df = pandas.read_csv(\n              data_path,\n          ).convert_dtypes()\n          print(\"Evaluation data information:\")\n          df.info(verbose=True)\n          # Converting column types that XGBoost does not support\n          for column_name, dtype in df.dtypes.items():\n              if dtype in [\"string\", \"object\"]:\n                  print(f\"Treating the {dtype.name} column '{column_name}' as categorical.\")\n                  df[column_name] = df[column_name].astype(\"category\")\n                  print(f\"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.\")\n              # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213\n              if pandas.api.types.is_float_dtype(dtype):\n                  # Converting from \"Float64\" to \"float64\"\n                  df[column_name] = df[column_name].astype(dtype.name.lower())\n          print(\"Final evaluation data information:\")\n          df.info(verbose=True)\n\n          if label_column_name is not None:\n              df = df.drop(columns=[label_column_name])\n\n          testing_data = xgboost.DMatrix(\n              data=df,\n              enable_categorical=True,\n          )\n\n          model = xgboost.Booster(model_file=model_path)\n\n          predictions = model.predict(testing_data)\n\n          Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)\n          numpy.savetxt(predictions_path, predictions)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Xgboost predict on CSV', description='Makes predictions using a trained XGBoost model.')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--predictions\", dest=\"predictions_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = xgboost_predict_on_CSV(**_parsed_args)\n    args:\n    - --data\n    - {inputPath: data}\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: label_column_name}\n        then:\n        - --label-column-name\n        - {inputValue: label_column_name}\n    - --predictions\n    - {outputPath: predictions}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Predict/component.yaml"
              },
              {
                "text": "name: Train XGBoost model on ApacheParquet\ndescription: Trains an XGBoost model.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Train/from_ApacheParquet/component.yaml'}\ninputs:\n- {name: training_data, type: ApacheParquet, description: Training data in the Apache\n    Parquet format.}\n- {name: label_column_name, type: String, description: Name of the column containing\n    the label data.}\n- {name: starting_model, type: XGBoostModel, description: Existing trained model to\n    start from (in the binary XGBoost format)., optional: true}\n- {name: num_iterations, type: Integer, description: Number of boosting iterations.,\n  default: '10', optional: true}\n- name: objective\n  type: String\n  description: |-\n    The learning task and the corresponding learning objective.\n    See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n    The most common values are:\n    \"reg:squarederror\" - Regression with squared loss (default).\n    \"reg:logistic\" - Logistic regression.\n    \"binary:logistic\" - Logistic regression for binary classification, output probability.\n    \"binary:logitraw\" - Logistic regression for binary classification, output score before logistic transformation\n    \"rank:pairwise\" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n    \"rank:ndcg\" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n  default: reg:squarederror\n  optional: true\n- {name: booster, type: String, description: 'The booster to use. Can be `gbtree`,\n    `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear`\n    uses linear functions.', default: gbtree, optional: true}\n- {name: learning_rate, type: Float, description: 'Step size shrinkage used in update\n    to prevents overfitting. Range: [0,1].', default: '0.3', optional: true}\n- name: min_split_loss\n  type: Float\n  description: |-\n    Minimum loss reduction required to make a further partition on a leaf node of the tree.\n    The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].\n  default: '0'\n  optional: true\n- name: max_depth\n  type: Integer\n  description: |-\n    Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n    0 indicates no limit on depth. Range: [0,Inf].\n  default: '6'\n  optional: true\n- {name: booster_params, type: JsonObject, description: 'Parameters for the booster.\n    See https://xgboost.readthedocs.io/en/latest/parameter.html', optional: true}\noutputs:\n- {name: model, type: XGBoostModel, description: Trained model in the binary XGBoost\n    format.}\n- {name: model_config, type: XGBoostModelConfig, description: The internal parameter\n    configuration of Booster as a JSON string.}\nimplementation:\n  container:\n    image: python:3.10\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'xgboost==1.6.1' 'pandas==1.4.3' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1\n      python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'\n      'pyarrow==9.0.0' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def train_XGBoost_model_on_ApacheParquet(\n          training_data_path,\n          model_path,\n          model_config_path,\n          label_column_name,\n          starting_model_path = None,\n          num_iterations = 10,\n          # Booster parameters\n          objective = \"reg:squarederror\",\n          booster = \"gbtree\",\n          learning_rate = 0.3,\n          min_split_loss = 0,\n          max_depth = 6,\n          booster_params = None,\n      ):\n          \"\"\"Trains an XGBoost model.\n\n          Args:\n              training_data_path: Training data in the Apache Parquet format.\n              model_path: Trained model in the binary XGBoost format.\n              model_config_path: The internal parameter configuration of Booster as a JSON string.\n              starting_model_path: Existing trained model to start from (in the binary XGBoost format).\n              label_column_name: Name of the column containing the label data.\n              num_iterations: Number of boosting iterations.\n              booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html\n              objective: The learning task and the corresponding learning objective.\n                  See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n                  The most common values are:\n                  \"reg:squarederror\" - Regression with squared loss (default).\n                  \"reg:logistic\" - Logistic regression.\n                  \"binary:logistic\" - Logistic regression for binary classification, output probability.\n                  \"binary:logitraw\" - Logistic regression for binary classification, output score before logistic transformation\n                  \"rank:pairwise\" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n                  \"rank:ndcg\" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n              booster: The booster to use. Can be `gbtree`, `gblinear` or `dart`; `gbtree` and `dart` use tree based models while `gblinear` uses linear functions.\n              learning_rate: Step size shrinkage used in update to prevents overfitting. Range: [0,1].\n              min_split_loss: Minimum loss reduction required to make a further partition on a leaf node of the tree.\n                  The larger `min_split_loss` is, the more conservative the algorithm will be. Range: [0,Inf].\n              max_depth: Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n                  0 indicates no limit on depth. Range: [0,Inf].\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          \"\"\"\n          import pandas\n          import xgboost\n\n          # Loading data\n          df = pandas.read_parquet(training_data_path)\n          print(\"Training data information:\")\n          df.info(verbose=True)\n          # Converting column types that XGBoost does not support\n          for column_name, dtype in df.dtypes.items():\n              if dtype in [\"string\", \"object\"]:\n                  print(f\"Treating the {dtype.name} column '{column_name}' as categorical.\")\n                  df[column_name] = df[column_name].astype(\"category\")\n                  print(f\"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.\")\n              # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213\n              if pandas.api.types.is_float_dtype(dtype):\n                  # Converting from \"Float64\" to \"float64\"\n                  df[column_name] = df[column_name].astype(dtype.name.lower())\n          print()\n          print(\"Final training data information:\")\n          df.info(verbose=True)\n\n          training_data = xgboost.DMatrix(\n              data=df.drop(columns=[label_column_name]),\n              label=df[[label_column_name]],\n              enable_categorical=True,\n          )\n          # Training\n          booster_params = booster_params or {}\n          booster_params.setdefault(\"objective\", objective)\n          booster_params.setdefault(\"booster\", booster)\n          booster_params.setdefault(\"learning_rate\", learning_rate)\n          booster_params.setdefault(\"min_split_loss\", min_split_loss)\n          booster_params.setdefault(\"max_depth\", max_depth)\n\n          starting_model = None\n          if starting_model_path:\n              starting_model = xgboost.Booster(model_file=starting_model_path)\n\n          print()\n          print(\"Training the model:\")\n          model = xgboost.train(\n              params=booster_params,\n              dtrain=training_data,\n              num_boost_round=num_iterations,\n              xgb_model=starting_model,\n              evals=[(training_data, \"training_data\")],\n          )\n\n          # Saving the model in binary format\n          model.save_model(model_path)\n\n          model_config_str = model.save_config()\n          with open(model_config_path, \"w\") as model_config_file:\n              model_config_file.write(model_config_str)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Train XGBoost model on ApacheParquet', description='Trains an XGBoost model.')\n      _parser.add_argument(\"--training-data\", dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--starting-model\", dest=\"starting_model_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--num-iterations\", dest=\"num_iterations\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--objective\", dest=\"objective\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--booster\", dest=\"booster\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--learning-rate\", dest=\"learning_rate\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--min-split-loss\", dest=\"min_split_loss\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--max-depth\", dest=\"max_depth\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--booster-params\", dest=\"booster_params\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model-config\", dest=\"model_config_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = train_XGBoost_model_on_ApacheParquet(**_parsed_args)\n    args:\n    - --training-data\n    - {inputPath: training_data}\n    - --label-column-name\n    - {inputValue: label_column_name}\n    - if:\n        cond: {isPresent: starting_model}\n        then:\n        - --starting-model\n        - {inputPath: starting_model}\n    - if:\n        cond: {isPresent: num_iterations}\n        then:\n        - --num-iterations\n        - {inputValue: num_iterations}\n    - if:\n        cond: {isPresent: objective}\n        then:\n        - --objective\n        - {inputValue: objective}\n    - if:\n        cond: {isPresent: booster}\n        then:\n        - --booster\n        - {inputValue: booster}\n    - if:\n        cond: {isPresent: learning_rate}\n        then:\n        - --learning-rate\n        - {inputValue: learning_rate}\n    - if:\n        cond: {isPresent: min_split_loss}\n        then:\n        - --min-split-loss\n        - {inputValue: min_split_loss}\n    - if:\n        cond: {isPresent: max_depth}\n        then:\n        - --max-depth\n        - {inputValue: max_depth}\n    - if:\n        cond: {isPresent: booster_params}\n        then:\n        - --booster-params\n        - {inputValue: booster_params}\n    - --model\n    - {outputPath: model}\n    - --model-config\n    - {outputPath: model_config}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Train/from_ApacheParquet/component.yaml"
              },
              {
                "text": "name: Xgboost predict on ApacheParquet\ndescription: Makes predictions using a trained XGBoost model.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Predict/from_ApacheParquet/component.yaml'}\ninputs:\n- {name: data, type: ApacheParquet, description: Feature data in Apache Parquet format.}\n- {name: model, type: XGBoostModel, description: Trained model in binary XGBoost format.}\n- {name: label_column_name, type: String, description: Optional. Name of the column\n    containing the label data that is excluded during the prediction., optional: true}\noutputs:\n- {name: predictions, description: Model predictions.}\nimplementation:\n  container:\n    image: python:3.10\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'xgboost==1.6.1' 'pandas==1.4.3' 'pyarrow==9.0.0' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1\n      python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.6.1' 'pandas==1.4.3' 'numpy<2'\n      'pyarrow==9.0.0' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def xgboost_predict_on_ApacheParquet(\n          data_path,\n          model_path,\n          predictions_path,\n          label_column_name = None,\n      ):\n          \"\"\"Makes predictions using a trained XGBoost model.\n\n          Args:\n              data_path: Feature data in Apache Parquet format.\n              model_path: Trained model in binary XGBoost format.\n              predictions_path: Model predictions.\n              label_column_name: Optional. Name of the column containing the label data that is excluded during the prediction.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          \"\"\"\n          from pathlib import Path\n\n          import numpy\n          import pandas\n          import xgboost\n\n          # Loading data\n          df = pandas.read_parquet(data_path)\n          print(\"Evaluation data information:\")\n          df.info(verbose=True)\n          # Converting column types that XGBoost does not support\n          for column_name, dtype in df.dtypes.items():\n              if dtype in [\"string\", \"object\"]:\n                  print(f\"Treating the {dtype.name} column '{column_name}' as categorical.\")\n                  df[column_name] = df[column_name].astype(\"category\")\n                  print(f\"Inferred {len(df[column_name].cat.categories)} categories for the '{column_name}' column.\")\n              # Working around the XGBoost issue with nullable floats: https://github.com/dmlc/xgboost/issues/8213\n              if pandas.api.types.is_float_dtype(dtype):\n                  # Converting from \"Float64\" to \"float64\"\n                  df[column_name] = df[column_name].astype(dtype.name.lower())\n          print(\"Final evaluation data information:\")\n          df.info(verbose=True)\n\n          if label_column_name:\n              df = df.drop(columns=[label_column_name])\n\n          evaluation_data = xgboost.DMatrix(\n              data=df,\n              enable_categorical=True,\n          )\n\n          # Training\n          model = xgboost.Booster(model_file=model_path)\n\n          predictions = model.predict(evaluation_data)\n\n          Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)\n          numpy.savetxt(predictions_path, predictions)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Xgboost predict on ApacheParquet', description='Makes predictions using a trained XGBoost model.')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--predictions\", dest=\"predictions_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = xgboost_predict_on_ApacheParquet(**_parsed_args)\n    args:\n    - --data\n    - {inputPath: data}\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: label_column_name}\n        then:\n        - --label-column-name\n        - {inputValue: label_column_name}\n    - --predictions\n    - {outputPath: predictions}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/XGBoost/Predict/from_ApacheParquet/component.yaml"
              }
            ],
            "name": "XGBoost"
          },
          {
            "components": [
              {
                "text": "name: Create fully connected pytorch network\ndescription: Creates fully-connected network in PyTorch ScriptModule format\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Create_fully_connected_network/component.yaml'\ninputs:\n- {name: layer_sizes, type: JsonArray}\n- {name: activation_name, type: String, default: relu, optional: true}\n- {name: random_seed, type: Integer, default: '0', optional: true}\noutputs:\n- {name: network, type: PyTorchScriptModule}\nimplementation:\n  container:\n    image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def create_fully_connected_pytorch_network(\n          layer_sizes,\n          network_path,\n          activation_name = 'relu',\n          random_seed = 0,\n      ):\n          '''Creates fully-connected network in PyTorch ScriptModule format'''\n          import torch\n          torch.manual_seed(random_seed)\n\n          activation = getattr(torch, activation_name, None) or getattr(torch.nn.functional, activation_name, None)\n          if not activation:\n              raise ValueError(f'Activation \"{activation_name}\" was not found.')\n\n          class ActivationLayer(torch.nn.Module):\n              def forward(self, input):\n                  return activation(input)\n\n          layers = []\n          for layer_idx in range(len(layer_sizes) - 1):\n              layer = torch.nn.Linear(layer_sizes[layer_idx], layer_sizes[layer_idx + 1])\n              layers.append(layer)\n              if layer_idx < len(layer_sizes) - 2:\n                  layers.append(ActivationLayer())\n\n          network = torch.nn.Sequential(*layers)\n          script_module = torch.jit.script(network)\n          print(script_module)\n          script_module.save(network_path)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Create fully connected pytorch network', description='Creates fully-connected network in PyTorch ScriptModule format')\n      _parser.add_argument(\"--layer-sizes\", dest=\"layer_sizes\", type=json.loads, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--activation-name\", dest=\"activation_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--random-seed\", dest=\"random_seed\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--network\", dest=\"network_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = create_fully_connected_pytorch_network(**_parsed_args)\n    args:\n    - --layer-sizes\n    - {inputValue: layer_sizes}\n    - if:\n        cond: {isPresent: activation_name}\n        then:\n        - --activation-name\n        - {inputValue: activation_name}\n    - if:\n        cond: {isPresent: random_seed}\n        then:\n        - --random-seed\n        - {inputValue: random_seed}\n    - --network\n    - {outputPath: network}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/PyTorch/Create_fully_connected_network/component.yaml"
              },
              {
                "text": "name: Train pytorch model from csv\ndescription: Trains PyTorch model.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Train_PyTorch_model/from_CSV/component.yaml'}\ninputs:\n- {name: model, type: PyTorchScriptModule, description: Model in PyTorch format.}\n- {name: training_data, type: CSV, description: Tabular dataset for training.}\n- {name: label_column_name, type: String, description: Name of the table column to\n    use as label.}\n- {name: loss_function_name, type: String, description: Name of the loss function.,\n  default: mse_loss, optional: true}\n- {name: number_of_epochs, type: Integer, description: Number of training epochs.,\n  default: '1', optional: true}\n- {name: learning_rate, type: Float, description: Learning rate of the optimizer.,\n  default: '0.1', optional: true}\n- {name: optimizer_name, type: String, description: Name of the optimizer., default: Adadelta,\n  optional: true}\n- {name: optimizer_parameters, type: JsonObject, description: Optimizer parameters\n    in dictionary form., optional: true}\n- {name: batch_size, type: Integer, description: Number of training samples to use\n    in each batch., default: '32', optional: true}\n- {name: batch_log_interval, type: Integer, description: Print training summary after\n    every N batches., default: '100', optional: true}\n- {name: random_seed, type: Integer, description: Controls the seed of the random\n    processes., default: '0', optional: true}\noutputs:\n- {name: trained_model, type: PyTorchScriptModule, description: Trained model in PyTorch\n    format.}\nimplementation:\n  container:\n    image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def train_pytorch_model_from_csv(\n          model_path,\n          training_data_path,\n          trained_model_path,\n          label_column_name,\n          loss_function_name = 'mse_loss',\n          number_of_epochs = 1,\n          learning_rate = 0.1,\n          optimizer_name = 'Adadelta',\n          optimizer_parameters = None,\n          batch_size = 32,\n          batch_log_interval = 100,\n          random_seed = 0,\n      ):\n          \"\"\"Trains PyTorch model.\n\n          Args:\n              model_path: Model in PyTorch format.\n              training_data_path: Tabular dataset for training.\n              trained_model_path: Trained model in PyTorch format.\n              label_column_name: Name of the table column to use as label.\n              loss_function_name: Name of the loss function.\n              number_of_epochs: Number of training epochs.\n              learning_rate: Learning rate of the optimizer.\n              optimizer_name: Name of the optimizer.\n              optimizer_parameters: Optimizer parameters in dictionary form.\n              batch_size: Number of training samples to use in each batch.\n              batch_log_interval: Print training summary after every N batches.\n              random_seed: Controls the seed of the random processes.\n          \"\"\"\n          import pandas\n          import torch\n\n          torch.manual_seed(random_seed)\n\n          use_cuda = torch.cuda.is_available()\n          device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n          model = torch.jit.load(model_path)\n          model.to(device)\n          model.train()\n\n          optimizer_class = getattr(torch.optim, optimizer_name, None)\n          if not optimizer_class:\n              raise ValueError(f'Optimizer \"{optimizer_name}\" was not found.')\n\n          optimizer_parameters = optimizer_parameters or {}\n          optimizer_parameters['lr'] = learning_rate\n          optimizer = optimizer_class(model.parameters(), **optimizer_parameters)\n\n          loss_function = getattr(torch, loss_function_name, None) or getattr(torch.nn, loss_function_name, None) or getattr(torch.nn.functional, loss_function_name, None)\n          if not loss_function:\n              raise ValueError(f'Loss function \"{loss_function_name}\" was not found.')\n\n          class CsvDataset(torch.utils.data.Dataset):\n\n              def __init__(self, file_path, label_column_name, drop_nan_columns_or_rows = 'columns'):\n                  dataframe = pandas.read_csv(file_path).convert_dtypes()\n                  # Preventing error: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object\n                  if drop_nan_columns_or_rows == 'columns':\n                      non_nan_data = dataframe.dropna(axis='columns')\n                      removed_columns = set(dataframe.columns) - set(non_nan_data.columns)\n                      if removed_columns:\n                          print('Skipping columns with NaNs: ' + str(removed_columns))\n                      dataframe = non_nan_data\n                  if drop_nan_columns_or_rows == 'rows':\n                      non_nan_data = dataframe.dropna(axis='index')\n                      number_of_removed_rows = len(dataframe) - len(non_nan_data)\n                      if number_of_removed_rows:\n                          print(f'Skipped {number_of_removed_rows} rows with NaNs.')\n                      dataframe = non_nan_data\n                  numerical_data = dataframe.select_dtypes(include='number')\n                  non_numerical_data = dataframe.select_dtypes(exclude='number')\n                  if not non_numerical_data.empty:\n                      print('Skipping non-number columns:')\n                      print(non_numerical_data.dtypes)\n                  self._dataframe = dataframe\n                  self.labels = numerical_data[[label_column_name]]\n                  self.features = numerical_data.drop(columns=[label_column_name])\n\n              def __len__(self):\n                  return len(self._dataframe)\n\n              def __getitem__(self, index):\n                  return [self.features.loc[index].to_numpy(dtype='float32'), self.labels.loc[index].to_numpy(dtype='float32')]\n\n          dataset = CsvDataset(\n              file_path=training_data_path,\n              label_column_name=label_column_name,\n          )\n          train_loader = torch.utils.data.DataLoader(\n              dataset=dataset,\n              batch_size=batch_size,\n              shuffle=True,\n          )\n\n          last_full_batch_loss = None\n          for epoch in range(1, number_of_epochs + 1):\n              for batch_idx, (data, target) in enumerate(train_loader):\n                  data, target = data.to(device), target.to(device)\n                  optimizer.zero_grad()\n                  output = model(data)\n                  loss = loss_function(output, target)\n                  loss.backward()\n                  optimizer.step()\n                  if len(data) == batch_size:\n                      last_full_batch_loss = loss.item()\n                  if batch_idx % batch_log_interval == 0:\n                      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                          epoch, batch_idx * len(data), len(train_loader.dataset),\n                          100. * batch_idx / len(train_loader), loss.item()))\n              print(f'Training epoch {epoch} completed. Last full batch loss: {last_full_batch_loss:.6f}')\n\n          # print(optimizer.state_dict())\n          model.save(trained_model_path)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Train pytorch model from csv', description='Trains PyTorch model.')\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--training-data\", dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--loss-function-name\", dest=\"loss_function_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--number-of-epochs\", dest=\"number_of_epochs\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--learning-rate\", dest=\"learning_rate\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--optimizer-name\", dest=\"optimizer_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--optimizer-parameters\", dest=\"optimizer_parameters\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--batch-size\", dest=\"batch_size\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--batch-log-interval\", dest=\"batch_log_interval\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--random-seed\", dest=\"random_seed\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--trained-model\", dest=\"trained_model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = train_pytorch_model_from_csv(**_parsed_args)\n    args:\n    - --model\n    - {inputPath: model}\n    - --training-data\n    - {inputPath: training_data}\n    - --label-column-name\n    - {inputValue: label_column_name}\n    - if:\n        cond: {isPresent: loss_function_name}\n        then:\n        - --loss-function-name\n        - {inputValue: loss_function_name}\n    - if:\n        cond: {isPresent: number_of_epochs}\n        then:\n        - --number-of-epochs\n        - {inputValue: number_of_epochs}\n    - if:\n        cond: {isPresent: learning_rate}\n        then:\n        - --learning-rate\n        - {inputValue: learning_rate}\n    - if:\n        cond: {isPresent: optimizer_name}\n        then:\n        - --optimizer-name\n        - {inputValue: optimizer_name}\n    - if:\n        cond: {isPresent: optimizer_parameters}\n        then:\n        - --optimizer-parameters\n        - {inputValue: optimizer_parameters}\n    - if:\n        cond: {isPresent: batch_size}\n        then:\n        - --batch-size\n        - {inputValue: batch_size}\n    - if:\n        cond: {isPresent: batch_log_interval}\n        then:\n        - --batch-log-interval\n        - {inputValue: batch_log_interval}\n    - if:\n        cond: {isPresent: random_seed}\n        then:\n        - --random-seed\n        - {inputValue: random_seed}\n    - --trained-model\n    - {outputPath: trained_model}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/PyTorch/Train_PyTorch_model/from_CSV/component.yaml"
              },
              {
                "text": "name: Convert to onnx from pytorch script module\ndescription: Creates fully-connected network in PyTorch ScriptModule format\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Convert_to_OnnxModel_from_PyTorchScriptModule/component.yaml'\ninputs:\n- {name: model, type: PyTorchScriptModule}\n- {name: list_of_input_shapes, type: JsonArray}\noutputs:\n- {name: converted_model, type: OnnxModel}\nimplementation:\n  container:\n    image: pytorch/pytorch:1.7.1-cuda11.0-cudnn8-runtime\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_to_onnx_from_pytorch_script_module(\n          model_path,\n          converted_model_path,\n          list_of_input_shapes,\n      ):\n          '''Creates fully-connected network in PyTorch ScriptModule format'''\n          import torch\n          model = torch.jit.load(model_path)\n          example_inputs = [\n              torch.ones(*input_shape)\n              for input_shape in list_of_input_shapes\n          ]\n          example_outputs = model.forward(*example_inputs)\n          torch.onnx.export(\n              model=model,\n              args=example_inputs,\n              f=converted_model_path,\n              verbose=True,\n              training=torch.onnx.TrainingMode.EVAL,\n              example_outputs=example_outputs,\n          )\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert to onnx from pytorch script module', description='Creates fully-connected network in PyTorch ScriptModule format')\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--list-of-input-shapes\", dest=\"list_of_input_shapes\", type=json.loads, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--converted-model\", dest=\"converted_model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = convert_to_onnx_from_pytorch_script_module(**_parsed_args)\n    args:\n    - --model\n    - {inputPath: model}\n    - --list-of-input-shapes\n    - {inputValue: list_of_input_shapes}\n    - --converted-model\n    - {outputPath: converted_model}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/PyTorch/Convert_to_OnnxModel_from_PyTorchScriptModule/component.yaml"
              },
              {
                "text": "name: Create PyTorch Model Archive with base handler\ninputs:\n- {name: Model, type: PyTorchScriptModule}\n- {name: Model name, type: String, default: model}\n- {name: Model version, type: String, default: \"1.0\"}\noutputs:\n- {name: Model archive, type: PyTorchModelArchive}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/PyTorch/Create_PyTorch_Model_Archive/with_base_handler/component.yaml'\nimplementation:\n  container:\n    image: pytorch/torchserve:0.6.0-cpu\n    command:\n    - bash\n    - -exc\n    - |\n      model_path=$0\n      model_name=$1\n      model_version=$2\n      output_model_archive_path=$3\n\n      mkdir -p \"$(dirname \"$output_model_archive_path\")\"\n\n      # TODO: Use the built-in base_handler once my fix is merged: https://github.com/pytorch/serve/pull/1682\n      echo '\n      from ts.torch_handler import base_handler\n      class BaseHandler(base_handler.BaseHandler):\n          pass\n      ' > base_handler.py  # torch-model-archiver needs the handler to have .py extension\n      torch-model-archiver --model-name \"$model_name\" --version \"$model_version\" --serialized-file \"$model_path\" --handler base_handler.py\n\n      # torch-model-archiver does not allow specifying the output path, but always writes to \"${model_name}.<format>\"\n      expected_model_archive_path=\"${model_name}.mar\"\n      mv \"$expected_model_archive_path\" \"$output_model_archive_path\"\n\n    - {inputPath: Model}\n    - {inputValue: Model name}\n    - {inputValue: Model version}\n    - {outputPath: Model archive}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/46d51383e6554b7f3ab4fd8cf614d8c2b422fb22/components/PyTorch/Create_PyTorch_Model_Archive/with_base_handler/component.yaml"
              }
            ],
            "name": "PyTorch"
          },
          {
            "components": [
              {
                "text": "name: Create fully connected tensorflow network\ndescription: Creates fully-connected network in Tensorflow SavedModel format\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Create_fully_connected_network/component.yaml'}\ninputs:\n- {name: layer_sizes, type: JsonArray}\n- {name: activation_name, type: String, default: relu, optional: true}\n- {name: random_seed, type: Integer, default: '0', optional: true}\noutputs:\n- {name: model, type: TensorflowSavedModel}\nimplementation:\n  container:\n    image: tensorflow/tensorflow:2.7.0\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def create_fully_connected_tensorflow_network(\n          layer_sizes,\n          model_path,\n          activation_name = \"relu\",\n          random_seed = 0,\n      ):\n          \"\"\"Creates fully-connected network in Tensorflow SavedModel format\"\"\"\n          import tensorflow as tf\n          tf.random.set_seed(seed=random_seed)\n\n          if len(layer_sizes) < 2:\n              raise ValueError(f\"Fully-connected network requires at least two layer sizes (input and output). Got {layer_sizes}.\")\n\n          model = tf.keras.models.Sequential()\n          model.add(tf.keras.Input(shape=(layer_sizes[0],)))\n          for layer_size in layer_sizes[1:-1]:\n              model.add(tf.keras.layers.Dense(units=layer_size, activation=activation_name))\n          # The last layer is left without activation\n          model.add(tf.keras.layers.Dense(units=layer_sizes[-1]))\n\n          # Using tf.keras.models.save_model instead of tf.saved_model.save to prevent downstream error:\n          #tf.saved_model.save(model, model_path)\n          # ValueError: Unable to create a Keras model from this SavedModel.\n          # This SavedModel was created with `tf.saved_model.save`, and lacks the Keras metadata.\n          # Please save your Keras model by calling `model.save`or `tf.keras.models.save_model`.\n          # See https://github.com/keras-team/keras/issues/16451\n          tf.keras.models.save_model(model, model_path)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Create fully connected tensorflow network', description='Creates fully-connected network in Tensorflow SavedModel format')\n      _parser.add_argument(\"--layer-sizes\", dest=\"layer_sizes\", type=json.loads, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--activation-name\", dest=\"activation_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--random-seed\", dest=\"random_seed\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = create_fully_connected_tensorflow_network(**_parsed_args)\n    args:\n    - --layer-sizes\n    - {inputValue: layer_sizes}\n    - if:\n        cond: {isPresent: activation_name}\n        then:\n        - --activation-name\n        - {inputValue: activation_name}\n    - if:\n        cond: {isPresent: random_seed}\n        then:\n        - --random-seed\n        - {inputValue: random_seed}\n    - --model\n    - {outputPath: model}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/f3a9769d35a057c31a498e0667cae2e4a830c5b0/components/tensorflow/Create_fully_connected_network/component.yaml"
              },
              {
                "text": "name: Train model using Keras on CSV\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Train_model_using_Keras/on_CSV/component.yaml'}\ninputs:\n- {name: training_data, type: CSV}\n- {name: model, type: TensorflowSavedModel}\n- {name: label_column_name, type: String}\n- {name: loss_function_name, type: String, default: mean_squared_error, optional: true}\n- {name: number_of_epochs, type: Integer, default: '1', optional: true}\n- {name: learning_rate, type: Float, default: '0.1', optional: true}\n- {name: optimizer_name, type: String, default: Adadelta, optional: true}\n- {name: optimizer_parameters, type: JsonObject, optional: true}\n- {name: batch_size, type: Integer, default: '32', optional: true}\n- {name: metric_names, type: JsonArray, optional: true}\n- {name: random_seed, type: Integer, default: '0', optional: true}\noutputs:\n- {name: trained_model, type: TensorflowSavedModel}\nimplementation:\n  container:\n    image: tensorflow/tensorflow:2.8.0\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def train_model_using_Keras_on_CSV(\n          training_data_path,\n          model_path,\n          trained_model_path,\n          label_column_name,\n          loss_function_name = \"mean_squared_error\",\n          number_of_epochs = 1,\n          learning_rate = 0.1,\n          optimizer_name = \"Adadelta\",\n          optimizer_parameters = None,\n          batch_size = 32,\n          metric_names = None,\n          random_seed = 0,\n      ):\n          import tensorflow as tf\n          tf.random.set_seed(seed=random_seed)\n\n          # Loading model using Keras. Model loaded using TensorFlow does not have .fit.\n          #model = tf.saved_model.load(export_dir=model_path)\n          keras_model = tf.keras.models.load_model(filepath=model_path)\n\n          optimizer_parameters = optimizer_parameters or {}\n          optimizer_parameters[\"learning_rate\"] = learning_rate\n          optimizer_config = {\n              \"class_name\": optimizer_name,\n              \"config\": optimizer_parameters,\n          }\n          optimizer = tf.keras.optimizers.get(optimizer_config)\n          loss = tf.keras.losses.get(loss_function_name)\n\n          training_dataset = tf.data.experimental.make_csv_dataset(\n              file_pattern=training_data_path,\n              batch_size=batch_size,\n              label_name=label_column_name,\n              header=True,\n              # Need to specify num_epochs=1 otherwise the training becomes infinite\n              num_epochs=1,\n              shuffle=True,\n              shuffle_seed=random_seed,\n              ignore_errors=True,\n          )\n          def stack_feature_batches(features_batch, labels_batch):\n              # Need to stack individual feature columns to create a single feature tensor\n              # Need to cast all column tensor types to float to prevent error:\n              # TypeError: Tensors in list passed to 'values' of 'Pack' Op have types [int32, float32, float32, int32, int32] that don't all match.\n              list_of_feature_batches = list(tf.cast(x=feature_batch, dtype=tf.float32) for feature_batch in features_batch.values())\n              return tf.stack(list_of_feature_batches, axis=-1), labels_batch\n\n          training_dataset = training_dataset.map(stack_feature_batches)\n\n          # Need to compile the model to prevent error:\n          # ValueError: No gradients provided for any variable: [..., ...].\n          keras_model.compile(\n              optimizer=optimizer,\n              loss=loss,\n              metrics=metric_names,\n          )\n          keras_model.fit(\n              training_dataset,\n              epochs=number_of_epochs,\n          )\n\n          # Using tf.keras.models.save_model instead of tf.saved_model.save to prevent downstream error:\n          #tf.saved_model.save(keras_model, trained_model_path)\n          # ValueError: Unable to create a Keras model from this SavedModel.\n          # This SavedModel was created with `tf.saved_model.save`, and lacks the Keras metadata.\n          # Please save your Keras model by calling `model.save`or `tf.keras.models.save_model`.\n          # See https://github.com/keras-team/keras/issues/16451\n          tf.keras.models.save_model(keras_model, trained_model_path)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Train model using Keras on CSV', description='')\n      _parser.add_argument(\"--training-data\", dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--loss-function-name\", dest=\"loss_function_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--number-of-epochs\", dest=\"number_of_epochs\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--learning-rate\", dest=\"learning_rate\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--optimizer-name\", dest=\"optimizer_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--optimizer-parameters\", dest=\"optimizer_parameters\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--batch-size\", dest=\"batch_size\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--metric-names\", dest=\"metric_names\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--random-seed\", dest=\"random_seed\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--trained-model\", dest=\"trained_model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = train_model_using_Keras_on_CSV(**_parsed_args)\n    args:\n    - --training-data\n    - {inputPath: training_data}\n    - --model\n    - {inputPath: model}\n    - --label-column-name\n    - {inputValue: label_column_name}\n    - if:\n        cond: {isPresent: loss_function_name}\n        then:\n        - --loss-function-name\n        - {inputValue: loss_function_name}\n    - if:\n        cond: {isPresent: number_of_epochs}\n        then:\n        - --number-of-epochs\n        - {inputValue: number_of_epochs}\n    - if:\n        cond: {isPresent: learning_rate}\n        then:\n        - --learning-rate\n        - {inputValue: learning_rate}\n    - if:\n        cond: {isPresent: optimizer_name}\n        then:\n        - --optimizer-name\n        - {inputValue: optimizer_name}\n    - if:\n        cond: {isPresent: optimizer_parameters}\n        then:\n        - --optimizer-parameters\n        - {inputValue: optimizer_parameters}\n    - if:\n        cond: {isPresent: batch_size}\n        then:\n        - --batch-size\n        - {inputValue: batch_size}\n    - if:\n        cond: {isPresent: metric_names}\n        then:\n        - --metric-names\n        - {inputValue: metric_names}\n    - if:\n        cond: {isPresent: random_seed}\n        then:\n        - --random-seed\n        - {inputValue: random_seed}\n    - --trained-model\n    - {outputPath: trained_model}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/c504a4010348c50eaaf6d4337586ccc008f4dcef/components/tensorflow/Train_model_using_Keras/on_CSV/component.yaml"
              },
              {
                "text": "name: Train model using Keras on ApacheParquet\ndescription: Trains TensorFlow model.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Train_model_using_Keras/on_ApacheParquet/component.yaml'}\ninputs:\n- {name: training_data, type: ApacheParquet, description: Tabular dataset for training.}\n- {name: model, type: TensorflowSavedModel, description: Model in TensorFlow format.}\n- {name: label_column_name, type: String, description: Name of the table column to\n    use as label.}\n- {name: loss_function_name, type: String, description: Name of the loss function.,\n  default: mean_squared_error, optional: true}\n- {name: number_of_epochs, type: Integer, description: Number of training epochs.,\n  default: '1', optional: true}\n- {name: learning_rate, type: Float, description: Learning rate of the optimizer.,\n  default: '0.1', optional: true}\n- {name: optimizer_name, type: String, description: Name of the optimizer., default: Adadelta,\n  optional: true}\n- {name: optimizer_parameters, type: JsonObject, description: Optimizer parameters\n    in dictionary form., optional: true}\n- {name: batch_size, type: Integer, description: Number of training samples to use\n    in each batch., default: '32', optional: true}\n- {name: metric_names, type: JsonArray, description: A list of metrics to evaluate\n    during the training., optional: true}\n- {name: random_seed, type: Integer, description: Controls the seed of the random\n    processes., default: '0', optional: true}\noutputs:\n- {name: trained_model, type: TensorflowSavedModel, description: Trained model in\n    TensorFlow format.}\nimplementation:\n  container:\n    image: tensorflow/tensorflow:2.8.0\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'tensorflow-io==0.25.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install\n      --quiet --no-warn-script-location 'tensorflow-io==0.25.0' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def train_model_using_Keras_on_ApacheParquet(\n          training_data_path,\n          model_path,\n          trained_model_path,\n          label_column_name,\n          loss_function_name = \"mean_squared_error\",\n          number_of_epochs = 1,\n          learning_rate = 0.1,\n          optimizer_name = \"Adadelta\",\n          optimizer_parameters = None,\n          batch_size = 32,\n          metric_names = None,\n          random_seed = 0,\n      ):\n          \"\"\"Trains TensorFlow model.\n\n          Args:\n              training_data_path: Tabular dataset for training.\n              model_path: Model in TensorFlow format.\n              trained_model_path: Trained model in TensorFlow format.\n              label_column_name: Name of the table column to use as label.\n              loss_function_name: Name of the loss function.\n              number_of_epochs: Number of training epochs.\n              learning_rate: Learning rate of the optimizer.\n              optimizer_name: Name of the optimizer.\n              optimizer_parameters: Optimizer parameters in dictionary form.\n              batch_size: Number of training samples to use in each batch.\n              metric_names: A list of metrics to evaluate during the training.\n              random_seed: Controls the seed of the random processes.\n          \"\"\"\n          import tensorflow as tf\n          import tensorflow_io as tfio\n\n          tf.random.set_seed(seed=random_seed)\n\n          FEATURES_COLUMN_NAME = \"features\"\n          SHUFFLE_BUFFER_SIZE = 10000\n\n          # Loading model using Keras. Model loaded using TensorFlow does not have .fit.\n          # model = tf.saved_model.load(export_dir=model_path)\n          keras_model = tf.keras.models.load_model(filepath=model_path)\n\n          optimizer_parameters = optimizer_parameters or {}\n          optimizer_parameters[\"learning_rate\"] = learning_rate\n          optimizer_config = {\n              \"class_name\": optimizer_name,\n              \"config\": optimizer_parameters,\n          }\n          optimizer = tf.keras.optimizers.get(optimizer_config)\n          loss = tf.keras.losses.get(loss_function_name)\n\n          def stack_feature_batches(columns_batch_dict):\n              label_batch = columns_batch_dict.pop(label_column_name.encode())\n              if FEATURES_COLUMN_NAME in columns_batch_dict:\n                  features_batch = columns_batch_dict[FEATURES_COLUMN_NAME]\n              else:\n                  # Need to stack individual feature columns to create a single feature tensor\n                  # Need to cast all column tensor types to float\n                  list_of_feature_batches = list(\n                      tf.cast(x=feature_batch, dtype=tf.float32)\n                      for feature_batch in columns_batch_dict.values()\n                  )\n                  features_batch = tf.stack(list_of_feature_batches, axis=-1)\n              return features_batch, label_batch\n\n          # ! parquet::ParquetException is thrown if the Parquet dataset has nulls values\n          training_dataset = tfio.IODataset.from_parquet(filename=training_data_path)\n\n          training_dataset = (\n              training_dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE, seed=random_seed)\n              .repeat(count=number_of_epochs)\n              .batch(\n                  batch_size=batch_size,\n                  num_parallel_calls=tf.data.AUTOTUNE,\n                  deterministic=True,\n              )\n              .map(\n                  map_func=stack_feature_batches,\n                  num_parallel_calls=tf.data.AUTOTUNE,\n                  deterministic=True,\n              )\n          )\n\n          # Need to compile the model to prevent error:\n          # ValueError: No gradients provided for any variable: [..., ...].\n          keras_model.compile(\n              optimizer=optimizer,\n              loss=loss,\n              metrics=metric_names,\n          )\n          keras_model.fit(\n              training_dataset,\n              epochs=number_of_epochs,\n          )\n\n          # Using tf.keras.models.save_model instead of tf.saved_model.save to prevent downstream error:\n          # tf.saved_model.save(keras_model, trained_model_path)\n          # ValueError: Unable to create a Keras model from this SavedModel.\n          # This SavedModel was created with `tf.saved_model.save`, and lacks the Keras metadata.\n          # Please save your Keras model by calling `model.save`or `tf.keras.models.save_model`.\n          # See https://github.com/keras-team/keras/issues/16451\n          tf.keras.models.save_model(keras_model, trained_model_path)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Train model using Keras on ApacheParquet', description='Trains TensorFlow model.')\n      _parser.add_argument(\"--training-data\", dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--loss-function-name\", dest=\"loss_function_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--number-of-epochs\", dest=\"number_of_epochs\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--learning-rate\", dest=\"learning_rate\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--optimizer-name\", dest=\"optimizer_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--optimizer-parameters\", dest=\"optimizer_parameters\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--batch-size\", dest=\"batch_size\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--metric-names\", dest=\"metric_names\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--random-seed\", dest=\"random_seed\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--trained-model\", dest=\"trained_model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = train_model_using_Keras_on_ApacheParquet(**_parsed_args)\n    args:\n    - --training-data\n    - {inputPath: training_data}\n    - --model\n    - {inputPath: model}\n    - --label-column-name\n    - {inputValue: label_column_name}\n    - if:\n        cond: {isPresent: loss_function_name}\n        then:\n        - --loss-function-name\n        - {inputValue: loss_function_name}\n    - if:\n        cond: {isPresent: number_of_epochs}\n        then:\n        - --number-of-epochs\n        - {inputValue: number_of_epochs}\n    - if:\n        cond: {isPresent: learning_rate}\n        then:\n        - --learning-rate\n        - {inputValue: learning_rate}\n    - if:\n        cond: {isPresent: optimizer_name}\n        then:\n        - --optimizer-name\n        - {inputValue: optimizer_name}\n    - if:\n        cond: {isPresent: optimizer_parameters}\n        then:\n        - --optimizer-parameters\n        - {inputValue: optimizer_parameters}\n    - if:\n        cond: {isPresent: batch_size}\n        then:\n        - --batch-size\n        - {inputValue: batch_size}\n    - if:\n        cond: {isPresent: metric_names}\n        then:\n        - --metric-names\n        - {inputValue: metric_names}\n    - if:\n        cond: {isPresent: random_seed}\n        then:\n        - --random-seed\n        - {inputValue: random_seed}\n    - --trained-model\n    - {outputPath: trained_model}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/92aa941c738e5b2fe957f987925053bf70996264/components/tensorflow/Train_model_using_Keras/on_ApacheParquet/component.yaml"
              },
              {
                "text": "name: Predict with TensorFlow model on ApacheParquet data\ndescription: Makes predictions using TensorFlow model.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/tensorflow/Predict/on_ApacheParquet/component.yaml'}\ninputs:\n- {name: dataset, type: ApacheParquet, description: Tabular dataset for prediction.}\n- {name: model, type: TensorflowSavedModel, description: Trained model in TensorFlow\n    format.}\n- {name: label_column_name, type: String, description: Name of the table column to\n    use as label., optional: true}\n- {name: batch_size, type: Integer, description: Number of samples to use in each\n    batch., default: '1000', optional: true}\noutputs:\n- {name: predictions, description: Predictions in multiline text format.}\nimplementation:\n  container:\n    image: tensorflow/tensorflow:2.9.1\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'tensorflow-io==0.26.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install\n      --quiet --no-warn-script-location 'tensorflow-io==0.26.0' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def predict_with_TensorFlow_model_on_ApacheParquet_data(\n          dataset_path,\n          model_path,\n          predictions_path,\n          label_column_name = None,\n          batch_size = 1000,\n      ):\n          \"\"\"Makes predictions using TensorFlow model.\n\n          Args:\n              dataset_path: Tabular dataset for prediction.\n              model_path: Trained model in TensorFlow format.\n              predictions_path: Predictions in multiline text format.\n              label_column_name: Name of the table column to use as label.\n              batch_size: Number of samples to use in each batch.\n          \"\"\"\n          import numpy\n          import tensorflow as tf\n          import tensorflow_io as tfio\n\n          FEATURES_COLUMN_NAME = \"features\"\n\n          model = tf.saved_model.load(export_dir=model_path)\n\n          def stack_feature_batches_and_drop_labels(columns_batch_dict):\n              if label_column_name:\n                  # batch dict keys have bytes type\n                  columns_batch_dict.pop(label_column_name.encode())\n\n              if FEATURES_COLUMN_NAME in columns_batch_dict:\n                  features_batch = columns_batch_dict[FEATURES_COLUMN_NAME]\n              else:\n                  # Need to stack individual feature columns to create a single feature tensor\n                  # Need to cast all column tensor types to float\n                  list_of_feature_batches = list(\n                      tf.cast(x=feature_batch, dtype=tf.float32)\n                      for feature_batch in columns_batch_dict.values()\n                  )\n                  features_batch = tf.stack(list_of_feature_batches, axis=-1)\n              return features_batch\n\n          # ! parquet::ParquetException is thrown if the Parquet dataset has nulls values\n          dataset = tfio.IODataset.from_parquet(filename=dataset_path)\n\n          dataset = dataset.batch(\n              batch_size=batch_size,\n              num_parallel_calls=tf.data.AUTOTUNE,\n              deterministic=True,\n          ).map(\n              map_func=stack_feature_batches_and_drop_labels,\n              num_parallel_calls=tf.data.AUTOTUNE,\n              deterministic=True,\n          )\n\n          with open(predictions_path, \"w\") as predictions_file:\n              for features_batch in dataset:\n                  predictions_tensor = model(features_batch)\n                  numpy.savetxt(predictions_file, predictions_tensor.numpy())\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Predict with TensorFlow model on ApacheParquet data', description='Makes predictions using TensorFlow model.')\n      _parser.add_argument(\"--dataset\", dest=\"dataset_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--batch-size\", dest=\"batch_size\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--predictions\", dest=\"predictions_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = predict_with_TensorFlow_model_on_ApacheParquet_data(**_parsed_args)\n    args:\n    - --dataset\n    - {inputPath: dataset}\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: label_column_name}\n        then:\n        - --label-column-name\n        - {inputValue: label_column_name}\n    - if:\n        cond: {isPresent: batch_size}\n        then:\n        - --batch-size\n        - {inputValue: batch_size}\n    - --predictions\n    - {outputPath: predictions}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/92aa941c738e5b2fe957f987925053bf70996264/components/tensorflow/Predict/on_ApacheParquet/component.yaml"
              }
            ],
            "name": "Tensorflow"
          },
          {
            "components": [
              {
                "text": "name: Catboost train regression\ndescription: |-\n  Train a CatBoost classifier model.\n\n      Args:\n          training_data_path: Path for the training data in CSV format.\n          model_path: Output path for the trained model in binary CatBoostModel format.\n          starting_model_path: Path for the existing trained model to start from.\n          label_column: Column containing the label data.\n\n          loss_function: The metric to use in training and also selector of the machine learning\n              problem to solve. Default = 'RMSE'. Possible values:\n              'RMSE', 'MAE', 'Quantile:alpha=value', 'LogLinQuantile:alpha=value', 'Poisson', 'MAPE', 'Lq:q=value'\n          num_iterations: Number of trees to add to the ensemble.\n          learning_rate: Step size shrinkage used in update to prevents overfitting.\n              Default value is selected automatically for binary classification with other parameters set to default.\n              In all other cases default is 0.03.\n          depth: Depth of a tree. All trees are the same depth. Default = 6\n          random_seed: Random number seed. Default = 0\n\n          cat_features: A list of Categorical features (indices or names).\n          additional_training_options: A dictionary with additional options to pass to CatBoostRegressor\n\n      Outputs:\n          model: Trained model in binary CatBoostModel format.\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: training_data, type: CSV}\n- {name: starting_model, type: CatBoostModel, optional: true}\n- {name: label_column, type: Integer, default: '0', optional: true}\n- {name: loss_function, type: String, default: RMSE, optional: true}\n- {name: num_iterations, type: Integer, default: '500', optional: true}\n- {name: learning_rate, type: Float, optional: true}\n- {name: depth, type: Integer, default: '6', optional: true}\n- {name: random_seed, type: Integer, default: '0', optional: true}\n- {name: cat_features, type: JsonArray, optional: true}\n- {name: additional_training_options, type: JsonObject, default: '{}', optional: true}\noutputs:\n- {name: model, type: CatBoostModel}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Train_regression/from_CSV/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'catboost==0.23' --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def catboost_train_regression(\n          training_data_path,\n          model_path,\n          starting_model_path = None,\n          label_column = 0,\n\n          loss_function = 'RMSE',\n          num_iterations = 500,\n          learning_rate = None,\n          depth = 6,\n          random_seed = 0,\n\n          cat_features = None,\n\n          additional_training_options = {},\n      ):\n          '''Train a CatBoost classifier model.\n\n          Args:\n              training_data_path: Path for the training data in CSV format.\n              model_path: Output path for the trained model in binary CatBoostModel format.\n              starting_model_path: Path for the existing trained model to start from.\n              label_column: Column containing the label data.\n\n              loss_function: The metric to use in training and also selector of the machine learning\n                  problem to solve. Default = 'RMSE'. Possible values:\n                  'RMSE', 'MAE', 'Quantile:alpha=value', 'LogLinQuantile:alpha=value', 'Poisson', 'MAPE', 'Lq:q=value'\n              num_iterations: Number of trees to add to the ensemble.\n              learning_rate: Step size shrinkage used in update to prevents overfitting.\n                  Default value is selected automatically for binary classification with other parameters set to default.\n                  In all other cases default is 0.03.\n              depth: Depth of a tree. All trees are the same depth. Default = 6\n              random_seed: Random number seed. Default = 0\n\n              cat_features: A list of Categorical features (indices or names).\n              additional_training_options: A dictionary with additional options to pass to CatBoostRegressor\n\n          Outputs:\n              model: Trained model in binary CatBoostModel format.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          import tempfile\n          from pathlib import Path\n\n          from catboost import CatBoostRegressor, Pool\n\n          column_descriptions = {label_column: 'Label'}\n          column_description_path = tempfile.NamedTemporaryFile(delete=False).name\n          with open(column_description_path, 'w') as column_description_file:\n              for idx, kind in column_descriptions.items():\n                  column_description_file.write('{}\\t{}\\n'.format(idx, kind))\n\n          train_data = Pool(\n              training_data_path,\n              column_description=column_description_path,\n              has_header=True,\n              delimiter=',',\n          )\n\n          model = CatBoostRegressor(\n              iterations=num_iterations,\n              depth=depth,\n              learning_rate=learning_rate,\n              loss_function=loss_function,\n              random_seed=random_seed,\n              verbose=True,\n              **additional_training_options,\n          )\n\n          model.fit(\n              train_data,\n              cat_features=cat_features,\n              init_model=starting_model_path,\n              #verbose=False,\n              #plot=True,\n          )\n          Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n          model.save_model(model_path)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Catboost train regression', description=\"Train a CatBoost classifier model.\\n\\n    Args:\\n        training_data_path: Path for the training data in CSV format.\\n        model_path: Output path for the trained model in binary CatBoostModel format.\\n        starting_model_path: Path for the existing trained model to start from.\\n        label_column: Column containing the label data.\\n\\n        loss_function: The metric to use in training and also selector of the machine learning\\n            problem to solve. Default = 'RMSE'. Possible values:\\n            'RMSE', 'MAE', 'Quantile:alpha=value', 'LogLinQuantile:alpha=value', 'Poisson', 'MAPE', 'Lq:q=value'\\n        num_iterations: Number of trees to add to the ensemble.\\n        learning_rate: Step size shrinkage used in update to prevents overfitting.\\n            Default value is selected automatically for binary classification with other parameters set to default.\\n            In all other cases default is 0.03.\\n        depth: Depth of a tree. All trees are the same depth. Default = 6\\n        random_seed: Random number seed. Default = 0\\n\\n        cat_features: A list of Categorical features (indices or names).\\n        additional_training_options: A dictionary with additional options to pass to CatBoostRegressor\\n\\n    Outputs:\\n        model: Trained model in binary CatBoostModel format.\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>\")\n      _parser.add_argument(\"--training-data\", dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--starting-model\", dest=\"starting_model_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column\", dest=\"label_column\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--loss-function\", dest=\"loss_function\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--num-iterations\", dest=\"num_iterations\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--learning-rate\", dest=\"learning_rate\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--depth\", dest=\"depth\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--random-seed\", dest=\"random_seed\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--cat-features\", dest=\"cat_features\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--additional-training-options\", dest=\"additional_training_options\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = catboost_train_regression(**_parsed_args)\n    args:\n    - --training-data\n    - {inputPath: training_data}\n    - if:\n        cond: {isPresent: starting_model}\n        then:\n        - --starting-model\n        - {inputPath: starting_model}\n    - if:\n        cond: {isPresent: label_column}\n        then:\n        - --label-column\n        - {inputValue: label_column}\n    - if:\n        cond: {isPresent: loss_function}\n        then:\n        - --loss-function\n        - {inputValue: loss_function}\n    - if:\n        cond: {isPresent: num_iterations}\n        then:\n        - --num-iterations\n        - {inputValue: num_iterations}\n    - if:\n        cond: {isPresent: learning_rate}\n        then:\n        - --learning-rate\n        - {inputValue: learning_rate}\n    - if:\n        cond: {isPresent: depth}\n        then:\n        - --depth\n        - {inputValue: depth}\n    - if:\n        cond: {isPresent: random_seed}\n        then:\n        - --random-seed\n        - {inputValue: random_seed}\n    - if:\n        cond: {isPresent: cat_features}\n        then:\n        - --cat-features\n        - {inputValue: cat_features}\n    - if:\n        cond: {isPresent: additional_training_options}\n        then:\n        - --additional-training-options\n        - {inputValue: additional_training_options}\n    - --model\n    - {outputPath: model}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Train_regression/from_CSV/component.yaml"
              },
              {
                "text": "name: Catboost train classifier\ndescription: |-\n  Train a CatBoost classifier model.\n\n      Args:\n          training_data_path: Path for the training data in CSV format.\n          model_path: Output path for the trained model in binary CatBoostModel format.\n          starting_model_path: Path for the existing trained model to start from.\n          label_column: Column containing the label data.\n\n          loss_function: The metric to use in training and also selector of the machine learning\n              problem to solve. Default = 'Logloss'\n          num_iterations: Number of trees to add to the ensemble.\n          learning_rate: Step size shrinkage used in update to prevents overfitting.\n              Default value is selected automatically for binary classification with other parameters set to default.\n              In all other cases default is 0.03.\n          depth: Depth of a tree. All trees are the same depth. Default = 6\n          random_seed: Random number seed. Default = 0\n\n          cat_features: A list of Categorical features (indices or names).\n          text_features: A list of Text features (indices or names).\n          additional_training_options: A dictionary with additional options to pass to CatBoostClassifier\n\n      Outputs:\n          model: Trained model in binary CatBoostModel format.\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: training_data, type: CSV}\n- {name: starting_model, type: CatBoostModel, optional: true}\n- {name: label_column, type: Integer, default: '0', optional: true}\n- {name: loss_function, type: String, default: Logloss, optional: true}\n- {name: num_iterations, type: Integer, default: '500', optional: true}\n- {name: learning_rate, type: Float, optional: true}\n- {name: depth, type: Integer, default: '6', optional: true}\n- {name: random_seed, type: Integer, default: '0', optional: true}\n- {name: cat_features, type: JsonArray, optional: true}\n- {name: text_features, type: JsonArray, optional: true}\n- {name: additional_training_options, type: JsonObject, default: '{}', optional: true}\noutputs:\n- {name: model, type: CatBoostModel}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Train_classifier/from_CSV/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'catboost==0.23' --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def catboost_train_classifier(\n          training_data_path,\n          model_path,\n          starting_model_path = None,\n          label_column = 0,\n\n          loss_function = 'Logloss',\n          num_iterations = 500,\n          learning_rate = None,\n          depth = 6,\n          random_seed = 0,\n\n          cat_features = None,\n          text_features = None,\n\n          additional_training_options = {},\n      ):\n          '''Train a CatBoost classifier model.\n\n          Args:\n              training_data_path: Path for the training data in CSV format.\n              model_path: Output path for the trained model in binary CatBoostModel format.\n              starting_model_path: Path for the existing trained model to start from.\n              label_column: Column containing the label data.\n\n              loss_function: The metric to use in training and also selector of the machine learning\n                  problem to solve. Default = 'Logloss'\n              num_iterations: Number of trees to add to the ensemble.\n              learning_rate: Step size shrinkage used in update to prevents overfitting.\n                  Default value is selected automatically for binary classification with other parameters set to default.\n                  In all other cases default is 0.03.\n              depth: Depth of a tree. All trees are the same depth. Default = 6\n              random_seed: Random number seed. Default = 0\n\n              cat_features: A list of Categorical features (indices or names).\n              text_features: A list of Text features (indices or names).\n              additional_training_options: A dictionary with additional options to pass to CatBoostClassifier\n\n          Outputs:\n              model: Trained model in binary CatBoostModel format.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          import tempfile\n          from pathlib import Path\n\n          from catboost import CatBoostClassifier, Pool\n\n          column_descriptions = {label_column: 'Label'}\n          column_description_path = tempfile.NamedTemporaryFile(delete=False).name\n          with open(column_description_path, 'w') as column_description_file:\n              for idx, kind in column_descriptions.items():\n                  column_description_file.write('{}\\t{}\\n'.format(idx, kind))\n\n          train_data = Pool(\n              training_data_path,\n              column_description=column_description_path,\n              has_header=True,\n              delimiter=',',\n          )\n\n          model = CatBoostClassifier(\n              iterations=num_iterations,\n              depth=depth,\n              learning_rate=learning_rate,\n              loss_function=loss_function,\n              random_seed=random_seed,\n              verbose=True,\n              **additional_training_options,\n          )\n\n          model.fit(\n              train_data,\n              cat_features=cat_features,\n              text_features=text_features,\n              init_model=starting_model_path,\n              #verbose=False,\n              #plot=True,\n          )\n          Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n          model.save_model(model_path)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Catboost train classifier', description=\"Train a CatBoost classifier model.\\n\\n    Args:\\n        training_data_path: Path for the training data in CSV format.\\n        model_path: Output path for the trained model in binary CatBoostModel format.\\n        starting_model_path: Path for the existing trained model to start from.\\n        label_column: Column containing the label data.\\n\\n        loss_function: The metric to use in training and also selector of the machine learning\\n            problem to solve. Default = 'Logloss'\\n        num_iterations: Number of trees to add to the ensemble.\\n        learning_rate: Step size shrinkage used in update to prevents overfitting.\\n            Default value is selected automatically for binary classification with other parameters set to default.\\n            In all other cases default is 0.03.\\n        depth: Depth of a tree. All trees are the same depth. Default = 6\\n        random_seed: Random number seed. Default = 0\\n\\n        cat_features: A list of Categorical features (indices or names).\\n        text_features: A list of Text features (indices or names).\\n        additional_training_options: A dictionary with additional options to pass to CatBoostClassifier\\n\\n    Outputs:\\n        model: Trained model in binary CatBoostModel format.\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>\")\n      _parser.add_argument(\"--training-data\", dest=\"training_data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--starting-model\", dest=\"starting_model_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column\", dest=\"label_column\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--loss-function\", dest=\"loss_function\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--num-iterations\", dest=\"num_iterations\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--learning-rate\", dest=\"learning_rate\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--depth\", dest=\"depth\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--random-seed\", dest=\"random_seed\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--cat-features\", dest=\"cat_features\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--text-features\", dest=\"text_features\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--additional-training-options\", dest=\"additional_training_options\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = catboost_train_classifier(**_parsed_args)\n    args:\n    - --training-data\n    - {inputPath: training_data}\n    - if:\n        cond: {isPresent: starting_model}\n        then:\n        - --starting-model\n        - {inputPath: starting_model}\n    - if:\n        cond: {isPresent: label_column}\n        then:\n        - --label-column\n        - {inputValue: label_column}\n    - if:\n        cond: {isPresent: loss_function}\n        then:\n        - --loss-function\n        - {inputValue: loss_function}\n    - if:\n        cond: {isPresent: num_iterations}\n        then:\n        - --num-iterations\n        - {inputValue: num_iterations}\n    - if:\n        cond: {isPresent: learning_rate}\n        then:\n        - --learning-rate\n        - {inputValue: learning_rate}\n    - if:\n        cond: {isPresent: depth}\n        then:\n        - --depth\n        - {inputValue: depth}\n    - if:\n        cond: {isPresent: random_seed}\n        then:\n        - --random-seed\n        - {inputValue: random_seed}\n    - if:\n        cond: {isPresent: cat_features}\n        then:\n        - --cat-features\n        - {inputValue: cat_features}\n    - if:\n        cond: {isPresent: text_features}\n        then:\n        - --text-features\n        - {inputValue: text_features}\n    - if:\n        cond: {isPresent: additional_training_options}\n        then:\n        - --additional-training-options\n        - {inputValue: additional_training_options}\n    - --model\n    - {outputPath: model}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Train_classifier/from_CSV/component.yaml"
              },
              {
                "text": "name: Catboost predict values\ndescription: |-\n  Predict values with a CatBoost model.\n\n      Args:\n          data_path: Path for the data in CSV format.\n          model_path: Path for the trained model in binary CatBoostModel format.\n          label_column: Column containing the label data.\n          predictions_path: Output path for the predictions.\n\n      Outputs:\n          predictions: Predictions in text format.\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: data, type: CSV}\n- {name: model, type: CatBoostModel}\n- {name: label_column, type: Integer, optional: true}\noutputs:\n- {name: predictions}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Predict_values/from_CSV/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'catboost==0.23' --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def catboost_predict_values(\n          data_path,\n          model_path,\n          predictions_path,\n\n          label_column = None,\n      ):\n          '''Predict values with a CatBoost model.\n\n          Args:\n              data_path: Path for the data in CSV format.\n              model_path: Path for the trained model in binary CatBoostModel format.\n              label_column: Column containing the label data.\n              predictions_path: Output path for the predictions.\n\n          Outputs:\n              predictions: Predictions in text format.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          import tempfile\n\n          from catboost import CatBoost, Pool\n          import numpy\n\n          if label_column:\n              column_descriptions = {label_column: 'Label'}\n              column_description_path = tempfile.NamedTemporaryFile(delete=False).name\n              with open(column_description_path, 'w') as column_description_file:\n                  for idx, kind in column_descriptions.items():\n                      column_description_file.write('{}\\t{}\\n'.format(idx, kind))\n          else:\n              column_description_path = None\n\n          eval_data = Pool(\n              data_path,\n              column_description=column_description_path,\n              has_header=True,\n              delimiter=',',\n          )\n\n          model = CatBoost()\n          model.load_model(model_path)\n\n          predictions = model.predict(eval_data, prediction_type='RawFormulaVal')\n          numpy.savetxt(predictions_path, predictions)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Catboost predict values', description='Predict values with a CatBoost model.\\n\\n    Args:\\n        data_path: Path for the data in CSV format.\\n        model_path: Path for the trained model in binary CatBoostModel format.\\n        label_column: Column containing the label data.\\n        predictions_path: Output path for the predictions.\\n\\n    Outputs:\\n        predictions: Predictions in text format.\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column\", dest=\"label_column\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--predictions\", dest=\"predictions_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = catboost_predict_values(**_parsed_args)\n    args:\n    - --data\n    - {inputPath: data}\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: label_column}\n        then:\n        - --label-column\n        - {inputValue: label_column}\n    - --predictions\n    - {outputPath: predictions}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Predict_values/from_CSV/component.yaml"
              },
              {
                "text": "name: Catboost predict classes\ndescription: |-\n  Predict classes using the CatBoost classifier model.\n\n      Args:\n          data_path: Path for the data in CSV format.\n          model_path: Path for the trained model in binary CatBoostModel format.\n          label_column: Column containing the label data.\n          predictions_path: Output path for the predictions.\n\n      Outputs:\n          predictions: Class predictions in text format.\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: data, type: CSV}\n- {name: model, type: CatBoostModel}\n- {name: label_column, type: Integer, optional: true}\noutputs:\n- {name: predictions}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Predict_classes/from_CSV/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'catboost==0.22' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'catboost==0.22' --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def catboost_predict_classes(\n          data_path,\n          model_path,\n          predictions_path,\n\n          label_column = None,\n      ):\n          '''Predict classes using the CatBoost classifier model.\n\n          Args:\n              data_path: Path for the data in CSV format.\n              model_path: Path for the trained model in binary CatBoostModel format.\n              label_column: Column containing the label data.\n              predictions_path: Output path for the predictions.\n\n          Outputs:\n              predictions: Class predictions in text format.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          import tempfile\n\n          from catboost import CatBoostClassifier, Pool\n          import numpy\n\n          if label_column:\n              column_descriptions = {label_column: 'Label'}\n              column_description_path = tempfile.NamedTemporaryFile(delete=False).name\n              with open(column_description_path, 'w') as column_description_file:\n                  for idx, kind in column_descriptions.items():\n                      column_description_file.write('{}\\t{}\\n'.format(idx, kind))\n          else:\n              column_description_path = None\n\n          eval_data = Pool(\n              data_path,\n              column_description=column_description_path,\n              has_header=True,\n              delimiter=',',\n          )\n\n          model = CatBoostClassifier()\n          model.load_model(model_path)\n\n          predictions = model.predict(eval_data)\n          numpy.savetxt(predictions_path, predictions, fmt='%s')\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Catboost predict classes', description='Predict classes using the CatBoost classifier model.\\n\\n    Args:\\n        data_path: Path for the data in CSV format.\\n        model_path: Path for the trained model in binary CatBoostModel format.\\n        label_column: Column containing the label data.\\n        predictions_path: Output path for the predictions.\\n\\n    Outputs:\\n        predictions: Class predictions in text format.\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column\", dest=\"label_column\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--predictions\", dest=\"predictions_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = catboost_predict_classes(**_parsed_args)\n    args:\n    - --data\n    - {inputPath: data}\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: label_column}\n        then:\n        - --label-column\n        - {inputValue: label_column}\n    - --predictions\n    - {outputPath: predictions}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Predict_classes/from_CSV/component.yaml"
              },
              {
                "text": "name: Catboost predict class probabilities\ndescription: |-\n  Predict class probabilities with a CatBoost model.\n\n      Args:\n          data_path: Path for the data in CSV format.\n          model_path: Path for the trained model in binary CatBoostModel format.\n          label_column: Column containing the label data.\n          predictions_path: Output path for the predictions.\n\n      Outputs:\n          predictions: Predictions in text format.\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: data, type: CSV}\n- {name: model, type: CatBoostModel}\n- {name: label_column, type: Integer, optional: true}\noutputs:\n- {name: predictions}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/Predict_class_probabilities/from_CSV/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'catboost==0.23' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'catboost==0.23' --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def catboost_predict_class_probabilities(\n          data_path,\n          model_path,\n          predictions_path,\n\n          label_column = None,\n      ):\n          '''Predict class probabilities with a CatBoost model.\n\n          Args:\n              data_path: Path for the data in CSV format.\n              model_path: Path for the trained model in binary CatBoostModel format.\n              label_column: Column containing the label data.\n              predictions_path: Output path for the predictions.\n\n          Outputs:\n              predictions: Predictions in text format.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          import tempfile\n\n          from catboost import CatBoost, Pool\n          import numpy\n\n          if label_column:\n              column_descriptions = {label_column: 'Label'}\n              column_description_path = tempfile.NamedTemporaryFile(delete=False).name\n              with open(column_description_path, 'w') as column_description_file:\n                  for idx, kind in column_descriptions.items():\n                      column_description_file.write('{}\\t{}\\n'.format(idx, kind))\n          else:\n              column_description_path = None\n\n          eval_data = Pool(\n              data_path,\n              column_description=column_description_path,\n              has_header=True,\n              delimiter=',',\n          )\n\n          model = CatBoost()\n          model.load_model(model_path)\n\n          predictions = model.predict(eval_data, prediction_type='Probability')\n          numpy.savetxt(predictions_path, predictions)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Catboost predict class probabilities', description='Predict class probabilities with a CatBoost model.\\n\\n    Args:\\n        data_path: Path for the data in CSV format.\\n        model_path: Path for the trained model in binary CatBoostModel format.\\n        label_column: Column containing the label data.\\n        predictions_path: Output path for the predictions.\\n\\n    Outputs:\\n        predictions: Predictions in text format.\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column\", dest=\"label_column\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--predictions\", dest=\"predictions_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = catboost_predict_class_probabilities(**_parsed_args)\n    args:\n    - --data\n    - {inputPath: data}\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: label_column}\n        then:\n        - --label-column\n        - {inputValue: label_column}\n    - --predictions\n    - {outputPath: predictions}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/Predict_class_probabilities/from_CSV/component.yaml"
              },
              {
                "text": "name: Convert CatBoostModel to ONNX\ndescription: |-\n  Convert CatBoost model to ONNX format.\n\n      Args:\n          model_path: Path of a trained model in binary CatBoost model format.\n          converted_model_path: Output path for the converted model.\n\n      Outputs:\n          converted_model: Model in ONNX format.\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: model, type: CatBoostModel}\noutputs:\n- {name: converted_model, type: ONNX}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/CatBoost/convert_CatBoostModel_to_ONNX/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'catboost==0.22' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'catboost==0.22' --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_CatBoostModel_to_ONNX(\n          model_path,\n          converted_model_path,\n      ):\n          '''Convert CatBoost model to ONNX format.\n\n          Args:\n              model_path: Path of a trained model in binary CatBoost model format.\n              converted_model_path: Output path for the converted model.\n\n          Outputs:\n              converted_model: Model in ONNX format.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          from catboost import CatBoost\n\n          model = CatBoost()\n          model.load_model(model_path)\n          model.save_model(converted_model_path, format=\"onnx\")\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert CatBoostModel to ONNX', description='Convert CatBoost model to ONNX format.\\n\\n    Args:\\n        model_path: Path of a trained model in binary CatBoost model format.\\n        converted_model_path: Output path for the converted model.\\n\\n    Outputs:\\n        converted_model: Model in ONNX format.\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--converted-model\", dest=\"converted_model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = convert_CatBoostModel_to_ONNX(**_parsed_args)\n    args:\n    - --model\n    - {inputPath: model}\n    - --converted-model\n    - {outputPath: converted_model}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/CatBoost/convert_CatBoostModel_to_ONNX/component.yaml"
              }
            ],
            "name": "CatBoost"
          },
          {
            "components": [
              {
                "text": "name: Create Vowpal Wabbit JSON dataset from CSV\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Vowpal_Wabbit/Create_JSON_dataset/from_CSV/component.yaml'}\ninputs:\n- {name: dataset, type: CSV}\n- {name: label_column_name, type: String, optional: true}\noutputs:\n- {name: converted_dataset, type: VowpalWabbitJsonDataset}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pandas==1.4.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'pandas==1.4.3' 'numpy<2' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def create_Vowpal_Wabbit_JSON_dataset_from_CSV(\n          dataset_path,\n          converted_dataset_path,\n          label_column_name = None,\n      ):\n          import json\n          import pandas\n\n          df = pandas.read_csv(dataset_path).convert_dtypes()\n\n          if label_column_name:\n              label_series = df[label_column_name]\n              features_df = df.drop(columns=[label_column_name])\n              label_values_list = label_series.to_list()\n              feature_records_list = features_df.to_dict(\"records\")\n\n              with open(converted_dataset_path, \"w\") as f:\n                  for features, label in zip(feature_records_list, label_values_list):\n                      non_nan_features = {\n                          k: v for k, v in features.items() if v == v and v is not None\n                      }\n                      vw_record = {\n                          \"_label\": label,\n                      }\n                      vw_record.update(non_nan_features)\n                      vw_record_line = json.dumps(vw_record)\n                      f.write(vw_record_line + \"\\n\")\n          else:\n              features_df = df\n              feature_records_list = features_df.to_dict(\"records\")\n\n              with open(converted_dataset_path, \"w\") as f:\n                  for features in feature_records_list:\n                      non_nan_features = {\n                          k: v for k, v in features.items() if v == v and v is not None\n                      }\n                      vw_record = non_nan_features\n                      vw_record_line = json.dumps(vw_record)\n                      f.write(vw_record_line + \"\\n\")\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Create Vowpal Wabbit JSON dataset from CSV', description='')\n      _parser.add_argument(\"--dataset\", dest=\"dataset_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--label-column-name\", dest=\"label_column_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--converted-dataset\", dest=\"converted_dataset_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = create_Vowpal_Wabbit_JSON_dataset_from_CSV(**_parsed_args)\n    args:\n    - --dataset\n    - {inputPath: dataset}\n    - if:\n        cond: {isPresent: label_column_name}\n        then:\n        - --label-column-name\n        - {inputValue: label_column_name}\n    - --converted-dataset\n    - {outputPath: converted_dataset}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/ML_frameworks/Vowpal_Wabbit/Create_JSON_dataset/from_CSV/component.yaml"
              },
              {
                "text": "name: Train regression model using Vowpal Wabbit on VowpalWabbitDataset\nmetadata:\n  annotations:\n    author: \"Alexey Volkov <alexey.volkov@ark-kun.com>\"\n    canonical_location: \"https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Vowpal_Wabbit/Train_regression_model/from_VowpalWabbitJsonDataset/component.yaml\"\ninputs:\n- {name: Dataset, type: VowpalWabbitJsonDataset}\n- {name: Initial model, type: VowpalWabbitRegressorModel, optional: true}\n- {name: Number of passes, type: Integer, default: \"1\"}\n- {name: Loss function, type: String, default: \"squared\", description: \"Supported values: squared, hinge, logistic, quantile, poisson. See https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Loss-functions\"}\noutputs:\n- {name: Model, type: VowpalWabbitRegressorModel}\n- {name: Readable model, type: VowpalWabbitReadableHashRegressorModel}\nimplementation:\n  container:\n    image: vowpalwabbit/vw-rel-alpine:9.0.1\n    # See https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Command-line-arguments\n    command:\n      - sh\n      - -exc\n      - |\n        # Creating directories for the outputs\n        mkdir -p \"$(dirname \"$4\")\" # Model\n        mkdir -p \"$(dirname \"$6\")\" # Readable model\n        \"$0\" \"$@\"\n      - ./vw\n      - --data\n      - {inputPath: Dataset}\n      - --final_regressor\n      - {outputPath: Model}\n      - --invert_hash\n      - {outputPath: Readable model}\n      - --passes\n      - {inputValue: Number of passes}\n      - --loss_function\n      - {inputValue: Loss function}\n      # Enable JSON parsing\n      - --json\n      - if:\n          cond: {isPresent: Initial model}\n          then:\n            - --initial_regressor\n            - {inputPath: Initial model}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/a2a629e776d5fa0204ce71370cab23282d3e4278/components/ML_frameworks/Vowpal_Wabbit/Train_regression_model/from_VowpalWabbitJsonDataset/component.yaml"
              },
              {
                "text": "name: Predict using Vowpal Wabbit model on VowpalWabbitJsonDataset\nmetadata:\n  annotations:\n    author: \"Alexey Volkov <alexey.volkov@ark-kun.com>\"\n    canonical_location: \"https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/ML_frameworks/Vowpal_Wabbit/Predict/from_VowpalWabbitJsonDataset/component.yaml\"\ninputs:\n- {name: Dataset, type: VowpalWabbitJsonDataset}\n- {name: Model, type: VowpalWabbitRegressorModel}\noutputs:\n- {name: Predictions}\nimplementation:\n  container:\n    image: vowpalwabbit/vw-rel-alpine:9.0.1\n    # See https://github.com/VowpalWabbit/vowpal_wabbit/wiki/Command-line-arguments\n    command:\n      - sh\n      - -exc\n      - |\n        # Creating directories for the outputs\n        mkdir -p \"$(dirname \"$6\")\" # Predictions\n        \"$0\" \"$@\"\n      - ./vw\n      - --data\n      - {inputPath: Dataset}\n      - --initial_regressor\n      - {inputPath: Model}\n      - --predictions\n      - {outputPath: Predictions}\n      # Ignore label information and just test\n      - --testonly\n      # Enable JSON parsing\n      - --json\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/a2a629e776d5fa0204ce71370cab23282d3e4278/components/ML_frameworks/Vowpal_Wabbit/Predict/from_VowpalWabbitJsonDataset/component.yaml"
              }
            ],
            "name": "Vowpal_Wabbit"
          },
          {
            "components": [
              {
                "text": "name: CsvExampleGen\ninputs:\n- {name: input_base, type: String}\n- name: input_config\n  type:\n    JsonObject: {data_type: 'proto:tfx.components.example_gen.Input'}\n- name: output_config\n  type:\n    JsonObject: {data_type: 'proto:tfx.components.example_gen.Output'}\n- name: range_config\n  type:\n    JsonObject: {data_type: 'proto:tfx.configs.RangeConfig'}\n  optional: true\noutputs:\n- {name: examples, type: Examples}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/ExampleGen/CsvExampleGen/component.yaml'\nimplementation:\n  container:\n    image: tensorflow/tfx:0.29.0\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def CsvExampleGen(\n          examples_path,\n          input_base,\n          input_config,\n          output_config,\n          range_config = None,\n      ):\n          from tfx.components.example_gen.csv_example_gen.component import CsvExampleGen as component_class\n\n          #Generated code\n          import os\n          import tempfile\n          from tensorflow.io import gfile\n          from google.protobuf import json_format, message\n          from tfx.types import channel_utils, artifact_utils\n          from tfx.components.base import base_executor\n\n          arguments = locals().copy()\n\n          component_class_args = {}\n\n          for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():\n              argument_value = arguments.get(name, None)\n              if argument_value is None:\n                  continue\n              parameter_type = execution_parameter.type\n              if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):\n                  argument_value_obj = parameter_type()\n                  json_format.Parse(argument_value, argument_value_obj)\n              else:\n                  argument_value_obj = argument_value\n              component_class_args[name] = argument_value_obj\n\n          for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():\n              artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel_parameter.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:\n                      # Recovering splits\n                      subdirs = gfile.listdir(artifact_path)\n                      # Workaround for https://github.com/tensorflow/tensorflow/issues/39167\n                      subdirs = [subdir.rstrip('/') for subdir in subdirs]\n                      split_names = [subdir.replace('Split-', '') for subdir in subdirs]\n                      artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))\n                  component_class_args[name] = channel_utils.as_channel([artifact])\n\n          component_class_instance = component_class(**component_class_args)\n\n          input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())\n          output_dict = {}\n          exec_properties = component_class_instance.exec_properties\n\n          # Generating paths for output artifacts\n          for name, channel in component_class_instance.outputs.items():\n              artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  artifact_list = [artifact]\n                  channel._artifacts = artifact_list\n                  output_dict[name] = artifact_list\n\n          print('component instance: ' + str(component_class_instance))\n\n          executor_context = base_executor.BaseExecutor.Context(\n              beam_pipeline_args=arguments.get('beam_pipeline_args'),\n              tmp_dir=tempfile.gettempdir(),\n              unique_id='tfx_component',\n          )\n          executor = component_class_instance.executor_spec.executor_class(executor_context)\n          executor.Do(\n              input_dict=input_dict,\n              output_dict=output_dict,\n              exec_properties=exec_properties,\n          )\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='CsvExampleGen', description='')\n      _parser.add_argument(\"--input-base\", dest=\"input_base\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--input-config\", dest=\"input_config\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--output-config\", dest=\"output_config\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--range-config\", dest=\"range_config\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--examples\", dest=\"examples_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = CsvExampleGen(**_parsed_args)\n    args:\n    - --input-base\n    - {inputValue: input_base}\n    - --input-config\n    - {inputValue: input_config}\n    - --output-config\n    - {inputValue: output_config}\n    - if:\n        cond: {isPresent: range_config}\n        then:\n        - --range-config\n        - {inputValue: range_config}\n    - --examples\n    - {outputPath: examples}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/ExampleGen/CsvExampleGen/component.yaml"
              },
              {
                "text": "name: StatisticsGen\ninputs:\n- {name: examples, type: Examples}\n- {name: schema, type: Schema, optional: true}\n- {name: exclude_splits, type: String, optional: true}\noutputs:\n- {name: statistics, type: ExampleStatistics}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/StatisticsGen/component.yaml'\nimplementation:\n  container:\n    image: tensorflow/tfx:0.29.0\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def StatisticsGen(\n          examples_path,\n          statistics_path,\n          schema_path = None,\n          exclude_splits = None,\n      ):\n          from tfx.components.statistics_gen.component import StatisticsGen as component_class\n\n          #Generated code\n          import os\n          import tempfile\n          from tensorflow.io import gfile\n          from google.protobuf import json_format, message\n          from tfx.types import channel_utils, artifact_utils\n          from tfx.components.base import base_executor\n\n          arguments = locals().copy()\n\n          component_class_args = {}\n\n          for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():\n              argument_value = arguments.get(name, None)\n              if argument_value is None:\n                  continue\n              parameter_type = execution_parameter.type\n              if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):\n                  argument_value_obj = parameter_type()\n                  json_format.Parse(argument_value, argument_value_obj)\n              else:\n                  argument_value_obj = argument_value\n              component_class_args[name] = argument_value_obj\n\n          for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():\n              artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel_parameter.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:\n                      # Recovering splits\n                      subdirs = gfile.listdir(artifact_path)\n                      # Workaround for https://github.com/tensorflow/tensorflow/issues/39167\n                      subdirs = [subdir.rstrip('/') for subdir in subdirs]\n                      split_names = [subdir.replace('Split-', '') for subdir in subdirs]\n                      artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))\n                  component_class_args[name] = channel_utils.as_channel([artifact])\n\n          component_class_instance = component_class(**component_class_args)\n\n          input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())\n          output_dict = {}\n          exec_properties = component_class_instance.exec_properties\n\n          # Generating paths for output artifacts\n          for name, channel in component_class_instance.outputs.items():\n              artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  artifact_list = [artifact]\n                  channel._artifacts = artifact_list\n                  output_dict[name] = artifact_list\n\n          print('component instance: ' + str(component_class_instance))\n\n          executor_context = base_executor.BaseExecutor.Context(\n              beam_pipeline_args=arguments.get('beam_pipeline_args'),\n              tmp_dir=tempfile.gettempdir(),\n              unique_id='tfx_component',\n          )\n          executor = component_class_instance.executor_spec.executor_class(executor_context)\n          executor.Do(\n              input_dict=input_dict,\n              output_dict=output_dict,\n              exec_properties=exec_properties,\n          )\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='StatisticsGen', description='')\n      _parser.add_argument(\"--examples\", dest=\"examples_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--schema\", dest=\"schema_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--exclude-splits\", dest=\"exclude_splits\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--statistics\", dest=\"statistics_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = StatisticsGen(**_parsed_args)\n    args:\n    - --examples\n    - {inputPath: examples}\n    - if:\n        cond: {isPresent: schema}\n        then:\n        - --schema\n        - {inputPath: schema}\n    - if:\n        cond: {isPresent: exclude_splits}\n        then:\n        - --exclude-splits\n        - {inputValue: exclude_splits}\n    - --statistics\n    - {outputPath: statistics}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/StatisticsGen/component.yaml"
              },
              {
                "text": "name: SchemaGen\ninputs:\n- {name: statistics, type: ExampleStatistics}\n- {name: infer_feature_shape, type: Integer, optional: true}\n- {name: exclude_splits, type: String, optional: true}\noutputs:\n- {name: schema, type: Schema}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/SchemaGen/component.yaml'\nimplementation:\n  container:\n    image: tensorflow/tfx:0.29.0\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def SchemaGen(\n          statistics_path,\n          schema_path,\n          infer_feature_shape = None,\n          exclude_splits = None,\n      ):\n          from tfx.components.schema_gen.component import SchemaGen as component_class\n\n          #Generated code\n          import os\n          import tempfile\n          from tensorflow.io import gfile\n          from google.protobuf import json_format, message\n          from tfx.types import channel_utils, artifact_utils\n          from tfx.components.base import base_executor\n\n          arguments = locals().copy()\n\n          component_class_args = {}\n\n          for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():\n              argument_value = arguments.get(name, None)\n              if argument_value is None:\n                  continue\n              parameter_type = execution_parameter.type\n              if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):\n                  argument_value_obj = parameter_type()\n                  json_format.Parse(argument_value, argument_value_obj)\n              else:\n                  argument_value_obj = argument_value\n              component_class_args[name] = argument_value_obj\n\n          for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():\n              artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel_parameter.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:\n                      # Recovering splits\n                      subdirs = gfile.listdir(artifact_path)\n                      # Workaround for https://github.com/tensorflow/tensorflow/issues/39167\n                      subdirs = [subdir.rstrip('/') for subdir in subdirs]\n                      split_names = [subdir.replace('Split-', '') for subdir in subdirs]\n                      artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))\n                  component_class_args[name] = channel_utils.as_channel([artifact])\n\n          component_class_instance = component_class(**component_class_args)\n\n          input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())\n          output_dict = {}\n          exec_properties = component_class_instance.exec_properties\n\n          # Generating paths for output artifacts\n          for name, channel in component_class_instance.outputs.items():\n              artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  artifact_list = [artifact]\n                  channel._artifacts = artifact_list\n                  output_dict[name] = artifact_list\n\n          print('component instance: ' + str(component_class_instance))\n\n          executor_context = base_executor.BaseExecutor.Context(\n              beam_pipeline_args=arguments.get('beam_pipeline_args'),\n              tmp_dir=tempfile.gettempdir(),\n              unique_id='tfx_component',\n          )\n          executor = component_class_instance.executor_spec.executor_class(executor_context)\n          executor.Do(\n              input_dict=input_dict,\n              output_dict=output_dict,\n              exec_properties=exec_properties,\n          )\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='SchemaGen', description='')\n      _parser.add_argument(\"--statistics\", dest=\"statistics_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--infer-feature-shape\", dest=\"infer_feature_shape\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--exclude-splits\", dest=\"exclude_splits\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--schema\", dest=\"schema_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = SchemaGen(**_parsed_args)\n    args:\n    - --statistics\n    - {inputPath: statistics}\n    - if:\n        cond: {isPresent: infer_feature_shape}\n        then:\n        - --infer-feature-shape\n        - {inputValue: infer_feature_shape}\n    - if:\n        cond: {isPresent: exclude_splits}\n        then:\n        - --exclude-splits\n        - {inputValue: exclude_splits}\n    - --schema\n    - {outputPath: schema}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/SchemaGen/component.yaml"
              },
              {
                "text": "name: ExampleValidator\ninputs:\n- {name: statistics, type: ExampleStatistics}\n- {name: schema, type: Schema}\n- {name: exclude_splits, type: String, optional: true}\noutputs:\n- {name: anomalies, type: ExampleAnomalies}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/ExampleValidator/component.yaml'\nimplementation:\n  container:\n    image: tensorflow/tfx:0.29.0\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def ExampleValidator(\n          statistics_path,\n          schema_path,\n          anomalies_path,\n          exclude_splits = None,\n      ):\n          from tfx.components.example_validator.component import ExampleValidator as component_class\n\n          #Generated code\n          import os\n          import tempfile\n          from tensorflow.io import gfile\n          from google.protobuf import json_format, message\n          from tfx.types import channel_utils, artifact_utils\n          from tfx.components.base import base_executor\n\n          arguments = locals().copy()\n\n          component_class_args = {}\n\n          for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():\n              argument_value = arguments.get(name, None)\n              if argument_value is None:\n                  continue\n              parameter_type = execution_parameter.type\n              if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):\n                  argument_value_obj = parameter_type()\n                  json_format.Parse(argument_value, argument_value_obj)\n              else:\n                  argument_value_obj = argument_value\n              component_class_args[name] = argument_value_obj\n\n          for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():\n              artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel_parameter.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:\n                      # Recovering splits\n                      subdirs = gfile.listdir(artifact_path)\n                      # Workaround for https://github.com/tensorflow/tensorflow/issues/39167\n                      subdirs = [subdir.rstrip('/') for subdir in subdirs]\n                      split_names = [subdir.replace('Split-', '') for subdir in subdirs]\n                      artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))\n                  component_class_args[name] = channel_utils.as_channel([artifact])\n\n          component_class_instance = component_class(**component_class_args)\n\n          input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())\n          output_dict = {}\n          exec_properties = component_class_instance.exec_properties\n\n          # Generating paths for output artifacts\n          for name, channel in component_class_instance.outputs.items():\n              artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  artifact_list = [artifact]\n                  channel._artifacts = artifact_list\n                  output_dict[name] = artifact_list\n\n          print('component instance: ' + str(component_class_instance))\n\n          executor_context = base_executor.BaseExecutor.Context(\n              beam_pipeline_args=arguments.get('beam_pipeline_args'),\n              tmp_dir=tempfile.gettempdir(),\n              unique_id='tfx_component',\n          )\n          executor = component_class_instance.executor_spec.executor_class(executor_context)\n          executor.Do(\n              input_dict=input_dict,\n              output_dict=output_dict,\n              exec_properties=exec_properties,\n          )\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='ExampleValidator', description='')\n      _parser.add_argument(\"--statistics\", dest=\"statistics_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--schema\", dest=\"schema_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--exclude-splits\", dest=\"exclude_splits\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--anomalies\", dest=\"anomalies_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = ExampleValidator(**_parsed_args)\n    args:\n    - --statistics\n    - {inputPath: statistics}\n    - --schema\n    - {inputPath: schema}\n    - if:\n        cond: {isPresent: exclude_splits}\n        then:\n        - --exclude-splits\n        - {inputValue: exclude_splits}\n    - --anomalies\n    - {outputPath: anomalies}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/ExampleValidator/component.yaml"
              },
              {
                "text": "name: Transform\ninputs:\n- {name: examples, type: Examples}\n- {name: schema, type: Schema}\n- {name: analyzer_cache, type: TransformCache, optional: true}\n- {name: module_file, type: String, optional: true}\n- {name: preprocessing_fn, type: String, optional: true}\n- {name: force_tf_compat_v1, type: Integer, optional: true}\n- {name: custom_config, type: String, optional: true}\n- name: splits_config\n  type:\n    JsonObject: {data_type: 'proto:tfx.components.transform.SplitsConfig'}\n  optional: true\noutputs:\n- {name: transform_graph, type: TransformGraph}\n- {name: transformed_examples, type: Examples}\n- {name: updated_analyzer_cache, type: TransformCache}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/Transform/component.yaml'\nimplementation:\n  container:\n    image: tensorflow/tfx:0.29.0\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def Transform(\n          examples_path,\n          schema_path,\n          transform_graph_path,\n          transformed_examples_path,\n          updated_analyzer_cache_path,\n          analyzer_cache_path = None,\n          module_file = None,\n          preprocessing_fn = None,\n          force_tf_compat_v1 = None,\n          custom_config = None,\n          splits_config = None,\n      ):\n          from tfx.components.transform.component import Transform as component_class\n\n          #Generated code\n          import os\n          import tempfile\n          from tensorflow.io import gfile\n          from google.protobuf import json_format, message\n          from tfx.types import channel_utils, artifact_utils\n          from tfx.components.base import base_executor\n\n          arguments = locals().copy()\n\n          component_class_args = {}\n\n          for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():\n              argument_value = arguments.get(name, None)\n              if argument_value is None:\n                  continue\n              parameter_type = execution_parameter.type\n              if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):\n                  argument_value_obj = parameter_type()\n                  json_format.Parse(argument_value, argument_value_obj)\n              else:\n                  argument_value_obj = argument_value\n              component_class_args[name] = argument_value_obj\n\n          for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():\n              artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel_parameter.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:\n                      # Recovering splits\n                      subdirs = gfile.listdir(artifact_path)\n                      # Workaround for https://github.com/tensorflow/tensorflow/issues/39167\n                      subdirs = [subdir.rstrip('/') for subdir in subdirs]\n                      split_names = [subdir.replace('Split-', '') for subdir in subdirs]\n                      artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))\n                  component_class_args[name] = channel_utils.as_channel([artifact])\n\n          component_class_instance = component_class(**component_class_args)\n\n          input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())\n          output_dict = {}\n          exec_properties = component_class_instance.exec_properties\n\n          # Generating paths for output artifacts\n          for name, channel in component_class_instance.outputs.items():\n              artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  artifact_list = [artifact]\n                  channel._artifacts = artifact_list\n                  output_dict[name] = artifact_list\n\n          print('component instance: ' + str(component_class_instance))\n\n          executor_context = base_executor.BaseExecutor.Context(\n              beam_pipeline_args=arguments.get('beam_pipeline_args'),\n              tmp_dir=tempfile.gettempdir(),\n              unique_id='tfx_component',\n          )\n          executor = component_class_instance.executor_spec.executor_class(executor_context)\n          executor.Do(\n              input_dict=input_dict,\n              output_dict=output_dict,\n              exec_properties=exec_properties,\n          )\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Transform', description='')\n      _parser.add_argument(\"--examples\", dest=\"examples_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--schema\", dest=\"schema_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--analyzer-cache\", dest=\"analyzer_cache_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--module-file\", dest=\"module_file\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--preprocessing-fn\", dest=\"preprocessing_fn\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--force-tf-compat-v1\", dest=\"force_tf_compat_v1\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--custom-config\", dest=\"custom_config\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--splits-config\", dest=\"splits_config\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transform-graph\", dest=\"transform_graph_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transformed-examples\", dest=\"transformed_examples_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--updated-analyzer-cache\", dest=\"updated_analyzer_cache_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = Transform(**_parsed_args)\n    args:\n    - --examples\n    - {inputPath: examples}\n    - --schema\n    - {inputPath: schema}\n    - if:\n        cond: {isPresent: analyzer_cache}\n        then:\n        - --analyzer-cache\n        - {inputPath: analyzer_cache}\n    - if:\n        cond: {isPresent: module_file}\n        then:\n        - --module-file\n        - {inputValue: module_file}\n    - if:\n        cond: {isPresent: preprocessing_fn}\n        then:\n        - --preprocessing-fn\n        - {inputValue: preprocessing_fn}\n    - if:\n        cond: {isPresent: force_tf_compat_v1}\n        then:\n        - --force-tf-compat-v1\n        - {inputValue: force_tf_compat_v1}\n    - if:\n        cond: {isPresent: custom_config}\n        then:\n        - --custom-config\n        - {inputValue: custom_config}\n    - if:\n        cond: {isPresent: splits_config}\n        then:\n        - --splits-config\n        - {inputValue: splits_config}\n    - --transform-graph\n    - {outputPath: transform_graph}\n    - --transformed-examples\n    - {outputPath: transformed_examples}\n    - --updated-analyzer-cache\n    - {outputPath: updated_analyzer_cache}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/Transform/component.yaml"
              },
              {
                "text": "name: Trainer\ninputs:\n- {name: examples, type: Examples}\n- name: train_args\n  type:\n    JsonObject: {data_type: 'proto:tfx.components.trainer.TrainArgs'}\n- name: eval_args\n  type:\n    JsonObject: {data_type: 'proto:tfx.components.trainer.EvalArgs'}\n- {name: transform_graph, type: TransformGraph, optional: true}\n- {name: schema, type: Schema, optional: true}\n- {name: base_model, type: Model, optional: true}\n- {name: hyperparameters, type: HyperParameters, optional: true}\n- {name: module_file, type: String, optional: true}\n- {name: run_fn, type: String, optional: true}\n- {name: trainer_fn, type: String, optional: true}\n- {name: custom_config, type: String, optional: true}\noutputs:\n- {name: model, type: Model}\n- {name: model_run, type: ModelRun}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/Trainer/component.yaml'\nimplementation:\n  container:\n    image: tensorflow/tfx:0.29.0\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def Trainer(\n          examples_path,\n          model_path,\n          model_run_path,\n          train_args,\n          eval_args,\n          transform_graph_path = None,\n          schema_path = None,\n          base_model_path = None,\n          hyperparameters_path = None,\n          module_file = None,\n          run_fn = None,\n          trainer_fn = None,\n          custom_config = None,\n      ):\n          from tfx.components.trainer.component import Trainer as component_class\n\n          #Generated code\n          import os\n          import tempfile\n          from tensorflow.io import gfile\n          from google.protobuf import json_format, message\n          from tfx.types import channel_utils, artifact_utils\n          from tfx.components.base import base_executor\n\n          arguments = locals().copy()\n\n          component_class_args = {}\n\n          for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():\n              argument_value = arguments.get(name, None)\n              if argument_value is None:\n                  continue\n              parameter_type = execution_parameter.type\n              if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):\n                  argument_value_obj = parameter_type()\n                  json_format.Parse(argument_value, argument_value_obj)\n              else:\n                  argument_value_obj = argument_value\n              component_class_args[name] = argument_value_obj\n\n          for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():\n              artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel_parameter.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:\n                      # Recovering splits\n                      subdirs = gfile.listdir(artifact_path)\n                      # Workaround for https://github.com/tensorflow/tensorflow/issues/39167\n                      subdirs = [subdir.rstrip('/') for subdir in subdirs]\n                      split_names = [subdir.replace('Split-', '') for subdir in subdirs]\n                      artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))\n                  component_class_args[name] = channel_utils.as_channel([artifact])\n\n          component_class_instance = component_class(**component_class_args)\n\n          input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())\n          output_dict = {}\n          exec_properties = component_class_instance.exec_properties\n\n          # Generating paths for output artifacts\n          for name, channel in component_class_instance.outputs.items():\n              artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  artifact_list = [artifact]\n                  channel._artifacts = artifact_list\n                  output_dict[name] = artifact_list\n\n          print('component instance: ' + str(component_class_instance))\n\n          executor_context = base_executor.BaseExecutor.Context(\n              beam_pipeline_args=arguments.get('beam_pipeline_args'),\n              tmp_dir=tempfile.gettempdir(),\n              unique_id='tfx_component',\n          )\n          executor = component_class_instance.executor_spec.executor_class(executor_context)\n          executor.Do(\n              input_dict=input_dict,\n              output_dict=output_dict,\n              exec_properties=exec_properties,\n          )\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Trainer', description='')\n      _parser.add_argument(\"--examples\", dest=\"examples_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--train-args\", dest=\"train_args\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--eval-args\", dest=\"eval_args\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--transform-graph\", dest=\"transform_graph_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--schema\", dest=\"schema_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--base-model\", dest=\"base_model_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--hyperparameters\", dest=\"hyperparameters_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--module-file\", dest=\"module_file\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--run-fn\", dest=\"run_fn\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--trainer-fn\", dest=\"trainer_fn\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--custom-config\", dest=\"custom_config\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model-run\", dest=\"model_run_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = Trainer(**_parsed_args)\n    args:\n    - --examples\n    - {inputPath: examples}\n    - --train-args\n    - {inputValue: train_args}\n    - --eval-args\n    - {inputValue: eval_args}\n    - if:\n        cond: {isPresent: transform_graph}\n        then:\n        - --transform-graph\n        - {inputPath: transform_graph}\n    - if:\n        cond: {isPresent: schema}\n        then:\n        - --schema\n        - {inputPath: schema}\n    - if:\n        cond: {isPresent: base_model}\n        then:\n        - --base-model\n        - {inputPath: base_model}\n    - if:\n        cond: {isPresent: hyperparameters}\n        then:\n        - --hyperparameters\n        - {inputPath: hyperparameters}\n    - if:\n        cond: {isPresent: module_file}\n        then:\n        - --module-file\n        - {inputValue: module_file}\n    - if:\n        cond: {isPresent: run_fn}\n        then:\n        - --run-fn\n        - {inputValue: run_fn}\n    - if:\n        cond: {isPresent: trainer_fn}\n        then:\n        - --trainer-fn\n        - {inputValue: trainer_fn}\n    - if:\n        cond: {isPresent: custom_config}\n        then:\n        - --custom-config\n        - {inputValue: custom_config}\n    - --model\n    - {outputPath: model}\n    - --model-run\n    - {outputPath: model_run}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/Trainer/component.yaml"
              },
              {
                "text": "name: Evaluator\ninputs:\n- {name: examples, type: Examples}\n- {name: model, type: Model, optional: true}\n- {name: baseline_model, type: Model, optional: true}\n- {name: schema, type: Schema, optional: true}\n- name: eval_config\n  type:\n    JsonObject: {data_type: 'proto:tensorflow_model_analysis.EvalConfig'}\n  optional: true\n- name: feature_slicing_spec\n  type:\n    JsonObject: {data_type: 'proto:tfx.components.evaluator.FeatureSlicingSpec'}\n  optional: true\n- {name: fairness_indicator_thresholds, type: JsonArray, optional: true}\n- {name: example_splits, type: String, optional: true}\n- {name: module_file, type: String, optional: true}\n- {name: module_path, type: String, optional: true}\noutputs:\n- {name: evaluation, type: ModelEvaluation}\n- {name: blessing, type: ModelBlessing}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/deprecated/tfx/Evaluator/component.yaml'\nimplementation:\n  container:\n    image: tensorflow/tfx:0.29.0\n    command:\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def Evaluator(\n          examples_path,\n          evaluation_path,\n          blessing_path,\n          model_path = None,\n          baseline_model_path = None,\n          schema_path = None,\n          eval_config = None,\n          feature_slicing_spec = None,\n          fairness_indicator_thresholds = None,\n          example_splits = None,\n          module_file = None,\n          module_path = None,\n      ):\n          from tfx.components.evaluator.component import Evaluator as component_class\n\n          #Generated code\n          import os\n          import tempfile\n          from tensorflow.io import gfile\n          from google.protobuf import json_format, message\n          from tfx.types import channel_utils, artifact_utils\n          from tfx.components.base import base_executor\n\n          arguments = locals().copy()\n\n          component_class_args = {}\n\n          for name, execution_parameter in component_class.SPEC_CLASS.PARAMETERS.items():\n              argument_value = arguments.get(name, None)\n              if argument_value is None:\n                  continue\n              parameter_type = execution_parameter.type\n              if isinstance(parameter_type, type) and issubclass(parameter_type, message.Message):\n                  argument_value_obj = parameter_type()\n                  json_format.Parse(argument_value, argument_value_obj)\n              else:\n                  argument_value_obj = argument_value\n              component_class_args[name] = argument_value_obj\n\n          for name, channel_parameter in component_class.SPEC_CLASS.INPUTS.items():\n              artifact_path = arguments.get(name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel_parameter.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  if channel_parameter.type.PROPERTIES and 'split_names' in channel_parameter.type.PROPERTIES:\n                      # Recovering splits\n                      subdirs = gfile.listdir(artifact_path)\n                      # Workaround for https://github.com/tensorflow/tensorflow/issues/39167\n                      subdirs = [subdir.rstrip('/') for subdir in subdirs]\n                      split_names = [subdir.replace('Split-', '') for subdir in subdirs]\n                      artifact.split_names = artifact_utils.encode_split_names(sorted(split_names))\n                  component_class_args[name] = channel_utils.as_channel([artifact])\n\n          component_class_instance = component_class(**component_class_args)\n\n          input_dict = channel_utils.unwrap_channel_dict(component_class_instance.inputs.get_all())\n          output_dict = {}\n          exec_properties = component_class_instance.exec_properties\n\n          # Generating paths for output artifacts\n          for name, channel in component_class_instance.outputs.items():\n              artifact_path = arguments.get('output_' + name + '_uri') or arguments.get(name + '_path')\n              if artifact_path:\n                  artifact = channel.type()\n                  artifact.uri = artifact_path.rstrip('/') + '/'  # Some TFX components require that the artifact URIs end with a slash\n                  artifact_list = [artifact]\n                  channel._artifacts = artifact_list\n                  output_dict[name] = artifact_list\n\n          print('component instance: ' + str(component_class_instance))\n\n          executor_context = base_executor.BaseExecutor.Context(\n              beam_pipeline_args=arguments.get('beam_pipeline_args'),\n              tmp_dir=tempfile.gettempdir(),\n              unique_id='tfx_component',\n          )\n          executor = component_class_instance.executor_spec.executor_class(executor_context)\n          executor.Do(\n              input_dict=input_dict,\n              output_dict=output_dict,\n              exec_properties=exec_properties,\n          )\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Evaluator', description='')\n      _parser.add_argument(\"--examples\", dest=\"examples_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--baseline-model\", dest=\"baseline_model_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--schema\", dest=\"schema_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--eval-config\", dest=\"eval_config\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--feature-slicing-spec\", dest=\"feature_slicing_spec\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--fairness-indicator-thresholds\", dest=\"fairness_indicator_thresholds\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--example-splits\", dest=\"example_splits\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--module-file\", dest=\"module_file\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--module-path\", dest=\"module_path\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--evaluation\", dest=\"evaluation_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--blessing\", dest=\"blessing_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = Evaluator(**_parsed_args)\n    args:\n    - --examples\n    - {inputPath: examples}\n    - if:\n        cond: {isPresent: model}\n        then:\n        - --model\n        - {inputPath: model}\n    - if:\n        cond: {isPresent: baseline_model}\n        then:\n        - --baseline-model\n        - {inputPath: baseline_model}\n    - if:\n        cond: {isPresent: schema}\n        then:\n        - --schema\n        - {inputPath: schema}\n    - if:\n        cond: {isPresent: eval_config}\n        then:\n        - --eval-config\n        - {inputValue: eval_config}\n    - if:\n        cond: {isPresent: feature_slicing_spec}\n        then:\n        - --feature-slicing-spec\n        - {inputValue: feature_slicing_spec}\n    - if:\n        cond: {isPresent: fairness_indicator_thresholds}\n        then:\n        - --fairness-indicator-thresholds\n        - {inputValue: fairness_indicator_thresholds}\n    - if:\n        cond: {isPresent: example_splits}\n        then:\n        - --example-splits\n        - {inputValue: example_splits}\n    - if:\n        cond: {isPresent: module_file}\n        then:\n        - --module-file\n        - {inputValue: module_file}\n    - if:\n        cond: {isPresent: module_path}\n        then:\n        - --module-path\n        - {inputValue: module_path}\n    - --evaluation\n    - {outputPath: evaluation}\n    - --blessing\n    - {outputPath: blessing}\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/deprecated/tfx/Evaluator/component.yaml"
              }
            ],
            "name": "TFX"
          }
        ],
        "name": "ML frameworks"
      },
      {
        "components": [
          {
            "text": "name: Convert csv to apache parquet\ndescription: |-\n  Converts CSV table to Apache Parquet.\n\n      [Apache Parquet](https://parquet.apache.org/)\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: data, type: CSV}\noutputs:\n- {name: output_data, type: ApacheParquet}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/from_CSV/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install\n      --quiet --no-warn-script-location 'pyarrow==0.17.1' --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_csv_to_apache_parquet(\n          data_path,\n          output_data_path,\n      ):\n          '''Converts CSV table to Apache Parquet.\n\n          [Apache Parquet](https://parquet.apache.org/)\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          from pyarrow import csv, parquet\n\n          table = csv.read_csv(data_path)\n          parquet.write_table(table, output_data_path)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert csv to apache parquet', description='Converts CSV table to Apache Parquet.\\n\\n    [Apache Parquet](https://parquet.apache.org/)\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--output-data\", dest=\"output_data_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = convert_csv_to_apache_parquet(**_parsed_args)\n\n      _output_serializers = [\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --data\n    - {inputPath: data}\n    - --output-data\n    - {outputPath: output_data}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/from_CSV/component.yaml"
          },
          {
            "text": "name: Convert tsv to apache parquet\ndescription: |-\n  Converts TSV table to Apache Parquet.\n\n      [Apache Parquet](https://parquet.apache.org/)\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: data, type: TSV}\noutputs:\n- {name: output_data, type: ApacheParquet}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/from_TSV/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install\n      --quiet --no-warn-script-location 'pyarrow==0.17.1' --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_tsv_to_apache_parquet(\n          data_path,\n          output_data_path,\n      ):\n          '''Converts TSV table to Apache Parquet.\n\n          [Apache Parquet](https://parquet.apache.org/)\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          from pyarrow import csv, parquet\n\n          table = csv.read_csv(data_path, parse_options=csv.ParseOptions(delimiter='\\t'))\n          parquet.write_table(table, output_data_path)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert tsv to apache parquet', description='Converts TSV table to Apache Parquet.\\n\\n    [Apache Parquet](https://parquet.apache.org/)\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--output-data\", dest=\"output_data_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = convert_tsv_to_apache_parquet(**_parsed_args)\n\n      _output_serializers = [\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --data\n    - {inputPath: data}\n    - --output-data\n    - {outputPath: output_data}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/from_TSV/component.yaml"
          },
          {
            "text": "name: Convert apache arrow feather to apache parquet\ndescription: |-\n  Converts Apache Arrow Feather to Apache Parquet.\n\n      [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)\n      [Apache Parquet](https://parquet.apache.org/)\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: data, type: ApacheArrowFeather}\noutputs:\n- {name: output_data, type: ApacheParquet}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/from_ApacheArrowFeather/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pyarrow==0.17.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install\n      --quiet --no-warn-script-location 'pyarrow==0.17.1' --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_apache_arrow_feather_to_apache_parquet(\n          data_path,\n          output_data_path,\n      ):\n          '''Converts Apache Arrow Feather to Apache Parquet.\n\n          [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)\n          [Apache Parquet](https://parquet.apache.org/)\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          from pyarrow import feather, parquet\n\n          table = feather.read_table(data_path)\n          parquet.write_table(table, output_data_path)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert apache arrow feather to apache parquet', description='Converts Apache Arrow Feather to Apache Parquet.\\n\\n    [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)\\n    [Apache Parquet](https://parquet.apache.org/)\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--output-data\", dest=\"output_data_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = convert_apache_arrow_feather_to_apache_parquet(**_parsed_args)\n\n      _output_serializers = [\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --data\n    - {inputPath: data}\n    - --output-data\n    - {outputPath: output_data}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/ApacheParquet/from_ApacheArrowFeather/component.yaml"
          },
          {
            "text": "name: Convert apache parquet to csv\ndescription: |-\n  Converts Apache Parquet to CSV.\n\n      [Apache Parquet](https://parquet.apache.org/)\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: data, type: ApacheParquet}\noutputs:\n- {name: output_data, type: CSV}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/to_CSV/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_apache_parquet_to_csv(\n          data_path,\n          output_data_path,\n      ):\n          '''Converts Apache Parquet to CSV.\n\n          [Apache Parquet](https://parquet.apache.org/)\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          from pyarrow import parquet\n\n          data_frame = parquet.read_pandas(data_path).to_pandas()\n          data_frame.to_csv(\n              output_data_path,\n              index=False,\n          )\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert apache parquet to csv', description='Converts Apache Parquet to CSV.\\n\\n    [Apache Parquet](https://parquet.apache.org/)\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--output-data\", dest=\"output_data_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = convert_apache_parquet_to_csv(**_parsed_args)\n    args:\n    - --data\n    - {inputPath: data}\n    - --output-data\n    - {outputPath: output_data}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/_converters/ApacheParquet/to_CSV/component.yaml"
          },
          {
            "text": "name: Convert apache parquet to tsv\ndescription: |-\n  Converts Apache Parquet to TSV.\n\n      [Apache Parquet](https://parquet.apache.org/)\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: data, type: ApacheParquet}\noutputs:\n- {name: output_data, type: TSV}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/to_TSV/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_apache_parquet_to_tsv(\n          data_path,\n          output_data_path,\n      ):\n          '''Converts Apache Parquet to TSV.\n\n          [Apache Parquet](https://parquet.apache.org/)\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          from pyarrow import parquet\n\n          data_frame = parquet.read_pandas(data_path).to_pandas()\n          data_frame.to_csv(\n              output_data_path,\n              index=False,\n              sep='\\t',\n          )\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert apache parquet to tsv', description='Converts Apache Parquet to TSV.\\n\\n    [Apache Parquet](https://parquet.apache.org/)\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--output-data\", dest=\"output_data_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = convert_apache_parquet_to_tsv(**_parsed_args)\n    args:\n    - --data\n    - {inputPath: data}\n    - --output-data\n    - {outputPath: output_data}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/_converters/ApacheParquet/to_TSV/component.yaml"
          },
          {
            "text": "name: Convert apache parquet to apache arrow feather\ndescription: |-\n  Converts Apache Parquet to Apache Arrow Feather.\n\n      [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)\n      [Apache Parquet](https://parquet.apache.org/)\n\n      Annotations:\n          author: Alexey Volkov <alexey.volkov@ark-kun.com>\ninputs:\n- {name: data, type: ApacheParquet}\noutputs:\n- {name: output_data, type: ApacheArrowFeather}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ApacheParquet/to_ApacheArrowFeather/component.yaml'\nimplementation:\n  container:\n    image: python:3.7\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'pyarrow==0.17.1' 'pandas==1.0.3' 'numpy<2'\n      --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_apache_parquet_to_apache_arrow_feather(\n          data_path,\n          output_data_path,\n      ):\n          '''Converts Apache Parquet to Apache Arrow Feather.\n\n          [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)\n          [Apache Parquet](https://parquet.apache.org/)\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n          '''\n          from pyarrow import feather, parquet\n\n          data_frame = parquet.read_pandas(data_path).to_pandas()\n          feather.write_feather(data_frame, output_data_path)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert apache parquet to apache arrow feather', description='Converts Apache Parquet to Apache Arrow Feather.\\n\\n    [Apache Arrow Feather](https://arrow.apache.org/docs/python/feather.html)\\n    [Apache Parquet](https://parquet.apache.org/)\\n\\n    Annotations:\\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--output-data\", dest=\"output_data_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = convert_apache_parquet_to_apache_arrow_feather(**_parsed_args)\n\n      _output_serializers = [\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --data\n    - {inputPath: data}\n    - --output-data\n    - {outputPath: output_data}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/96a177dd71d54c98573c4101d9a05ac801f1fa54/components/_converters/ApacheParquet/to_ApacheArrowFeather/component.yaml"
          },
          {
            "text": "name: Convert to XGBoostJsonModel from XGBoostModel\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/XGBoostJsonModel/from_XGBoostModel/component.yaml'}\ninputs:\n- {name: model, type: XGBoostModel}\noutputs:\n- {name: converted_model, type: XGBoostJsonModel}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'xgboost==1.5.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'xgboost==1.5.0' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_to_XGBoostJsonModel_from_XGBoostModel(\n          model_path,\n          converted_model_path,\n      ):\n          import os\n          import xgboost\n\n          model = xgboost.Booster(model_file=model_path)\n\n          # The file path needs to have .json extension so that the model is saved in the JSON format.\n          tmp_converted_model_path = converted_model_path + \".json\"\n          model.save_model(tmp_converted_model_path)\n          os.rename(tmp_converted_model_path, converted_model_path)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert to XGBoostJsonModel from XGBoostModel', description='')\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--converted-model\", dest=\"converted_model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = convert_to_XGBoostJsonModel_from_XGBoostModel(**_parsed_args)\n    args:\n    - --model\n    - {inputPath: model}\n    - --converted-model\n    - {outputPath: converted_model}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/4fca0aa607c00e60d5eb342630acc175e6f51fc2/components/_converters/XGBoostJsonModel/from_XGBoostModel/component.yaml"
          },
          {
            "text": "name: Convert to XGBoostModel from XGBoostJsonModel\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/XGBoostJsonModel/to_XGBoostModel/component.yaml'}\ninputs:\n- {name: model, type: XGBoostJsonModel}\noutputs:\n- {name: converted_model, type: XGBoostModel}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'xgboost==1.5.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'xgboost==1.5.0' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_to_XGBoostModel_from_XGBoostJsonModel(\n          model_path,\n          converted_model_path,\n      ):\n          import os\n          import shutil\n          import tempfile\n          import xgboost\n\n          # The file path needs to have .json extension so that the model is loaded as JSON format.\n          with tempfile.NamedTemporaryFile(suffix=\".json\") as tmp_model_file:\n              tmp_model_path = tmp_model_file.name\n              shutil.copy(model_path, tmp_model_path)\n              model = xgboost.Booster(model_file=tmp_model_path)\n\n          tmp_converted_model_path = converted_model_path + \".bst\"\n          model.save_model(tmp_converted_model_path)\n          os.rename(tmp_converted_model_path, converted_model_path)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert to XGBoostModel from XGBoostJsonModel', description='')\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--converted-model\", dest=\"converted_model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = convert_to_XGBoostModel_from_XGBoostJsonModel(**_parsed_args)\n    args:\n    - --model\n    - {inputPath: model}\n    - --converted-model\n    - {outputPath: converted_model}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/4fca0aa607c00e60d5eb342630acc175e6f51fc2/components/_converters/XGBoostJsonModel/to_XGBoostModel/component.yaml"
          },
          {
            "text": "name: Convert to OnnxModel from XGBoostJsonModel\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/from_XGBoostJsonModel/component.yaml'}\ninputs:\n- {name: model, type: XGBoostJsonModel}\n- {name: model_graph_name, type: String, optional: true}\n- {name: doc_string, type: String, default: '', optional: true}\n- {name: target_opset, type: Integer, optional: true}\noutputs:\n- {name: converted_model, type: OnnxModel}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'xgboost==1.5.2' 'onnx==1.11.0' 'onnxmltools==1.10.0' || PIP_DISABLE_PIP_VERSION_CHECK=1\n      python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.5.2' 'onnx==1.11.0'\n      'onnxmltools==1.10.0' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_to_OnnxModel_from_XGBoostJsonModel(\n          model_path,\n          converted_model_path,\n          model_graph_name = None,\n          doc_string = \"\",\n          target_opset = None,\n      ):\n          import xgboost\n          import onnx\n          import onnxmltools\n\n          # The file path needs to have .json extension so that the model is loaded as JSON format.\n          import os\n          import shutil\n          import tempfile\n          with tempfile.NamedTemporaryFile(suffix=\".json\") as tmp_model_file:\n              tmp_model_path = tmp_model_file.name\n              shutil.copy(model_path, tmp_model_path)\n              model = xgboost.Booster(model_file=tmp_model_path)\n\n          # Workaround for https://github.com/onnx/onnxmltools/issues/499\n          # Although I'm not sure this formula is correct given https://github.com/dmlc/xgboost/pull/6569\n          model.best_ntree_limit = model.num_boosted_rounds()\n\n          converted_model = onnxmltools.convert_xgboost(\n              model=model,\n              name=model_graph_name,\n              initial_types=[\n                  (\n                      \"input\",\n                      onnxmltools.convert.common.data_types.FloatTensorType(\n                          shape=[None, model.num_features()]\n                      ),\n                  )\n              ],\n              doc_string=doc_string,\n              target_opset=target_opset,\n          )\n          onnx.save_model(proto=converted_model, f=converted_model_path)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert to OnnxModel from XGBoostJsonModel', description='')\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model-graph-name\", dest=\"model_graph_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--doc-string\", dest=\"doc_string\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--target-opset\", dest=\"target_opset\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--converted-model\", dest=\"converted_model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = convert_to_OnnxModel_from_XGBoostJsonModel(**_parsed_args)\n    args:\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: model_graph_name}\n        then:\n        - --model-graph-name\n        - {inputValue: model_graph_name}\n    - if:\n        cond: {isPresent: doc_string}\n        then:\n        - --doc-string\n        - {inputValue: doc_string}\n    - if:\n        cond: {isPresent: target_opset}\n        then:\n        - --target-opset\n        - {inputValue: target_opset}\n    - --converted-model\n    - {outputPath: converted_model}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/4619f84eebc54230153ba48b0e67ac8446dc31c6/components/_converters/OnnxModel/from_XGBoostJsonModel/component.yaml"
          },
          {
            "text": "name: Convert to OnnxModel from XGBoostModel\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/from_XGBoostModel/component.yaml'}\ninputs:\n- {name: model, type: XGBoostModel}\n- {name: model_graph_name, type: String, optional: true}\n- {name: doc_string, type: String, default: '', optional: true}\n- {name: target_opset, type: Integer, optional: true}\noutputs:\n- {name: converted_model, type: OnnxModel}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'xgboost==1.5.2' 'onnx==1.11.0' 'onnxmltools==1.10.0' || PIP_DISABLE_PIP_VERSION_CHECK=1\n      python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.5.2' 'onnx==1.11.0'\n      'onnxmltools==1.10.0' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_to_OnnxModel_from_XGBoostModel(\n          model_path,\n          converted_model_path,\n          model_graph_name = None,\n          doc_string = \"\",\n          target_opset = None,\n      ):\n          import xgboost\n          import onnx\n          import onnxmltools\n\n          model = xgboost.Booster(model_file=model_path)\n\n          # Workaround for https://github.com/onnx/onnxmltools/issues/499\n          # Although I'm not sure this formula is correct given https://github.com/dmlc/xgboost/pull/6569\n          model.best_ntree_limit = model.num_boosted_rounds()\n\n          converted_model = onnxmltools.convert_xgboost(\n              model=model,\n              name=model_graph_name,\n              initial_types=[\n                  (\n                      \"input\",\n                      onnxmltools.convert.common.data_types.FloatTensorType(\n                          shape=[None, model.num_features()]\n                      ),\n                  )\n              ],\n              doc_string=doc_string,\n              target_opset=target_opset,\n          )\n          onnx.save_model(proto=converted_model, f=converted_model_path)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert to OnnxModel from XGBoostModel', description='')\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model-graph-name\", dest=\"model_graph_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--doc-string\", dest=\"doc_string\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--target-opset\", dest=\"target_opset\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--converted-model\", dest=\"converted_model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = convert_to_OnnxModel_from_XGBoostModel(**_parsed_args)\n    args:\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: model_graph_name}\n        then:\n        - --model-graph-name\n        - {inputValue: model_graph_name}\n    - if:\n        cond: {isPresent: doc_string}\n        then:\n        - --doc-string\n        - {inputValue: doc_string}\n    - if:\n        cond: {isPresent: target_opset}\n        then:\n        - --target-opset\n        - {inputValue: target_opset}\n    - --converted-model\n    - {outputPath: converted_model}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/ae9d04e833d973d785809e7734021b06bcfea9bc/components/_converters/OnnxModel/from_XGBoostModel/component.yaml"
          },
          {
            "text": "name: To ONNX from Tensorflow SavedModel\ninputs:\n- {name: Model, type: TensorflowSavedModel}\noutputs:\n- {name: Model, type: OnnxModel}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/from_TensorflowSavedModel/component.yaml'\nimplementation:\n  container:\n    image: tensorflow/tensorflow:2.3.0\n    command:\n    - sh\n    - -exc\n    - python3 -m pip install tf2onnx==1.6.3 && \"$0\" \"$@\"\n    - python3\n    - -m\n    - tf2onnx.convert\n    - --saved-model\n    - {inputPath: Model}\n    - --output\n    - {outputPath: Model}\n    - --fold_const\n    - --verbose\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/_converters/OnnxModel/from_TensorflowSavedModel/component.yaml"
          },
          {
            "text": "name: Convert to TensorflowSavedModel from OnnxModel\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/OnnxModel/to_TensorflowSavedModel/component.yaml'\ninputs:\n- {name: model, type: OnnxModel}\noutputs:\n- {name: converted_model, type: TensorflowSavedModel}\nimplementation:\n  container:\n    image: tensorflow/tensorflow:2.8.0\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'onnx-tf==1.9.0' 'onnx==1.11.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m\n      pip install --quiet --no-warn-script-location 'onnx-tf==1.9.0' 'onnx==1.11.0'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_to_TensorflowSavedModel_from_OnnxModel(\n          model_path,\n          converted_model_path,\n      ):\n          import onnx\n          import onnx_tf\n\n          onnx_model = onnx.load(model_path)\n          tf_rep = onnx_tf.backend.prepare(onnx_model)\n          tf_rep.export_graph(converted_model_path)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert to TensorflowSavedModel from OnnxModel', description='')\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--converted-model\", dest=\"converted_model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = convert_to_TensorflowSavedModel_from_OnnxModel(**_parsed_args)\n    args:\n    - --model\n    - {inputPath: model}\n    - --converted-model\n    - {outputPath: converted_model}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/2fc275072568cd0cdf73743ab49aa90928303f2c/components/_converters/OnnxModel/to_TensorflowSavedModel/component.yaml"
          },
          {
            "text": "name: Convert to OnnxModel from ScikitLearnPickleModel\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/_converters/ScikitLearnPickleModel/to_OnnxModel/component.yaml'}\ninputs:\n- {name: model, type: ScikitLearnPickleModel}\n- {name: doc_string, type: String, default: '', optional: true}\n- {name: target_opset, type: Integer, optional: true}\noutputs:\n- {name: converted_model, type: OnnxModel}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'skl2onnx==1.11' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet\n      --no-warn-script-location 'skl2onnx==1.11' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def _make_parent_dirs_and_return_path(file_path: str):\n          import os\n          os.makedirs(os.path.dirname(file_path), exist_ok=True)\n          return file_path\n\n      def convert_to_OnnxModel_from_ScikitLearnPickleModel(\n          model_path,\n          converted_model_path,\n          doc_string = \"\",\n          target_opset = None,\n      ):\n          import onnx\n          import pickle\n          import skl2onnx\n\n          with open(model_path, \"rb\") as model_file:\n              model = pickle.load(model_file)\n\n          # Funny hack to infer the model input shape\n          # Just try passing arrays of different size to the model.predict method and check what works, lol.\n          def get_input_output_shapes(model):\n              for input_length_candidate in range(100000):\n                  try:\n                      prediction = model.predict(X=[[0.0] * input_length_candidate])\n                      input_length = input_length_candidate\n                      output_shape = prediction.shape[1:]\n                      return (input_length, output_shape)\n                  except:\n                      pass\n              return None\n\n          input_length, _ = get_input_output_shapes(model)\n\n          # Setting model name is not necessary, but why not.\n          model_type = type(model)\n          model_type_name = model_type.__module__ + \".\" + model_type.__name__\n\n          onnx_model = skl2onnx.convert_sklearn(\n              model=model,\n              initial_types=[\n                  (\"input\", skl2onnx.common.data_types.FloatTensorType([None, input_length]))\n              ],\n              name=model_type_name,\n              verbose=1,\n              # TODO: Include the original model hash digest so that the model can be traced.\n              doc_string=doc_string,\n              target_opset=target_opset,\n          )\n          print(onnx_model)\n          onnx.save_model(\n              proto=onnx_model, f=converted_model_path,\n          )\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Convert to OnnxModel from ScikitLearnPickleModel', description='')\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--doc-string\", dest=\"doc_string\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--target-opset\", dest=\"target_opset\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--converted-model\", dest=\"converted_model_path\", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n      _parsed_args = vars(_parser.parse_args())\n\n      _outputs = convert_to_OnnxModel_from_ScikitLearnPickleModel(**_parsed_args)\n    args:\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: doc_string}\n        then:\n        - --doc-string\n        - {inputValue: doc_string}\n    - if:\n        cond: {isPresent: target_opset}\n        then:\n        - --target-opset\n        - {inputValue: target_opset}\n    - --converted-model\n    - {outputPath: converted_model}\n",
            "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/f7f79decd10955b8408541f6f695a614538a6901/components/_converters/ScikitLearnPickleModel/to_OnnxModel/component.yaml"
          }
        ],
        "name": "Converters"
      },
      {
        "folders": [
          {
            "folders": [
              {
                "components": [
                  {
                    "text": "name: Create tabular dataset from CSV for Google Cloud Vertex AI\ndescription: Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in\n  GCS.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_GCS/component.yaml'}\ninputs:\n- {name: data, type: CSV, description: Data in CSV format that should be imported\n    into the dataset.}\n- name: display_name\n  type: String\n  description: |-\n    Display name for the AutoML Dataset.\n    Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.\n  optional: true\n- {name: labels, type: JsonObject, optional: true}\n- {name: project, type: String, description: 'Google Cloud project ID. If not set,\n    the default one will be used.', optional: true}\n- {name: location, type: String, description: Google Cloud region. AutoML Tables only\n    supports us-central1., default: us-central1, optional: true}\n- {name: encryption_spec_key_name, type: String, optional: true}\n- {name: staging_bucket, type: String, optional: true}\noutputs:\n- {name: dataset_name, type: GoogleCloudVertexAiTabularDatasetName}\n- {name: dataset_dict, type: JsonObject}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'git+https://github.com/Ark-kun/python-aiplatform@8f61efb3a7903a6e0ef47d957f26ef3083581c7e#egg=google-cloud-aiplatform&subdirectory=.'\n      'google-api-python-client==2.29.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'git+https://github.com/Ark-kun/python-aiplatform@8f61efb3a7903a6e0ef47d957f26ef3083581c7e#egg=google-cloud-aiplatform&subdirectory=.'\n      'google-api-python-client==2.29.0' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def create_tabular_dataset_from_CSV_for_Google_Cloud_Vertex_AI(\n          data_path,  # data_type: \"CSV\"\n          display_name = None,\n          labels = None,\n          project = None,\n          location = 'us-central1',\n          encryption_spec_key_name = None,\n          staging_bucket = None,\n      ):\n          '''Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n\n          Args:\n              data_path: Data in CSV format that should be imported into the dataset.\n              display_name: Display name for the AutoML Dataset.\n                  Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.\n              project: Google Cloud project ID. If not set, the default one will be used.\n              location: Google Cloud region. AutoML Tables only supports us-central1.\n          Returns:\n              dataset_name: Dataset name (fully-qualified)\n              dataset_dict: Dataset object in JSON format\n          '''\n\n          import datetime\n          import json\n          import logging\n          import os\n          import tempfile\n\n          from google.cloud import aiplatform\n          from google.cloud.aiplatform import utils as aiplatform_utils\n          from google.protobuf import json_format\n\n          logging.getLogger().setLevel(logging.INFO)\n\n          if not display_name:\n              display_name = 'Dataset_' + datetime.datetime.utcnow().strftime(\"%Y_%m_%d_%H_%M_%S\")\n\n          # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.\n          # This leads to failure when trying to create any resource in the project.\n          # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).\n          # We can try and get the GCP project ID/number from the environment variables.\n          if not project:\n              project_number = os.environ.get(\"CLOUD_ML_PROJECT_ID\")\n              if project_number:\n                  print(f\"Inferred project number: {project_number}\")\n                  project = project_number\n                  # To improve the naming we try to convert the project number into the user project ID.\n                  try:\n                      from googleapiclient import discovery\n\n                      cloud_resource_manager_service = discovery.build(\n                          \"cloudresourcemanager\", \"v3\"\n                      )\n                      project_id = (\n                          cloud_resource_manager_service.projects()\n                          .get(name=f\"projects/{project_number}\")\n                          .execute()[\"projectId\"]\n                      )\n                      if project_id:\n                          print(f\"Inferred project ID: {project_id}\")\n                          project = project_id\n                  except Exception as e:\n                      print(e)\n\n          if not location:\n              location = os.environ.get(\"CLOUD_ML_REGION\")\n\n          aiplatform.init(\n              project=project,\n              location=location,\n              staging_bucket=staging_bucket,\n              encryption_spec_key_name=encryption_spec_key_name,\n          )\n\n          # Stage the data\n          # The file needs to have .CSV extension, so we need to rename or link it.\n          staging_dir = tempfile.mkdtemp()\n          staged_data_path = os.path.join(staging_dir, \"dataset.csv\")\n          # Need to use symlink to prevent OSError: [Errno 18] Invalid cross-device link.\n          os.symlink(src=data_path, dst=staged_data_path)\n          staged_data_uri = aiplatform_utils.stage_local_data_in_gcs(data_path=staged_data_path)\n\n          labels = labels or {}\n          labels[\"component-source\"] = \"github-com-ark-kun-pipeline-components\"\n\n          # Create the dataset\n          dataset = aiplatform.TabularDataset.create(\n              display_name=display_name,\n              gcs_source=staged_data_uri,\n              labels=labels,\n          )\n          (_, dataset_project, _, dataset_location, _, dataset_id) = dataset.resource_name.split('/')\n          dataset_web_url = f'https://console.cloud.google.com/vertex-ai/locations/{dataset_location}/datasets/{dataset_id}/analyze?project={dataset_project}'\n          logging.info(f'Created dataset {dataset.name}.')\n          logging.info(f'Link: {dataset_web_url}')\n          dataset_json = json.dumps(dataset.to_dict(), indent=2)\n          print(dataset_json)\n          return (dataset.resource_name, dataset_json, dataset_web_url)\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Create tabular dataset from CSV for Google Cloud Vertex AI', description='Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.')\n      _parser.add_argument(\"--data\", dest=\"data_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--display-name\", dest=\"display_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--labels\", dest=\"labels\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--project\", dest=\"project\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--location\", dest=\"location\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--encryption-spec-key-name\", dest=\"encryption_spec_key_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--staging-bucket\", dest=\"staging_bucket\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = create_tabular_dataset_from_CSV_for_Google_Cloud_Vertex_AI(**_parsed_args)\n\n      _output_serializers = [\n          str,\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --data\n    - {inputPath: data}\n    - if:\n        cond: {isPresent: display_name}\n        then:\n        - --display-name\n        - {inputValue: display_name}\n    - if:\n        cond: {isPresent: labels}\n        then:\n        - --labels\n        - {inputValue: labels}\n    - if:\n        cond: {isPresent: project}\n        then:\n        - --project\n        - {inputValue: project}\n    - if:\n        cond: {isPresent: location}\n        then:\n        - --location\n        - {inputValue: location}\n    - if:\n        cond: {isPresent: encryption_spec_key_name}\n        then:\n        - --encryption-spec-key-name\n        - {inputValue: encryption_spec_key_name}\n    - if:\n        cond: {isPresent: staging_bucket}\n        then:\n        - --staging-bucket\n        - {inputValue: staging_bucket}\n    - '----output-paths'\n    - {outputPath: dataset_name}\n    - {outputPath: dataset_dict}\n",
                    "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/47f3621344c884666a926c8a15d77562f1cc5e0a/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_CSV/component.yaml"
                  },
                  {
                    "text": "name: Create tabular dataset from GCS for Google Cloud Vertex AI\ndescription: Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in\n  GCS.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_GCS/component.yaml'}\ninputs:\n- name: data_uri\n  type: GoogleCloudStorageUri\n  description: |-\n    Google Cloud Storage URI pointing to the data in CSV format that should be imported into the dataset.\n    The bucket must be a regional bucket in the us-central1 region.\n    The file name must have a (case-insensitive) '.CSV' file extension.\n- name: display_name\n  type: String\n  description: |-\n    Display name for the AutoML Dataset.\n    Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.\n  optional: true\n- name: encryption_spec_key_name\n  type: String\n  description: |-\n    Optional. The Cloud KMS resource identifier of the customer\n    managed encryption key used to protect a resource. Has the\n    form:\n    ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.\n    The key needs to be in the same region as where the compute\n    resource is created.\n  optional: true\n- {name: project, type: String, description: 'Google Cloud project ID. If not set,\n    the default one will be used.', optional: true}\n- {name: location, type: String, description: Google Cloud region. AutoML Tables only\n    supports us-central1., default: us-central1, optional: true}\noutputs:\n- {name: dataset_name, type: GoogleCloudVertexAiTabularDatasetName}\n- {name: dataset_dict, type: JsonObject}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'google-cloud-aiplatform==1.1.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.1.1'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def create_tabular_dataset_from_GCS_for_Google_Cloud_Vertex_AI(\n          data_uri,  # data_type: \"CSV\"\n          display_name = None,\n          encryption_spec_key_name = None,\n          project = None,\n          location = 'us-central1',\n      ):\n          '''Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n\n          Args:\n              data_uri: Google Cloud Storage URI pointing to the data in CSV format that should be imported into the dataset.\n                  The bucket must be a regional bucket in the us-central1 region.\n                  The file name must have a (case-insensitive) '.CSV' file extension.\n              display_name: Display name for the AutoML Dataset.\n                  Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.\n              encryption_spec_key_name (Optional[str]):\n                  Optional. The Cloud KMS resource identifier of the customer\n                  managed encryption key used to protect a resource. Has the\n                  form:\n                  ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.\n                  The key needs to be in the same region as where the compute\n                  resource is created.\n              project: Google Cloud project ID. If not set, the default one will be used.\n              location: Google Cloud region. AutoML Tables only supports us-central1.\n          Returns:\n              dataset_name: Dataset name (fully-qualified)\n              dataset_dict: Dataset object in JSON format\n          '''\n\n          import datetime\n          import json\n          import logging\n          import os\n\n          from google.cloud import aiplatform\n          from google.protobuf import json_format\n\n          logging.getLogger().setLevel(logging.INFO)\n\n          if not display_name:\n              display_name = 'Dataset_' + datetime.datetime.utcnow().strftime(\"%Y_%m_%d_%H_%M_%S\")\n\n          # Hack to enable passing multiple URIs\n          # I could have created another component or added another input, but it seems to be too much hassle for now.\n          # An alternative would have been to accept comma-delimited or semicolon-delimited URLs.\n          if data_uri.startswith(\"[\"):\n              data_uris = json.loads(data_uri)\n          else:\n              data_uris = [data_uri]\n\n          # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.\n          # This leads to failure when trying to create any resource in the project.\n          # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).\n          # We can try and get the GCP project ID/number from the environment variables.\n          if not project:\n              project_number = os.environ.get(\"CLOUD_ML_PROJECT_ID\")\n              if project_number:\n                  print(f\"Inferred project number: {project_number}\")\n                  project = project_number\n\n          if not location:\n              location = os.environ.get(\"CLOUD_ML_REGION\")\n\n          aiplatform.init(\n              project=project,\n              location=location,\n              encryption_spec_key_name=encryption_spec_key_name,\n          )\n          dataset = aiplatform.TabularDataset.create(\n              display_name=display_name,\n              gcs_source=data_uris,\n          )\n          (_, dataset_project, _, dataset_location, _, dataset_id) = dataset.resource_name.split('/')\n          dataset_web_url = f'https://console.cloud.google.com/vertex-ai/locations/{dataset_location}/datasets/{dataset_id}/analyze?project={dataset_project}'\n          logging.info(f'Created dataset {dataset.name}.')\n          logging.info(f'Link: {dataset_web_url}')\n          dataset_json = json_format.MessageToJson(dataset._gca_resource._pb)\n          print(dataset_json)\n          return (dataset.resource_name, dataset_json, dataset_web_url)\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Create tabular dataset from GCS for Google Cloud Vertex AI', description='Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.')\n      _parser.add_argument(\"--data-uri\", dest=\"data_uri\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--display-name\", dest=\"display_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--encryption-spec-key-name\", dest=\"encryption_spec_key_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--project\", dest=\"project\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--location\", dest=\"location\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = create_tabular_dataset_from_GCS_for_Google_Cloud_Vertex_AI(**_parsed_args)\n\n      _output_serializers = [\n          str,\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --data-uri\n    - {inputValue: data_uri}\n    - if:\n        cond: {isPresent: display_name}\n        then:\n        - --display-name\n        - {inputValue: display_name}\n    - if:\n        cond: {isPresent: encryption_spec_key_name}\n        then:\n        - --encryption-spec-key-name\n        - {inputValue: encryption_spec_key_name}\n    - if:\n        cond: {isPresent: project}\n        then:\n        - --project\n        - {inputValue: project}\n    - if:\n        cond: {isPresent: location}\n        then:\n        - --location\n        - {inputValue: location}\n    - '----output-paths'\n    - {outputPath: dataset_name}\n    - {outputPath: dataset_dict}\n",
                    "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/00d020c29a144cee7fd35f2d05053addb942f536/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_GCS/component.yaml"
                  },
                  {
                    "text": "name: Create tabular dataset from BigQuery for Google Cloud Vertex AI\ndescription: Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in\n  GCS.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_BigQuery/component.yaml'}\ninputs:\n- name: data_uri\n  type: GoogleCloudBigQueryUri\n  description: |-\n    Google Cloud BigQuery URI pointing to the data that should be imported into the dataset.\n    The bucket must be a regional bucket in the us-central1 region.\n    The file name must have a (case-insensitive) '.CSV' file extension.\n- name: display_name\n  type: String\n  description: |-\n    Display name for the AutoML Dataset.\n    Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.\n  optional: true\n- name: encryption_spec_key_name\n  type: String\n  description: |-\n    Optional. The Cloud KMS resource identifier of the customer\n    managed encryption key used to protect a resource. Has the\n    form:\n    ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.\n    The key needs to be in the same region as where the compute\n    resource is created.\n  optional: true\n- {name: project, type: String, description: 'Google Cloud project ID. If not set,\n    the default one will be used.', optional: true}\n- {name: location, type: String, description: Google Cloud region. AutoML Tables only\n    supports us-central1., default: us-central1, optional: true}\noutputs:\n- {name: dataset_name, type: GoogleCloudVertexAiTabularDatasetName}\n- {name: dataset_dict, type: JsonObject}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'google-cloud-aiplatform==1.1.1' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.1.1'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def create_tabular_dataset_from_BigQuery_for_Google_Cloud_Vertex_AI(\n          data_uri,\n          display_name = None,\n          encryption_spec_key_name = None,\n          project = None,\n          location = 'us-central1',\n      ):\n          '''Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n\n          Args:\n              data_uri: Google Cloud BigQuery URI pointing to the data that should be imported into the dataset.\n                  The bucket must be a regional bucket in the us-central1 region.\n                  The file name must have a (case-insensitive) '.CSV' file extension.\n              display_name: Display name for the AutoML Dataset.\n                  Allowed characters are ASCII Latin letters A-Z and a-z, an underscore (_), and ASCII digits 0-9.\n              encryption_spec_key_name (Optional[str]):\n                  Optional. The Cloud KMS resource identifier of the customer\n                  managed encryption key used to protect a resource. Has the\n                  form:\n                  ``projects/my-project/locations/my-region/keyRings/my-kr/cryptoKeys/my-key``.\n                  The key needs to be in the same region as where the compute\n                  resource is created.\n              project: Google Cloud project ID. If not set, the default one will be used.\n              location: Google Cloud region. AutoML Tables only supports us-central1.\n          Returns:\n              dataset_name: Dataset name (fully-qualified)\n              dataset_dict: Dataset object in JSON format\n          '''\n\n          import datetime\n          import json\n          import logging\n          import os\n\n          from google.cloud import aiplatform\n          from google.protobuf import json_format\n\n          logging.getLogger().setLevel(logging.INFO)\n\n          if not display_name:\n              display_name = 'Dataset_' + datetime.datetime.utcnow().strftime(\"%Y_%m_%d_%H_%M_%S\")\n\n          # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.\n          # This leads to failure when trying to create any resource in the project.\n          # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).\n          # We can try and get the GCP project ID/number from the environment variables.\n          if not project:\n              project_number = os.environ.get(\"CLOUD_ML_PROJECT_ID\")\n              if project_number:\n                  print(f\"Inferred project number: {project_number}\")\n                  project = project_number\n\n          if not location:\n              location = os.environ.get(\"CLOUD_ML_REGION\")\n\n          aiplatform.init(\n              project=project,\n              location=location,\n              encryption_spec_key_name=encryption_spec_key_name,\n          )\n          dataset = aiplatform.TabularDataset.create(\n              display_name=display_name,\n              bq_source=data_uri,\n          )\n          (_, dataset_project, _, dataset_location, _, dataset_id) = dataset.resource_name.split('/')\n          dataset_web_url = f'https://console.cloud.google.com/vertex-ai/locations/{dataset_location}/datasets/{dataset_id}/analyze?project={dataset_project}'\n          logging.info(f'Created dataset {dataset.name}.')\n          logging.info(f'Link: {dataset_web_url}')\n          dataset_json = json_format.MessageToJson(dataset._gca_resource._pb)\n          print(dataset_json)\n          return (dataset.resource_name, dataset_json, dataset_web_url)\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Create tabular dataset from BigQuery for Google Cloud Vertex AI', description='Creates Google Cloud Vertex AI Tabular Dataset from CSV data stored in GCS.')\n      _parser.add_argument(\"--data-uri\", dest=\"data_uri\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--display-name\", dest=\"display_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--encryption-spec-key-name\", dest=\"encryption_spec_key_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--project\", dest=\"project\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--location\", dest=\"location\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = create_tabular_dataset_from_BigQuery_for_Google_Cloud_Vertex_AI(**_parsed_args)\n\n      _output_serializers = [\n          str,\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --data-uri\n    - {inputValue: data_uri}\n    - if:\n        cond: {isPresent: display_name}\n        then:\n        - --display-name\n        - {inputValue: display_name}\n    - if:\n        cond: {isPresent: encryption_spec_key_name}\n        then:\n        - --encryption-spec-key-name\n        - {inputValue: encryption_spec_key_name}\n    - if:\n        cond: {isPresent: project}\n        then:\n        - --project\n        - {inputValue: project}\n    - if:\n        cond: {isPresent: location}\n        then:\n        - --location\n        - {inputValue: location}\n    - '----output-paths'\n    - {outputPath: dataset_name}\n    - {outputPath: dataset_dict}\n",
                    "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/00d020c29a144cee7fd35f2d05053addb942f536/components/google-cloud/Vertex_AI/AutoML/Tables/Create_dataset/from_BigQuery/component.yaml"
                  },
                  {
                    "text": "name: Train tabular model using Google Cloud Vertex AI AutoML\ndescription: Trains model using Google Cloud Vertex AI AutoML.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Train_model/component.yaml'}\ninputs:\n- name: dataset_name\n  type: GoogleCloudVertexAiTabularDatasetName\n  description: |-\n    Required. The full name of dataset  (datasets.TabularDataset) within the same Project from which data will be used to train the Model. The\n    Dataset must use schema compatible with Model being trained,\n    and what is compatible should be described in the used\n    TrainingPipeline's [training_task_definition]\n    [google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition].\n    For tabular Datasets, all their data is exported to\n    training, to pick and choose from.\n- {name: target_column, type: String, description: Required. The name of the column\n    values of which the Model is to predict.}\n- name: optimization_prediction_type\n  type: String\n  description: |-\n    The type of prediction the Model is to produce.\n    \"classification\" - Predict one out of multiple target values is\n    picked for each row.\n    \"regression\" - Predict a value based on its relation to other values.\n    This type is available only to columns that contain\n    semantically numeric values, i.e. integers or floating\n    point number, even if stored as e.g. strings.\n- name: training_fraction_split\n  type: Float\n  description: |-\n    Required. The fraction of the input data that is to be\n    used to train the Model. This is ignored if Dataset is not provided.\n  default: '0.8'\n  optional: true\n- name: validation_fraction_split\n  type: Float\n  description: |-\n    Required. The fraction of the input data that is to be\n    used to validate the Model. This is ignored if Dataset is not provided.\n  default: '0.1'\n  optional: true\n- name: test_fraction_split\n  type: Float\n  description: |-\n    Required. The fraction of the input data that is to be\n    used to evaluate the Model. This is ignored if Dataset is not provided.\n  default: '0.1'\n  optional: true\n- name: predefined_split_column_name\n  type: String\n  description: |-\n    Optional. The key is a name of one of the Dataset's data\n    columns. The value of the key (either the label's value or\n    value in the column) must be one of {``training``,\n    ``validation``, ``test``}, and it defines to which set the\n    given piece of data is assigned. If for a piece of data the\n    key is not present or has an invalid value, that piece is\n    ignored by the pipeline.\n\n    Supported only for tabular and time series Datasets.\n  optional: true\n- name: weight_column\n  type: String\n  description: |-\n    Optional. Name of the column that should be used as the weight column.\n    Higher values in this column give more importance to the row\n    during Model training. The column must have numeric values between 0 and\n    10000 inclusively, and 0 value means that the row is ignored.\n    If the weight column field is not set, then all rows are assumed to have\n    equal weight of 1.\n  optional: true\n- name: budget_milli_node_hours\n  type: Integer\n  description: |-\n    Optional. The train budget of creating this Model, expressed in milli node\n    hours i.e. 1,000 value in this field means 1 node hour.\n    The training cost of the model will not exceed this budget. The final\n    cost will be attempted to be close to the budget, though may end up\n    being (even) noticeably smaller - at the backend's discretion. This\n    especially may happen when further model training ceases to provide\n    any improvements.\n    If the budget is set to a value known to be insufficient to train a\n    Model for the given training set, the training won't be attempted and\n    will error.\n    The minimum value is 1000 and the maximum is 72000.\n  default: '1000'\n  optional: true\n- name: model_display_name\n  type: String\n  description: |-\n    Optional. If the script produces a managed Vertex AI Model. The display name of\n    the Model. The name can be up to 128 characters long and can be consist\n    of any UTF-8 characters.\n\n    If not provided upon creation, the job's display_name is used.\n  optional: true\n- name: disable_early_stopping\n  type: Boolean\n  description: |-\n    Required. If true, the entire budget is used. This disables the early stopping\n    feature. By default, the early stopping feature is enabled, which means\n    that training might stop before the entire training budget has been\n    used, if further training does no longer brings significant improvement\n    to the model.\n  default: \"False\"\n  optional: true\n- name: optimization_objective\n  type: String\n  description: |-\n    Optional. Objective function the Model is to be optimized towards. The training\n    task creates a Model that maximizes/minimizes the value of the objective\n    function over the validation set.\n\n    The supported optimization objectives depend on the prediction type, and\n    in the case of classification also the number of distinct values in the\n    target column (two distint values -> binary, 3 or more distinct values\n    -> multi class).\n    If the field is not set, the default objective function is used.\n\n    Classification (binary):\n    \"maximize-au-roc\" (default) - Maximize the area under the receiver\n                                operating characteristic (ROC) curve.\n    \"minimize-log-loss\" - Minimize log loss.\n    \"maximize-au-prc\" - Maximize the area under the precision-recall curve.\n    \"maximize-precision-at-recall\" - Maximize precision for a specified\n                                    recall value.\n    \"maximize-recall-at-precision\" - Maximize recall for a specified\n                                    precision value.\n\n    Classification (multi class):\n    \"minimize-log-loss\" (default) - Minimize log loss.\n\n    Regression:\n    \"minimize-rmse\" (default) - Minimize root-mean-squared error (RMSE).\n    \"minimize-mae\" - Minimize mean-absolute error (MAE).\n    \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE).\n  optional: true\n- name: optimization_objective_recall_value\n  type: Float\n  description: |-\n    Optional. Required when maximize-precision-at-recall optimizationObjective was\n    picked, represents the recall value at which the optimization is done.\n\n    The minimum value is 0 and the maximum is 1.0.\n  optional: true\n- name: optimization_objective_precision_value\n  type: Float\n  description: |-\n    Optional. Required when maximize-recall-at-precision optimizationObjective was\n    picked, represents the precision value at which the optimization is\n    done.\n\n    The minimum value is 0 and the maximum is 1.0.\n  optional: true\n- {name: project, type: String, optional: true}\n- {name: location, type: String, default: us-central1, optional: true}\n- {name: encryption_spec_key_name, type: String, optional: true}\noutputs:\n- {name: model_name, type: GoogleCloudVertexAiModelName}\n- {name: model_dict, type: JsonObject}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'google-cloud-aiplatform==1.6.2' 'google-api-python-client==2.29.0' || PIP_DISABLE_PIP_VERSION_CHECK=1\n      python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.6.2'\n      'google-api-python-client==2.29.0' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def train_tabular_model_using_Google_Cloud_Vertex_AI_AutoML(\n          # AutoMLTabularTrainingJob.run required parameters\n          dataset_name,\n          target_column,\n\n          # AutoMLTabularTrainingJob.__init__ required parameters\n          # display_name: str,\n          optimization_prediction_type,\n\n          # AutoMLTabularTrainingJob.run parameters\n          training_fraction_split = 0.8,\n          validation_fraction_split = 0.1,\n          test_fraction_split = 0.1,\n          predefined_split_column_name = None,\n          weight_column = None,\n          budget_milli_node_hours = 1000,\n          model_display_name = None,\n          disable_early_stopping = False,\n\n          # AutoMLTabularTrainingJob.__init__ parameters\n          optimization_objective = None,\n          #column_transformations: Union[Dict, List[Dict], NoneType] = None,\n          optimization_objective_recall_value = None,\n          optimization_objective_precision_value = None,\n\n          project = None,\n          location = 'us-central1',\n          #training_encryption_spec_key_name: str = None,\n          #model_encryption_spec_key_name: str = None,\n          encryption_spec_key_name = None,\n      ):\n          '''Trains model using Google Cloud Vertex AI AutoML.\n\n          Data fraction splits:\n          Any of ``training_fraction_split``, ``validation_fraction_split`` and\n          ``test_fraction_split`` may optionally be provided, they must sum to up to 1. If\n          the provided ones sum to less than 1, the remainder is assigned to sets as\n          decided by Vertex AI. If none of the fractions are set, by default roughly 80%\n          of data will be used for training, 10% for validation, and 10% for test.\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n\n          Args:\n              dataset_name:\n                  Required. The full name of dataset  (datasets.TabularDataset) within the same Project from which data will be used to train the Model. The\n                  Dataset must use schema compatible with Model being trained,\n                  and what is compatible should be described in the used\n                  TrainingPipeline's [training_task_definition]\n                  [google.cloud.aiplatform.v1beta1.TrainingPipeline.training_task_definition].\n                  For tabular Datasets, all their data is exported to\n                  training, to pick and choose from.\n              target_column (str):\n                  Required. The name of the column values of which the Model is to predict.\n              training_fraction_split (float):\n                  Required. The fraction of the input data that is to be\n                  used to train the Model. This is ignored if Dataset is not provided.\n              validation_fraction_split (float):\n                  Required. The fraction of the input data that is to be\n                  used to validate the Model. This is ignored if Dataset is not provided.\n              test_fraction_split (float):\n                  Required. The fraction of the input data that is to be\n                  used to evaluate the Model. This is ignored if Dataset is not provided.\n              predefined_split_column_name (str):\n                  Optional. The key is a name of one of the Dataset's data\n                  columns. The value of the key (either the label's value or\n                  value in the column) must be one of {``training``,\n                  ``validation``, ``test``}, and it defines to which set the\n                  given piece of data is assigned. If for a piece of data the\n                  key is not present or has an invalid value, that piece is\n                  ignored by the pipeline.\n\n                  Supported only for tabular and time series Datasets.\n              weight_column (str):\n                  Optional. Name of the column that should be used as the weight column.\n                  Higher values in this column give more importance to the row\n                  during Model training. The column must have numeric values between 0 and\n                  10000 inclusively, and 0 value means that the row is ignored.\n                  If the weight column field is not set, then all rows are assumed to have\n                  equal weight of 1.\n              budget_milli_node_hours (int):\n                  Optional. The train budget of creating this Model, expressed in milli node\n                  hours i.e. 1,000 value in this field means 1 node hour.\n                  The training cost of the model will not exceed this budget. The final\n                  cost will be attempted to be close to the budget, though may end up\n                  being (even) noticeably smaller - at the backend's discretion. This\n                  especially may happen when further model training ceases to provide\n                  any improvements.\n                  If the budget is set to a value known to be insufficient to train a\n                  Model for the given training set, the training won't be attempted and\n                  will error.\n                  The minimum value is 1000 and the maximum is 72000.\n              model_display_name (str):\n                  Optional. If the script produces a managed Vertex AI Model. The display name of\n                  the Model. The name can be up to 128 characters long and can be consist\n                  of any UTF-8 characters.\n\n                  If not provided upon creation, the job's display_name is used.\n              disable_early_stopping (bool):\n                  Required. If true, the entire budget is used. This disables the early stopping\n                  feature. By default, the early stopping feature is enabled, which means\n                  that training might stop before the entire training budget has been\n                  used, if further training does no longer brings significant improvement\n                  to the model.\n\n              optimization_prediction_type (str):\n                  The type of prediction the Model is to produce.\n                  \"classification\" - Predict one out of multiple target values is\n                  picked for each row.\n                  \"regression\" - Predict a value based on its relation to other values.\n                  This type is available only to columns that contain\n                  semantically numeric values, i.e. integers or floating\n                  point number, even if stored as e.g. strings.\n              optimization_objective (str):\n                  Optional. Objective function the Model is to be optimized towards. The training\n                  task creates a Model that maximizes/minimizes the value of the objective\n                  function over the validation set.\n\n                  The supported optimization objectives depend on the prediction type, and\n                  in the case of classification also the number of distinct values in the\n                  target column (two distint values -> binary, 3 or more distinct values\n                  -> multi class).\n                  If the field is not set, the default objective function is used.\n\n                  Classification (binary):\n                  \"maximize-au-roc\" (default) - Maximize the area under the receiver\n                                              operating characteristic (ROC) curve.\n                  \"minimize-log-loss\" - Minimize log loss.\n                  \"maximize-au-prc\" - Maximize the area under the precision-recall curve.\n                  \"maximize-precision-at-recall\" - Maximize precision for a specified\n                                                  recall value.\n                  \"maximize-recall-at-precision\" - Maximize recall for a specified\n                                                  precision value.\n\n                  Classification (multi class):\n                  \"minimize-log-loss\" (default) - Minimize log loss.\n\n                  Regression:\n                  \"minimize-rmse\" (default) - Minimize root-mean-squared error (RMSE).\n                  \"minimize-mae\" - Minimize mean-absolute error (MAE).\n                  \"minimize-rmsle\" - Minimize root-mean-squared log error (RMSLE).\n              column_transformations (Optional[Union[Dict, List[Dict]]]):\n                  Optional. Transformations to apply to the input columns (i.e. columns other\n                  than the targetColumn). Each transformation may produce multiple\n                  result values from the column's value, and all are used for training.\n                  When creating transformation for BigQuery Struct column, the column\n                  should be flattened using \".\" as the delimiter.\n                  If an input column has no transformations on it, such a column is\n                  ignored by the training, except for the targetColumn, which should have\n                  no transformations defined on.\n              optimization_objective_recall_value (float):\n                  Optional. Required when maximize-precision-at-recall optimizationObjective was\n                  picked, represents the recall value at which the optimization is done.\n\n                  The minimum value is 0 and the maximum is 1.0.\n              optimization_objective_precision_value (float):\n                  Optional. Required when maximize-recall-at-precision optimizationObjective was\n                  picked, represents the precision value at which the optimization is\n                  done.\n\n                  The minimum value is 0 and the maximum is 1.0.\n\n          Returns:\n              model_name: Model name (fully-qualified)\n              model_dict: Model metadata in JSON format\n          '''\n\n          import datetime\n          import logging\n          import os\n\n          from google.cloud import aiplatform\n          from google.protobuf import json_format\n\n          logging.getLogger().setLevel(logging.INFO)\n\n          if not model_display_name:\n              model_display_name = 'TablesModel_' + datetime.datetime.utcnow().strftime(\"%Y_%m_%d_%H_%M_%S\")\n\n          # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.\n          # This leads to failure when trying to create any resource in the project.\n          # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).\n          # We can try and get the GCP project ID/number from the environment variables.\n          if not project:\n              project_number = os.environ.get(\"CLOUD_ML_PROJECT_ID\")\n              if project_number:\n                  print(f\"Inferred project number: {project_number}\")\n                  project = project_number\n                  # To improve the naming we try to convert the project number into the user project ID.\n                  try:\n                      from googleapiclient import discovery\n\n                      cloud_resource_manager_service = discovery.build(\n                          \"cloudresourcemanager\", \"v3\"\n                      )\n                      project_id = (\n                          cloud_resource_manager_service.projects()\n                          .get(name=f\"projects/{project_number}\")\n                          .execute()[\"projectId\"]\n                      )\n                      if project_id:\n                          print(f\"Inferred project ID: {project_id}\")\n                          project = project_id\n                  except Exception as e:\n                      print(e)\n\n          aiplatform.init(\n              project=project,\n              location=location,\n              encryption_spec_key_name=encryption_spec_key_name,\n          )\n\n          model = aiplatform.AutoMLTabularTrainingJob(\n              display_name='AutoMLTabularTrainingJob_' + datetime.datetime.utcnow().strftime(\"%Y_%m_%d_%H_%M_%S\"),\n              optimization_prediction_type=optimization_prediction_type,\n              optimization_objective=optimization_objective,\n              #column_transformations=column_transformations,\n              optimization_objective_recall_value=optimization_objective_recall_value,\n              optimization_objective_precision_value=optimization_objective_precision_value,\n          ).run(\n              dataset=aiplatform.TabularDataset(dataset_name=dataset_name),\n              target_column=target_column,\n              training_fraction_split=training_fraction_split,\n              validation_fraction_split=validation_fraction_split,\n              test_fraction_split=test_fraction_split,\n              predefined_split_column_name=predefined_split_column_name,\n              weight_column=weight_column,\n              budget_milli_node_hours=budget_milli_node_hours,\n              model_display_name=model_display_name,\n              disable_early_stopping=disable_early_stopping,\n          )\n\n          (_, model_project, _, model_location, _, model_id) = model.resource_name.split('/')\n          model_web_url = f'https://console.cloud.google.com/vertex-ai/locations/{model_location}/models/{model_id}/evaluate?project={model_project}'\n          logging.info(f'Created model {model.name}.')\n          logging.info(f'Link: {model_web_url}')\n          model_json = json_format.MessageToJson(model._gca_resource._pb)\n          print(model_json)\n          return (model.resource_name, model_json, model_web_url)\n\n      def _deserialize_bool(s) -> bool:\n          from distutils.util import strtobool\n          return strtobool(s) == 1\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Train tabular model using Google Cloud Vertex AI AutoML', description='Trains model using Google Cloud Vertex AI AutoML.')\n      _parser.add_argument(\"--dataset-name\", dest=\"dataset_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--target-column\", dest=\"target_column\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--optimization-prediction-type\", dest=\"optimization_prediction_type\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--training-fraction-split\", dest=\"training_fraction_split\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--validation-fraction-split\", dest=\"validation_fraction_split\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--test-fraction-split\", dest=\"test_fraction_split\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--predefined-split-column-name\", dest=\"predefined_split_column_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--weight-column\", dest=\"weight_column\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--budget-milli-node-hours\", dest=\"budget_milli_node_hours\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--model-display-name\", dest=\"model_display_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--disable-early-stopping\", dest=\"disable_early_stopping\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--optimization-objective\", dest=\"optimization_objective\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--optimization-objective-recall-value\", dest=\"optimization_objective_recall_value\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--optimization-objective-precision-value\", dest=\"optimization_objective_precision_value\", type=float, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--project\", dest=\"project\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--location\", dest=\"location\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--encryption-spec-key-name\", dest=\"encryption_spec_key_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = train_tabular_model_using_Google_Cloud_Vertex_AI_AutoML(**_parsed_args)\n\n      _output_serializers = [\n          str,\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --dataset-name\n    - {inputValue: dataset_name}\n    - --target-column\n    - {inputValue: target_column}\n    - --optimization-prediction-type\n    - {inputValue: optimization_prediction_type}\n    - if:\n        cond: {isPresent: training_fraction_split}\n        then:\n        - --training-fraction-split\n        - {inputValue: training_fraction_split}\n    - if:\n        cond: {isPresent: validation_fraction_split}\n        then:\n        - --validation-fraction-split\n        - {inputValue: validation_fraction_split}\n    - if:\n        cond: {isPresent: test_fraction_split}\n        then:\n        - --test-fraction-split\n        - {inputValue: test_fraction_split}\n    - if:\n        cond: {isPresent: predefined_split_column_name}\n        then:\n        - --predefined-split-column-name\n        - {inputValue: predefined_split_column_name}\n    - if:\n        cond: {isPresent: weight_column}\n        then:\n        - --weight-column\n        - {inputValue: weight_column}\n    - if:\n        cond: {isPresent: budget_milli_node_hours}\n        then:\n        - --budget-milli-node-hours\n        - {inputValue: budget_milli_node_hours}\n    - if:\n        cond: {isPresent: model_display_name}\n        then:\n        - --model-display-name\n        - {inputValue: model_display_name}\n    - if:\n        cond: {isPresent: disable_early_stopping}\n        then:\n        - --disable-early-stopping\n        - {inputValue: disable_early_stopping}\n    - if:\n        cond: {isPresent: optimization_objective}\n        then:\n        - --optimization-objective\n        - {inputValue: optimization_objective}\n    - if:\n        cond: {isPresent: optimization_objective_recall_value}\n        then:\n        - --optimization-objective-recall-value\n        - {inputValue: optimization_objective_recall_value}\n    - if:\n        cond: {isPresent: optimization_objective_precision_value}\n        then:\n        - --optimization-objective-precision-value\n        - {inputValue: optimization_objective_precision_value}\n    - if:\n        cond: {isPresent: project}\n        then:\n        - --project\n        - {inputValue: project}\n    - if:\n        cond: {isPresent: location}\n        then:\n        - --location\n        - {inputValue: location}\n    - if:\n        cond: {isPresent: encryption_spec_key_name}\n        then:\n        - --encryption-spec-key-name\n        - {inputValue: encryption_spec_key_name}\n    - '----output-paths'\n    - {outputPath: model_name}\n    - {outputPath: model_dict}\n",
                    "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/ab85ecc9c30d4d68a2993ca87861f5e531a4f41b/components/google-cloud/Vertex_AI/AutoML/Tables/Train_model/component.yaml"
                  },
                  {
                    "text": "name: Get model tuning trials for Google Cloud Vertex AI AutoML Tables\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/AutoML/Tables/Get_model_tuning_trials/component.yaml'}\ninputs:\n- {name: model_name, type: GoogleCloudVertexAiModelName}\noutputs:\n- {name: tuning_trials, type: JsonArray}\n- {name: model_structures, type: JsonArray}\n- {name: extra_entries, type: JsonArray}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'google-cloud-logging==2.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m\n      pip install --quiet --no-warn-script-location 'google-cloud-logging==2.7.0'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def get_model_tuning_trials_for_Google_Cloud_Vertex_AI_AutoML_Tables(\n          model_name,\n      ):\n          import json\n          from google.cloud import logging as cloud_logging\n\n          (_, project, _, location, _, model_id) = model_name.split(\"/\")\n\n          # Need to specify project when initializing client.\n          # Otherwise we'll get error when running on Vertex AI Pipelines:\n          # google.api_core.exceptions.PermissionDenied: 403 The caller does not have permission\n          cloud_logging_client = cloud_logging.Client(project=project)\n\n          # Full filter:\n          # resource.type=\"cloudml_job\" resource.labels.job_id=\"{job_id}\" resource.labels.project_id=\"{project_id}\" labels.log_type=\"automl_tables\" jsonPayload.\"@type\"=\"type.googleapis.com/google.cloud.automl.master.TuningTrial\"\n          log_filter=f'resource.labels.job_id=\"{model_id}\"'\n          log_entry_list = list(cloud_logging_client.list_entries(filter_=log_filter))\n\n          tuning_trials = []\n          model_structures = []\n          extra_entries = []\n          for entry in log_entry_list:\n              if entry.payload.get(\"@type\") == \"type.googleapis.com/google.cloud.automl.master.TuningTrial\":\n                  tuning_trials.append(entry.payload)\n              elif entry.payload.get(\"@type\") == \"type.googleapis.com/google.cloud.automl.master.TablesModelStructure\":\n                  model_structures.append(entry.payload)\n              else:\n                  extra_entries.append(entry.payload)\n\n          # Manually serializing the results for pretty and stable output\n          print(\"Tuning trials:\")\n          tuning_trials_json = json.dumps(tuning_trials, sort_keys=True, indent=2)\n          print(tuning_trials_json)\n\n          print(\"Model structures:\")\n          model_structures_json = json.dumps(model_structures, sort_keys=True, indent=2)\n          print(model_structures_json)\n\n          print(\"Extra entries:\")\n          extra_entries_json = json.dumps(extra_entries, sort_keys=True, indent=2)\n          print(extra_entries_json)\n\n          return (tuning_trials_json, model_structures_json, extra_entries_json)\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Get model tuning trials for Google Cloud Vertex AI AutoML Tables', description='')\n      _parser.add_argument(\"--model-name\", dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=3)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = get_model_tuning_trials_for_Google_Cloud_Vertex_AI_AutoML_Tables(**_parsed_args)\n\n      _output_serializers = [\n          _serialize_json,\n          _serialize_json,\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --model-name\n    - {inputValue: model_name}\n    - '----output-paths'\n    - {outputPath: tuning_trials}\n    - {outputPath: model_structures}\n    - {outputPath: extra_entries}\n",
                    "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/a31b7c9652646f2cd035a0b3a23e0723c632521b/components/google-cloud/Vertex_AI/AutoML/Tables/Get_model_tuning_trials/component.yaml"
                  }
                ],
                "name": "AutoML"
              },
              {
                "components": [
                  {
                    "text": "name: Upload XGBoost model to Google Cloud Vertex AI\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Upload_XGBoost_model/component.yaml'}\ninputs:\n- {name: model, type: XGBoostModel}\n- {name: xgboost_version, type: String, optional: true}\n- {name: display_name, type: String, optional: true}\n- {name: description, type: String, optional: true}\n- {name: project, type: String, optional: true}\n- {name: location, type: String, default: us-central1, optional: true}\n- {name: labels, type: JsonObject, optional: true}\n- {name: staging_bucket, type: String, optional: true}\noutputs:\n- {name: model_name, type: GoogleCloudVertexAiModelName}\n- {name: model_dict, type: JsonObject}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'git+https://github.com/Ark-kun/python-aiplatform@8f61efb3a7903a6e0ef47d957f26ef3083581c7e#egg=google-cloud-aiplatform&subdirectory=.'\n      'google-api-python-client==2.29.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'git+https://github.com/Ark-kun/python-aiplatform@8f61efb3a7903a6e0ef47d957f26ef3083581c7e#egg=google-cloud-aiplatform&subdirectory=.'\n      'google-api-python-client==2.29.0' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def upload_XGBoost_model_to_Google_Cloud_Vertex_AI(\n          model_path,\n          xgboost_version = None,\n\n          display_name = None,\n          description = None,\n\n          # Uncomment when anyone requests these:\n          # instance_schema_uri: str = None,\n          # parameters_schema_uri: str = None,\n          # prediction_schema_uri: str = None,\n          # explanation_metadata: \"google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata\" = None,\n          # explanation_parameters: \"google.cloud.aiplatform_v1.types.explanation.ExplanationParameters\" = None,\n\n          project = None,\n          location = \"us-central1\",\n          labels = None,\n          # encryption_spec_key_name: str = None,\n          staging_bucket = None,\n      ):\n          kwargs = locals()\n          kwargs.pop(\"model_path\")\n\n          import json\n          import os\n          from google.cloud import aiplatform\n\n          # Problem: Unlike KFP, when running on Vertex AI, google.auth.default() returns incorrect GCP project ID.\n          # This leads to failure when trying to create any resource in the project.\n          # google.api_core.exceptions.PermissionDenied: 403 Permission 'aiplatform.models.upload' denied on resource '//aiplatform.googleapis.com/projects/gbd40bc90c7804989-tp/locations/us-central1' (or it may not exist).\n          # We can try and get the GCP project ID/number from the environment variables.\n          if not project:\n              project_number = os.environ.get(\"CLOUD_ML_PROJECT_ID\")\n              if project_number:\n                  print(f\"Inferred project number: {project_number}\")\n                  kwargs[\"project\"] = project_number\n                  # To improve the naming we try to convert the project number into the user project ID.\n                  try:\n                      from googleapiclient import discovery\n\n                      cloud_resource_manager_service = discovery.build(\n                          \"cloudresourcemanager\", \"v3\"\n                      )\n                      project_id = (\n                          cloud_resource_manager_service.projects()\n                          .get(name=f\"projects/{project_number}\")\n                          .execute()[\"projectId\"]\n                      )\n                      if project_id:\n                          print(f\"Inferred project ID: {project_id}\")\n                          kwargs[\"project\"] = project_id\n                  except Exception as e:\n                      print(e)\n\n          if not location:\n              kwargs[\"location\"] = os.environ.get(\"CLOUD_ML_REGION\")\n\n          if not labels:\n              kwargs[\"labels\"] = {}\n          kwargs[\"labels\"][\"component-source\"] = \"github-com-ark-kun-pipeline-components\"\n\n          model = aiplatform.Model.upload_xgboost_model_file(\n              model_file_path=model_path,\n              **kwargs,\n          )\n          model_json = json.dumps(model.to_dict(), indent=2)\n          print(model_json)\n          return (model.resource_name, model_json)\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Upload XGBoost model to Google Cloud Vertex AI', description='')\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--xgboost-version\", dest=\"xgboost_version\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--display-name\", dest=\"display_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--description\", dest=\"description\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--project\", dest=\"project\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--location\", dest=\"location\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--labels\", dest=\"labels\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--staging-bucket\", dest=\"staging_bucket\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = upload_XGBoost_model_to_Google_Cloud_Vertex_AI(**_parsed_args)\n\n      _output_serializers = [\n          str,\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: xgboost_version}\n        then:\n        - --xgboost-version\n        - {inputValue: xgboost_version}\n    - if:\n        cond: {isPresent: display_name}\n        then:\n        - --display-name\n        - {inputValue: display_name}\n    - if:\n        cond: {isPresent: description}\n        then:\n        - --description\n        - {inputValue: description}\n    - if:\n        cond: {isPresent: project}\n        then:\n        - --project\n        - {inputValue: project}\n    - if:\n        cond: {isPresent: location}\n        then:\n        - --location\n        - {inputValue: location}\n    - if:\n        cond: {isPresent: labels}\n        then:\n        - --labels\n        - {inputValue: labels}\n    - if:\n        cond: {isPresent: staging_bucket}\n        then:\n        - --staging-bucket\n        - {inputValue: staging_bucket}\n    - '----output-paths'\n    - {outputPath: model_name}\n    - {outputPath: model_dict}\n",
                    "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/2c24c0c0730c818b89f676c4dc5c9d6cb90ab01d/components/google-cloud/Vertex_AI/Models/Upload_XGBoost_model/component.yaml"
                  },
                  {
                    "text": "name: Upload Scikit learn pickle model to Google Cloud Vertex AI\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Upload_Scikit-learn_pickle_model/component.yaml'}\ninputs:\n- {name: model, type: ScikitLearnPickleModel}\n- {name: sklearn_version, type: String, optional: true}\n- {name: display_name, type: String, optional: true}\n- {name: description, type: String, optional: true}\n- {name: project, type: String, optional: true}\n- {name: location, type: String, default: us-central1, optional: true}\n- {name: labels, type: JsonObject, optional: true}\n- {name: staging_bucket, type: String, optional: true}\noutputs:\n- {name: model_name, type: GoogleCloudVertexAiModelName}\n- {name: model_dict, type: JsonObject}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'git+https://github.com/Ark-kun/python-aiplatform@9b50f62b9d1409644656fb3202edc7be19c722f4#egg=google-cloud-aiplatform&subdirectory=.'\n      || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'git+https://github.com/Ark-kun/python-aiplatform@9b50f62b9d1409644656fb3202edc7be19c722f4#egg=google-cloud-aiplatform&subdirectory=.'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def upload_Scikit_learn_pickle_model_to_Google_Cloud_Vertex_AI(\n          model_path,\n          sklearn_version = None,\n\n          display_name = None,\n          description = None,\n\n          # Uncomment when anyone requests these:\n          # instance_schema_uri: str = None,\n          # parameters_schema_uri: str = None,\n          # prediction_schema_uri: str = None,\n          # explanation_metadata: \"google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata\" = None,\n          # explanation_parameters: \"google.cloud.aiplatform_v1.types.explanation.ExplanationParameters\" = None,\n\n          project = None,\n          location = \"us-central1\",\n          labels = None,\n          # encryption_spec_key_name: str = None,\n          staging_bucket = None,\n      ):\n          import datetime\n          import json\n          import os\n          import shutil\n          import tempfile\n          from google.cloud import aiplatform\n\n          if not location:\n              location = os.environ.get(\"CLOUD_ML_REGION\")\n\n          if not labels:\n              labels = {}\n          labels[\"component-source\"] = \"github-com-ark-kun-pipeline-components\"\n\n          # The serving container decides the model type based on the model file extension.\n          # So we need to rename the mode file (e.g. /tmp/inputs/model/data) to *.pkl\n          _, renamed_model_path = tempfile.mkstemp(suffix=\".pkl\")\n          shutil.copyfile(src=model_path, dst=renamed_model_path)\n\n          display_name = display_name or \"Scikit-learn model \" + datetime.datetime.now().isoformat(sep=\" \")\n\n          model = aiplatform.Model.upload_scikit_learn_model_file(\n              model_file_path=renamed_model_path,\n              sklearn_version=sklearn_version,\n\n              display_name=display_name,\n              description=description,\n\n              # instance_schema_uri=instance_schema_uri,\n              # parameters_schema_uri=parameters_schema_uri,\n              # prediction_schema_uri=prediction_schema_uri,\n              # explanation_metadata=explanation_metadata,\n              # explanation_parameters=explanation_parameters,\n\n              project=project,\n              location=location,\n              labels=labels,\n              # encryption_spec_key_name=encryption_spec_key_name,\n              staging_bucket=staging_bucket,\n          )\n          model_json = json.dumps(model.to_dict(), indent=2)\n          print(model_json)\n          return (model.resource_name, model_json)\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Upload Scikit learn pickle model to Google Cloud Vertex AI', description='')\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--sklearn-version\", dest=\"sklearn_version\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--display-name\", dest=\"display_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--description\", dest=\"description\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--project\", dest=\"project\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--location\", dest=\"location\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--labels\", dest=\"labels\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--staging-bucket\", dest=\"staging_bucket\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = upload_Scikit_learn_pickle_model_to_Google_Cloud_Vertex_AI(**_parsed_args)\n\n      _output_serializers = [\n          str,\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: sklearn_version}\n        then:\n        - --sklearn-version\n        - {inputValue: sklearn_version}\n    - if:\n        cond: {isPresent: display_name}\n        then:\n        - --display-name\n        - {inputValue: display_name}\n    - if:\n        cond: {isPresent: description}\n        then:\n        - --description\n        - {inputValue: description}\n    - if:\n        cond: {isPresent: project}\n        then:\n        - --project\n        - {inputValue: project}\n    - if:\n        cond: {isPresent: location}\n        then:\n        - --location\n        - {inputValue: location}\n    - if:\n        cond: {isPresent: labels}\n        then:\n        - --labels\n        - {inputValue: labels}\n    - if:\n        cond: {isPresent: staging_bucket}\n        then:\n        - --staging-bucket\n        - {inputValue: staging_bucket}\n    - '----output-paths'\n    - {outputPath: model_name}\n    - {outputPath: model_dict}\n",
                    "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/25dc317e649a19a53139a08ccbe496a248693fe4/components/google-cloud/Vertex_AI/Models/Upload_Scikit-learn_pickle_model/component.yaml"
                  },
                  {
                    "text": "name: Upload Tensorflow model to Google Cloud Vertex AI\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Upload_Tensorflow_model/component.yaml'}\ninputs:\n- {name: model, type: TensorflowSavedModel}\n- {name: tensorflow_version, type: String, default: '2.7', optional: true}\n- name: use_gpu\n  type: Boolean\n  default: \"False\"\n  optional: true\n- {name: display_name, type: String, default: Tensorflow model, optional: true}\n- {name: description, type: String, optional: true}\n- {name: project, type: String, optional: true}\n- {name: location, type: String, default: us-central1, optional: true}\n- {name: labels, type: JsonObject, optional: true}\n- {name: staging_bucket, type: String, optional: true}\noutputs:\n- {name: model_name, type: GoogleCloudVertexAiModelName}\n- {name: model_dict, type: JsonObject}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'git+https://github.com/Ark-kun/python-aiplatform@1e1cbfef76b7c5c8db705d5c4c17c3691de7b032#egg=google-cloud-aiplatform&subdirectory=.'\n      || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'git+https://github.com/Ark-kun/python-aiplatform@1e1cbfef76b7c5c8db705d5c4c17c3691de7b032#egg=google-cloud-aiplatform&subdirectory=.'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def upload_Tensorflow_model_to_Google_Cloud_Vertex_AI(\n          model_path,\n          tensorflow_version = \"2.7\",  # TODO: Remove the explicit default once the upload_tensorflow_saved_model supports None\n          use_gpu = False,\n\n          display_name = \"Tensorflow model\",\n          description = None,\n\n          # Uncomment when anyone requests these:\n          # instance_schema_uri: str = None,\n          # parameters_schema_uri: str = None,\n          # prediction_schema_uri: str = None,\n          # explanation_metadata: \"google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata\" = None,\n          # explanation_parameters: \"google.cloud.aiplatform_v1.types.explanation.ExplanationParameters\" = None,\n\n          project = None,\n          location = \"us-central1\",\n          labels = None,\n          # encryption_spec_key_name: str = None,\n          staging_bucket = None,\n      ):\n          import json\n          import os\n          from google.cloud import aiplatform\n\n          if not location:\n              location = os.environ.get(\"CLOUD_ML_REGION\")\n\n          if not labels:\n              labels = {}\n          labels[\"component-source\"] = \"github-com-ark-kun-pipeline-components\"\n\n          model = aiplatform.Model.upload_tensorflow_saved_model(\n              saved_model_dir=model_path,\n              tensorflow_version=tensorflow_version,\n              use_gpu=use_gpu,\n\n              display_name=display_name,\n              description=description,\n\n              # instance_schema_uri=instance_schema_uri,\n              # parameters_schema_uri=parameters_schema_uri,\n              # prediction_schema_uri=prediction_schema_uri,\n              # explanation_metadata=explanation_metadata,\n              # explanation_parameters=explanation_parameters,\n\n              project=project,\n              location=location,\n              labels=labels,\n              # encryption_spec_key_name=encryption_spec_key_name,\n              staging_bucket=staging_bucket,\n          )\n          model_json = json.dumps(model.to_dict(), indent=2)\n          print(model_json)\n          return (model.resource_name, model_json)\n\n      def _deserialize_bool(s) -> bool:\n          from distutils.util import strtobool\n          return strtobool(s) == 1\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Upload Tensorflow model to Google Cloud Vertex AI', description='')\n      _parser.add_argument(\"--model\", dest=\"model_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--tensorflow-version\", dest=\"tensorflow_version\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--use-gpu\", dest=\"use_gpu\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--display-name\", dest=\"display_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--description\", dest=\"description\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--project\", dest=\"project\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--location\", dest=\"location\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--labels\", dest=\"labels\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--staging-bucket\", dest=\"staging_bucket\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = upload_Tensorflow_model_to_Google_Cloud_Vertex_AI(**_parsed_args)\n\n      _output_serializers = [\n          str,\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --model\n    - {inputPath: model}\n    - if:\n        cond: {isPresent: tensorflow_version}\n        then:\n        - --tensorflow-version\n        - {inputValue: tensorflow_version}\n    - if:\n        cond: {isPresent: use_gpu}\n        then:\n        - --use-gpu\n        - {inputValue: use_gpu}\n    - if:\n        cond: {isPresent: display_name}\n        then:\n        - --display-name\n        - {inputValue: display_name}\n    - if:\n        cond: {isPresent: description}\n        then:\n        - --description\n        - {inputValue: description}\n    - if:\n        cond: {isPresent: project}\n        then:\n        - --project\n        - {inputValue: project}\n    - if:\n        cond: {isPresent: location}\n        then:\n        - --location\n        - {inputValue: location}\n    - if:\n        cond: {isPresent: labels}\n        then:\n        - --labels\n        - {inputValue: labels}\n    - if:\n        cond: {isPresent: staging_bucket}\n        then:\n        - --staging-bucket\n        - {inputValue: staging_bucket}\n    - '----output-paths'\n    - {outputPath: model_name}\n    - {outputPath: model_dict}\n",
                    "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/719783ef44c04348ea23e247a93021d91cfe602d/components/google-cloud/Vertex_AI/Models/Upload_Tensorflow_model/component.yaml"
                  },
                  {
                    "text": "name: Upload PyTorch model archive to Google Cloud Vertex AI\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Upload_PyTorch_model_archive/component.yaml'}\ninputs:\n- {name: model_archive, type: PyTorchModelArchive}\n- {name: torchserve_version, type: String, default: 0.6.0, optional: true}\n- name: use_gpu\n  type: Boolean\n  default: \"False\"\n  optional: true\n- {name: display_name, type: String, optional: true}\n- {name: description, type: String, optional: true}\n- {name: project, type: String, optional: true}\n- {name: location, type: String, optional: true}\n- {name: labels, type: JsonObject, optional: true}\n- {name: staging_bucket, type: String, optional: true}\noutputs:\n- {name: model_name, type: GoogleCloudVertexAiModelName}\n- {name: model_dict, type: JsonObject}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'google-cloud-aiplatform==1.13.1' 'google-cloud-build==3.8.3' || PIP_DISABLE_PIP_VERSION_CHECK=1\n      python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.13.1'\n      'google-cloud-build==3.8.3' --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def upload_PyTorch_model_archive_to_Google_Cloud_Vertex_AI(\n          model_archive_path,\n          torchserve_version = \"0.6.0\",\n          use_gpu = False,\n\n          display_name = None,\n          description = None,\n\n          # Uncomment when anyone requests these:\n          # instance_schema_uri: str = None,\n          # parameters_schema_uri: str = None,\n          # prediction_schema_uri: str = None,\n          # explanation_metadata: \"google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata\" = None,\n          # explanation_parameters: \"google.cloud.aiplatform_v1.types.explanation.ExplanationParameters\" = None,\n\n          project = None,\n          location = None,\n          labels = None,\n          # encryption_spec_key_name: str = None,\n          staging_bucket = None,\n      ):\n          import json\n          import os\n          from google.cloud import aiplatform\n\n          if not location:\n              location = os.environ.get(\"CLOUD_ML_REGION\")\n\n          if not labels:\n              labels = {}\n          labels[\"component-source\"] = \"github-com-ark-kun-pipeline-components\"\n\n          container_image_tag = torchserve_version + \"-\" + (\"gpu\" if use_gpu else \"cpu\")\n          container_image_uri = f\"pytorch/torchserve:{container_image_tag}\"\n\n          # Vertex Endpoints refuse to support non-Google container registries.\n          # We have to work around this to reduce user frustration\n          # TODO: Remove this code when Vertex Endpoints service starts supporting other container registries.\n          def copy_container_image(\n              src_container_image_uri,\n              dst_container_image_uri,\n              project_id,\n          ):\n              from google.cloud.devtools import cloudbuild\n              from google import protobuf\n              build_client = cloudbuild.CloudBuildClient()\n              build_config = cloudbuild.Build(\n                  images=[dst_container_image_uri],\n                  steps=[\n                      cloudbuild.BuildStep(\n                          name=\"gcr.io/cloud-builders/docker\",\n                          entrypoint=\"bash\",\n                          args=[\n                              \"-exc\",\n                              'docker pull --quiet \"$0\" && docker tag \"$0\" \"$1\"',\n                              src_container_image_uri,\n                              dst_container_image_uri,\n                          ],\n                      ),\n                  ],\n                  timeout=protobuf.duration_pb2.Duration(\n                      seconds=1800,\n                  ),\n              )\n              build_operation = build_client.create_build(\n                  project_id=project_id,\n                  build=build_config,\n              )\n              try:\n                  result = build_operation.result()\n              except:\n                  print(f\"Logs are available at [{build_operation.metadata.build.log_url}].\")\n                  raise\n              return result\n\n          project_id = aiplatform.initializer.global_config.project\n          mirrored_container_uri = f\"gcr.io/{project_id}/container_mirror/{container_image_uri}\"\n          # FIX: Only mirror when image does not exist\n          # docker does is unable to get the registry data from inside container (it cannot connecto to docker socket):\n          # docker.errors.DockerException: Error while fetching server API version: ('Connection aborted.', FileNotFoundError(2, 'No such file or directory'))\n          # import docker\n          # try:\n          #     docker_client = docker.from_env()\n          #     docker_client.images.get_registry_data(mirrored_container_uri)\n          # except docker.errors.NotFound:\n          if True:\n              print(f\"Mirroring {container_image_uri} to {mirrored_container_uri}\")\n              copy_container_image(\n                  src_container_image_uri=container_image_uri,\n                  dst_container_image_uri=mirrored_container_uri,\n                  project_id=project_id,\n              )\n          container_image_uri = mirrored_container_uri\n          # End of container image mirroring code\n\n          model_archive_file_name = os.path.basename(model_archive_path)\n          model_archive_dir = os.path.dirname(model_archive_path)\n\n          model = aiplatform.Model.upload(\n              # FIX: Use public image or mirror the official image\n              #serving_container_image_uri=\"gcr.io/avolkov-31337/mirror/pytorch/torchserve\",\n              serving_container_image_uri=container_image_uri,\n              artifact_uri=model_archive_dir,\n              serving_container_command=[\n                  \"bash\",\n                  \"-exc\",\n                  '''\n      model_archive_uri=\"$0\"\n      #model_archive_local_path=$(mktemp --suffix \".mar\")\n      # For some reason the model must already be inside the model-store directory.\n      model_archive_local_path=./model-store/model.mar\n\n      # Downloading the model archive from GCS\n      # TODO: Fix gsutil bugs (requires project ID, has auth issues) and use gsutil instead.\n      # gsutil cp \"$model_archive_uri\" \"$model_archive_local_path\"\n      pip install google-cloud-storage\n      python -c '\n      import sys\n      from google.cloud import storage\n\n      model_archive_uri = sys.argv[1]\n      model_archive_local_path = sys.argv[2]\n\n      storage_client = storage.Client()\n      blob = storage.Blob.from_string(uri=model_archive_uri, client=storage_client)\n      blob.download_to_filename(filename=model_archive_local_path)\n      ' \"$model_archive_uri\" \"$model_archive_local_path\"\n\n      #Note: config.properties is owned by root. Our user is not root.\n      echo \"\n      service_envelope=json\n      # Needed for external access\n      inference_address=http://0.0.0.0:8080\n      management_address=http://0.0.0.0:8081\n      \" > config2.properties\n      torchserve --start --foreground --no-config-snapshots --models main-model=\"$model_archive_local_path\" --model-store ./model-store/ --ts-config config2.properties\n                  ''',\n                  \"$(AIP_STORAGE_URI)/\" + model_archive_file_name,\n              ],\n              serving_container_predict_route=\"/predictions/main-model\",\n              #serving_container_predict_route=\"/v1/models/main-model:predict\",\n              serving_container_health_route=\"/ping\",\n              serving_container_ports=[8080],\n\n              display_name=display_name,\n              description=description,\n\n              # instance_schema_uri=instance_schema_uri,\n              # parameters_schema_uri=parameters_schema_uri,\n              # prediction_schema_uri=prediction_schema_uri,\n              # explanation_metadata=explanation_metadata,\n              # explanation_parameters=explanation_parameters,\n\n              project=project,\n              location=location,\n              labels=labels,\n              # encryption_spec_key_name=encryption_spec_key_name,\n              staging_bucket=staging_bucket,\n          )\n          model_json = json.dumps(model.to_dict(), indent=2)\n          print(model_json)\n          return (model.resource_name, model_json)\n\n      def _deserialize_bool(s) -> bool:\n          from distutils.util import strtobool\n          return strtobool(s) == 1\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import json\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Upload PyTorch model archive to Google Cloud Vertex AI', description='')\n      _parser.add_argument(\"--model-archive\", dest=\"model_archive_path\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--torchserve-version\", dest=\"torchserve_version\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--use-gpu\", dest=\"use_gpu\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--display-name\", dest=\"display_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--description\", dest=\"description\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--project\", dest=\"project\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--location\", dest=\"location\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--labels\", dest=\"labels\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--staging-bucket\", dest=\"staging_bucket\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = upload_PyTorch_model_archive_to_Google_Cloud_Vertex_AI(**_parsed_args)\n\n      _output_serializers = [\n          str,\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --model-archive\n    - {inputPath: model_archive}\n    - if:\n        cond: {isPresent: torchserve_version}\n        then:\n        - --torchserve-version\n        - {inputValue: torchserve_version}\n    - if:\n        cond: {isPresent: use_gpu}\n        then:\n        - --use-gpu\n        - {inputValue: use_gpu}\n    - if:\n        cond: {isPresent: display_name}\n        then:\n        - --display-name\n        - {inputValue: display_name}\n    - if:\n        cond: {isPresent: description}\n        then:\n        - --description\n        - {inputValue: description}\n    - if:\n        cond: {isPresent: project}\n        then:\n        - --project\n        - {inputValue: project}\n    - if:\n        cond: {isPresent: location}\n        then:\n        - --location\n        - {inputValue: location}\n    - if:\n        cond: {isPresent: labels}\n        then:\n        - --labels\n        - {inputValue: labels}\n    - if:\n        cond: {isPresent: staging_bucket}\n        then:\n        - --staging-bucket\n        - {inputValue: staging_bucket}\n    - '----output-paths'\n    - {outputPath: model_name}\n    - {outputPath: model_dict}\n",
                    "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d1e7a3ccf8f8e0324e15922d6fd90d667fc5281b/components/google-cloud/Vertex_AI/Models/Upload_PyTorch_model_archive/component.yaml"
                  },
                  {
                    "text": "name: Deploy model to endpoint for Google Cloud Vertex AI Model\ndescription: Deploys Google Cloud Vertex AI Model to a Google Cloud Vertex AI Endpoint.\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml'}\ninputs:\n- {name: model_name, type: GoogleCloudVertexAiModelName, description: Full resource\n    name of a Google Cloud Vertex AI Model}\n- name: endpoint_name\n  type: GoogleCloudVertexAiEndpointName\n  description: |-\n    Optional. Full name of Google Cloud Vertex Endpoint. A new\n    endpoint is created if the name is not passed.\n  optional: true\n- name: machine_type\n  type: String\n  description: |-\n    The type of the machine. See the [list of machine types\n    supported for prediction\n    ](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).\n    Defaults to \"n1-standard-2\"\n  default: n1-standard-2\n  optional: true\n- name: min_replica_count\n  type: Integer\n  description: |-\n    Optional. The minimum number of machine replicas this deployed\n    model will be always deployed on. If traffic against it increases,\n    it may dynamically be deployed onto more replicas, and as traffic\n    decreases, some of these extra replicas may be freed.\n  default: '1'\n  optional: true\n- name: max_replica_count\n  type: Integer\n  description: |-\n    Optional. The maximum number of replicas this deployed model may\n    be deployed on when the traffic against it increases. If requested\n    value is too large, the deployment will error, but if deployment\n    succeeds then the ability to scale the model to that many replicas\n    is guaranteed (barring service outages). If traffic against the\n    deployed model increases beyond what its replicas at maximum may\n    handle, a portion of the traffic will be dropped. If this value\n    is not provided, the smaller value of min_replica_count or 1 will\n    be used.\n  default: '1'\n  optional: true\n- name: accelerator_type\n  type: String\n  description: |-\n    Optional. Hardware accelerator type. Must also set accelerator_count if used.\n    One of ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100,\n    NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4\n  optional: true\n- {name: accelerator_count, type: Integer, description: Optional. The number of accelerators\n    to attach to a worker replica., optional: true}\noutputs:\n- {name: endpoint_name, type: GoogleCloudVertexAiEndpointName}\n- {name: endpoint_dict, type: JsonObject}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'google-cloud-aiplatform==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.7.0'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def deploy_model_to_endpoint_for_Google_Cloud_Vertex_AI_Model(\n          model_name,\n          endpoint_name = None,\n          machine_type = \"n1-standard-2\",\n          min_replica_count = 1,\n          max_replica_count = 1,\n          accelerator_type = None,\n          accelerator_count = None,\n          #\n          # Uncomment when anyone requests these:\n          # deployed_model_display_name: str = None,\n          # traffic_percentage: int = 0,\n          # traffic_split: dict = None,\n          # service_account: str = None,\n          # explanation_metadata: \"google.cloud.aiplatform_v1.types.explanation_metadata.ExplanationMetadata\" = None,\n          # explanation_parameters: \"google.cloud.aiplatform_v1.types.explanation.ExplanationParameters\" = None,\n          #\n          # encryption_spec_key_name: str = None,\n      ):\n          \"\"\"Deploys Google Cloud Vertex AI Model to a Google Cloud Vertex AI Endpoint.\n\n          Args:\n              model_name: Full resource name of a Google Cloud Vertex AI Model\n              endpoint_name: Optional. Full name of Google Cloud Vertex Endpoint. A new\n                  endpoint is created if the name is not passed.\n              machine_type: The type of the machine. See the [list of machine types\n                  supported for prediction\n                  ](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#machine-types).\n                  Defaults to \"n1-standard-2\"\n              min_replica_count (int):\n                  Optional. The minimum number of machine replicas this deployed\n                  model will be always deployed on. If traffic against it increases,\n                  it may dynamically be deployed onto more replicas, and as traffic\n                  decreases, some of these extra replicas may be freed.\n              max_replica_count (int):\n                  Optional. The maximum number of replicas this deployed model may\n                  be deployed on when the traffic against it increases. If requested\n                  value is too large, the deployment will error, but if deployment\n                  succeeds then the ability to scale the model to that many replicas\n                  is guaranteed (barring service outages). If traffic against the\n                  deployed model increases beyond what its replicas at maximum may\n                  handle, a portion of the traffic will be dropped. If this value\n                  is not provided, the smaller value of min_replica_count or 1 will\n                  be used.\n              accelerator_type (str):\n                  Optional. Hardware accelerator type. Must also set accelerator_count if used.\n                  One of ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100,\n                  NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4\n              accelerator_count (int):\n                  Optional. The number of accelerators to attach to a worker replica.\n          \"\"\"\n          import json\n          from google.cloud import aiplatform\n\n          model = aiplatform.Model(model_name=model_name)\n\n          if endpoint_name:\n              endpoint = aiplatform.Endpoint(endpoint_name=endpoint_name)\n          else:\n              endpoint_display_name = model.display_name[:118] + \"_endpoint\"\n              endpoint = aiplatform.Endpoint.create(\n                  display_name=endpoint_display_name,\n                  project=model.project,\n                  location=model.location,\n                  # encryption_spec_key_name=encryption_spec_key_name,\n                  labels={\"component-source\": \"github-com-ark-kun-pipeline-components\"},\n              )\n\n          endpoint = model.deploy(\n              endpoint=endpoint,\n              # deployed_model_display_name=deployed_model_display_name,\n              machine_type=machine_type,\n              min_replica_count=min_replica_count,\n              max_replica_count=max_replica_count,\n              accelerator_type=accelerator_type,\n              accelerator_count=accelerator_count,\n              # service_account=service_account,\n              # explanation_metadata=explanation_metadata,\n              # explanation_parameters=explanation_parameters,\n              # encryption_spec_key_name=encryption_spec_key_name,\n          )\n\n          endpoint_json = json.dumps(endpoint.to_dict(), indent=2)\n          print(endpoint_json)\n          return (endpoint.resource_name, endpoint_json)\n\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Deploy model to endpoint for Google Cloud Vertex AI Model', description='Deploys Google Cloud Vertex AI Model to a Google Cloud Vertex AI Endpoint.')\n      _parser.add_argument(\"--model-name\", dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--endpoint-name\", dest=\"endpoint_name\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--machine-type\", dest=\"machine_type\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--min-replica-count\", dest=\"min_replica_count\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--max-replica-count\", dest=\"max_replica_count\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--accelerator-type\", dest=\"accelerator_type\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--accelerator-count\", dest=\"accelerator_count\", type=int, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = deploy_model_to_endpoint_for_Google_Cloud_Vertex_AI_Model(**_parsed_args)\n\n      _output_serializers = [\n          str,\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --model-name\n    - {inputValue: model_name}\n    - if:\n        cond: {isPresent: endpoint_name}\n        then:\n        - --endpoint-name\n        - {inputValue: endpoint_name}\n    - if:\n        cond: {isPresent: machine_type}\n        then:\n        - --machine-type\n        - {inputValue: machine_type}\n    - if:\n        cond: {isPresent: min_replica_count}\n        then:\n        - --min-replica-count\n        - {inputValue: min_replica_count}\n    - if:\n        cond: {isPresent: max_replica_count}\n        then:\n        - --max-replica-count\n        - {inputValue: max_replica_count}\n    - if:\n        cond: {isPresent: accelerator_type}\n        then:\n        - --accelerator-type\n        - {inputValue: accelerator_type}\n    - if:\n        cond: {isPresent: accelerator_count}\n        then:\n        - --accelerator-count\n        - {inputValue: accelerator_count}\n    - '----output-paths'\n    - {outputPath: endpoint_name}\n    - {outputPath: endpoint_dict}\n",
                    "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/b2cdd60fe93d609111729ef64e79a8b8a2713435/components/google-cloud/Vertex_AI/Models/Deploy_to_endpoint/component.yaml"
                  },
                  {
                    "text": "name: Export model to GCS for Google Cloud Vertex AI Model\nmetadata:\n  annotations: {author: Alexey Volkov <alexey.volkov@ark-kun.com>, canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Vertex_AI/Models/Export/to_GCS/component.yaml'}\ninputs:\n- {name: model_name, type: GoogleCloudVertexAiModelName}\n- {name: output_prefix_gcs_uri, type: String}\n- {name: export_format, type: String, optional: true}\noutputs:\n- {name: model_dir_uri, type: String}\nimplementation:\n  container:\n    image: python:3.9\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'google-cloud-aiplatform==1.6.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3\n      -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform==1.6.2'\n      --user) && \"$0\" \"$@\"\n    - sh\n    - -ec\n    - |\n      program_path=$(mktemp)\n      printf \"%s\" \"$0\" > \"$program_path\"\n      python3 -u \"$program_path\" \"$@\"\n    - |\n      def export_model_to_GCS_for_Google_Cloud_Vertex_AI_Model(\n          model_name,\n          output_prefix_gcs_uri,  # GoogleCloudStorageURI\n          export_format = None,\n      ):\n          # Choose output_prefix_gcs_uri properly to avoid the following error:\n          # google.api_core.exceptions.FailedPrecondition: 400 The Cloud Storage bucket of `gs://<output_prefix_gcs_uri>/model-7972079425934065664/tf-saved-model/2021-11-08T10:44:57.671790Z` is in location `us`.\n          # It must be in the same regional location as the service location `us-central1`.\n          from google.cloud import aiplatform\n\n          model = aiplatform.Model(model_name=model_name)\n\n          print(\"Available export formats:\")\n          print(model.supported_export_formats)\n          if not export_format:\n              export_format = list(model.supported_export_formats.keys())[0]\n              print(f\"Auto-selected export formats: {export_format}\")\n\n          result = model.export_model(\n              export_format_id=export_format,\n              artifact_destination=output_prefix_gcs_uri,\n          )\n\n          # == \"gs://<artifact_destination>/model-7972079425934065664/tf-saved-model/2021-11-08T00:54:18.367871Z\"\n          artifact_output_uri = result[\"artifactOutputUri\"]\n\n          return (artifact_output_uri,)\n\n      def _serialize_str(str_value: str) -> str:\n          if not isinstance(str_value, str):\n              raise TypeError('Value \"{}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n          return str_value\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Export model to GCS for Google Cloud Vertex AI Model', description='')\n      _parser.add_argument(\"--model-name\", dest=\"model_name\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--output-prefix-gcs-uri\", dest=\"output_prefix_gcs_uri\", type=str, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--export-format\", dest=\"export_format\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = export_model_to_GCS_for_Google_Cloud_Vertex_AI_Model(**_parsed_args)\n\n      _output_serializers = [\n          _serialize_str,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --model-name\n    - {inputValue: model_name}\n    - --output-prefix-gcs-uri\n    - {inputValue: output_prefix_gcs_uri}\n    - if:\n        cond: {isPresent: export_format}\n        then:\n        - --export-format\n        - {inputValue: export_format}\n    - '----output-paths'\n    - {outputPath: model_dir_uri}\n",
                    "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d45e011ad8b62b4fe36c12289a624e5e1573c68d/components/google-cloud/Vertex_AI/Models/Export/to_GCS/component.yaml"
                  }
                ],
                "name": "Models"
              }
            ],
            "name": "Vertex AI"
          },
          {
            "components": [
              {
                "text": "name: Download from GCS\ninputs:\n- {name: GCS path, type: URI}\noutputs:\n- {name: Data}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/download/component.yaml'\nimplementation:\n    container:\n        image: google/cloud-sdk\n        command:\n        - bash # Pattern comparison only works in Bash\n        - -ex\n        - -c\n        - |\n            if [ -n \"${GOOGLE_APPLICATION_CREDENTIALS}\" ]; then\n                gcloud auth activate-service-account --key-file=\"${GOOGLE_APPLICATION_CREDENTIALS}\"\n            fi\n\n            uri=\"$0\"\n            output_path=\"$1\"\n\n            # Checking whether the URI points to a single blob, a directory or a URI pattern\n            # URI points to a blob when that URI does not end with slash and listing that URI only yields the same URI\n            if [[ \"$uri\" != */ ]] && (gsutil ls \"$uri\" | grep --fixed-strings --line-regexp \"$uri\"); then\n                mkdir -p \"$(dirname \"$output_path\")\"\n                gsutil -m cp -r \"$uri\" \"$output_path\"\n            else\n                mkdir -p \"$output_path\" # When source path is a directory, gsutil requires the destination to also be a directory\n                gsutil -m rsync -r \"$uri\" \"$output_path\" # gsutil cp has different path handling than Linux cp. It always puts the source directory (name) inside the destination directory. gsutil rsync does not have that problem.\n            fi\n        - inputValue: GCS path\n        - outputPath: Data\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/download/component.yaml"
              },
              {
                "text": "name: Upload to GCS\ninputs:\n- {name: Data}\n- {name: GCS path, type: URI}\noutputs:\n- {name: GCS path, type: String}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/upload_to_explicit_uri/component.yaml'\nimplementation:\n    container:\n        image: google/cloud-sdk\n        command:\n        - sh\n        - -ex\n        - -c\n        - |\n            if [ -n \"${GOOGLE_APPLICATION_CREDENTIALS}\" ]; then\n                gcloud auth activate-service-account --key-file=\"${GOOGLE_APPLICATION_CREDENTIALS}\"\n            fi\n            gsutil cp -r \"$0\" \"$1\"\n            mkdir -p \"$(dirname \"$2\")\"\n            printf \"%s\" \"$1\" > \"$2\"\n        - inputPath: Data\n        - inputValue: GCS path\n        - outputPath: GCS path\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_explicit_uri/component.yaml"
              },
              {
                "text": "name: Upload to GCS with unique name\ndescription: Upload to GCS with unique URI suffix\ninputs:\n- {name: Data}\n- {name: GCS path prefix, type: URI}\noutputs:\n- {name: GCS path, type: String}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/upload_to_unique_uri/component.yaml'\nimplementation:\n    container:\n        image: google/cloud-sdk\n        command:\n        - sh\n        - -ex\n        - -c\n        - |\n            data_path=\"$0\"\n            url_prefix=\"$1\"\n            output_path=\"$2\"\n            random_string=$(< dev/urandom tr -dc A-Za-z0-9 | head -c 64)\n            uri=\"${url_prefix}${random_string}\"\n            if [ -n \"${GOOGLE_APPLICATION_CREDENTIALS}\" ]; then\n                gcloud auth activate-service-account --key-file=\"${GOOGLE_APPLICATION_CREDENTIALS}\"\n            fi\n            gsutil cp -r \"$data_path\" \"$uri\"\n            mkdir -p \"$(dirname \"$output_path\")\"\n            printf \"%s\" \"$uri\" > \"$output_path\"\n        - inputPath: Data\n        - {inputValue: GCS path prefix}\n        - outputPath: GCS path",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/6210648f30b2b3a8c01cc10be338da98300efb6b/components/google-cloud/storage/upload_to_unique_uri/component.yaml"
              },
              {
                "text": "name: List blobs\ninputs:\n- {name: GCS path, type: URI, description: 'GCS path for listing. For recursive listing use the \"gs://bucket/path/**\" syntax\".'}\noutputs:\n- {name: Paths}\nmetadata:\n    annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>\n        canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/storage/list/component.yaml'\n        volatile_component: 'true'\nimplementation:\n    container:\n        image: google/cloud-sdk\n        command:\n        - sh\n        - -ex\n        - -c\n        - |\n            if [ -n \"${GOOGLE_APPLICATION_CREDENTIALS}\" ]; then\n                gcloud auth activate-service-account --key-file=\"${GOOGLE_APPLICATION_CREDENTIALS}\"\n            fi\n            mkdir -p \"$(dirname \"$1\")\"\n            gsutil ls \"$0\" > \"$1\"\n        - inputValue: GCS path\n        - outputPath: Paths\n",
                "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/google-cloud/storage/list/component.yaml"
              }
            ],
            "name": "Storage"
          },
          {
            "folders": [
              {
                "components": [
                  {
                    "text": "name: Suggest parameter sets from measurements using gcp ai platform optimizer\ndescription: Suggests trials (parameter sets) to evaluate.\ninputs:\n- {name: parameter_specs, type: JsonArray, description: 'List of parameter specs.\n    See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#parameterspec'}\n- {name: metrics_for_parameter_sets, type: JsonArray, description: 'List of parameter\n    sets and evaluation metrics for them. Each list item contains \"parameters\" dict\n    and \"metrics\" dict. Example: {\"parameters\": {\"p1\": 1.1, \"p2\": 2.2}, \"metrics\":\n    {\"metric1\": 101, \"metric2\": 102} }'}\n- {name: suggestion_count, type: Integer, description: Number of suggestions to request.}\n- name: maximize\n  type: Boolean\n  description: Whether to miaximize or minimize when optimizing a single metric.Default\n    is to minimize. Ignored if metric_specs list is provided.\n  default: \"False\"\n  optional: true\n- {name: metric_specs, type: JsonArray, description: 'List of metric specs. See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#metricspec',\n  optional: true}\n- {name: gcp_project_id, type: String, optional: true}\n- {name: gcp_region, type: String, default: us-central1, optional: true}\noutputs:\n- {name: suggested_parameter_sets, type: JsonArray}\nmetadata:\n  annotations:\n    author: Alexey Volkov <alexey.volkov@ark-kun.com>\n    canonical_location: 'https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/google-cloud/Optimizer/Suggest_parameter_sets_based_on_measurements/component.yaml'\nimplementation:\n  container:\n    image: python:3.8\n    command:\n    - sh\n    - -c\n    - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'google-api-python-client==1.12.3' 'google-auth==1.21.3'\n      || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location\n      'google-api-python-client==1.12.3' 'google-auth==1.21.3'\n      --user) && \"$0\" \"$@\"\n    - python3\n    - -u\n    - -c\n    - |\n      def suggest_parameter_sets_from_measurements_using_gcp_ai_platform_optimizer(\n          parameter_specs,\n          metrics_for_parameter_sets,\n          suggestion_count,\n          maximize = False,\n          metric_specs = None,\n          gcp_project_id = None,\n          gcp_region = \"us-central1\",\n      ):\n          \"\"\"Suggests trials (parameter sets) to evaluate.\n          See https://cloud.google.com/ai-platform/optimizer/docs\n\n          Annotations:\n              author: Alexey Volkov <alexey.volkov@ark-kun.com>\n\n          Args:\n              parameter_specs: List of parameter specs. See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#parameterspec\n              metrics_for_parameter_sets: List of parameter sets and evaluation metrics for them. Each list item contains \"parameters\" dict and \"metrics\" dict. Example: {\"parameters\": {\"p1\": 1.1, \"p2\": 2.2}, \"metrics\": {\"metric1\": 101, \"metric2\": 102} }\n              maximize: Whether to miaximize or minimize when optimizing a single metric.Default is to minimize. Ignored if metric_specs list is provided.\n              metric_specs: List of metric specs. See https://cloud.google.com/ai-platform/optimizer/docs/reference/rest/v1/projects.locations.studies#metricspec\n              suggestion_count: Number of suggestions to request.\n\n              suggested_parameter_sets: List of parameter set dictionaries.\n          \"\"\"\n\n          import logging\n          import random\n          import time\n\n          import google.auth\n          from googleapiclient import discovery\n\n          logging.getLogger().setLevel(logging.INFO)\n\n          client_id = 'client1'\n\n          credentials, default_project_id = google.auth.default()\n\n          # Validating and inferring the arguments\n          if not gcp_project_id:\n              gcp_project_id = default_project_id\n\n          # Building the API client.\n          # The main API does not work, so we need to build from the published discovery document.\n          def create_caip_optimizer_client(project_id):\n              from googleapiclient import discovery\n              # The discovery is broken. See https://github.com/googleapis/google-api-python-client/issues/1470\n              # return discovery.build(\"ml\", \"v1\")\n              return discovery.build(\"ml\", \"v1\", discoveryServiceUrl='https://storage.googleapis.com/caip-optimizer-public/api/ml_public_google_rest_v1.json')\n\n          # Workaround for the Optimizer bug: Optimizer returns resource names that use project number, but only supports resource names with project IDs when making requests\n          def get_project_number(project_id):\n              service = discovery.build('cloudresourcemanager', 'v1', credentials=credentials)\n              response = service.projects().get(projectId=project_id).execute()\n              return response['projectNumber']\n\n          gcp_project_number = get_project_number(gcp_project_id)\n\n          def fix_resource_name(name):\n              return name.replace(gcp_project_number, gcp_project_id)\n\n          ml_api = create_caip_optimizer_client(gcp_project_id)\n          studies_api = ml_api.projects().locations().studies()\n          trials_api = ml_api.projects().locations().studies().trials()\n          operations_api = ml_api.projects().locations().operations()\n\n          random_integer = random.SystemRandom().getrandbits(256)\n          study_id = '{:064x}'.format(random_integer)\n\n          if not metric_specs:\n              metric_specs=[{\n                  'metric': 'metric',\n                  'goal': 'MAXIMIZE' if maximize else 'MINIMIZE',\n              }]\n          study_config = {\n              'algorithm': 'ALGORITHM_UNSPECIFIED',  # Let the service choose the `default` algorithm.\n              'parameters': parameter_specs,\n              'metrics': metric_specs,\n          }\n          study = {'study_config': study_config}\n\n          logging.info(f'Creating temporary study {study_id}')\n          create_study_request = studies_api.create(\n              parent=f'projects/{gcp_project_id}/locations/{gcp_region}',\n              studyId=study_id,\n              body=study,\n          )\n          create_study_response = create_study_request.execute()\n          study_name = create_study_response['name']\n\n          paremeter_type_names = {parameter_spec['parameter']: parameter_spec['type'] for parameter_spec in parameter_specs}\n          def parameter_name_and_value_to_dict(parameter_name, parameter_value):\n              result = {'parameter': parameter_name}\n              paremeter_type_name = paremeter_type_names[parameter_name]\n              if paremeter_type_name in ['DOUBLE', 'DISCRETE']:\n                  result['floatValue'] = parameter_value\n              elif paremeter_type_name == 'INTEGER':\n                  result['intValue'] = parameter_value\n              elif paremeter_type_name == 'CATEGORICAL':\n                  result['stringValue'] = parameter_value\n              else:\n                  raise TypeError(f'Unsupported parameter type \"{paremeter_type_name}\"')\n              return result\n\n          try:\n              logging.info(f'Adding {len(metrics_for_parameter_sets)} measurements to the study.')\n              for parameters_and_metrics in metrics_for_parameter_sets:\n                  parameter_set = parameters_and_metrics['parameters']\n                  metrics_set = parameters_and_metrics['metrics']\n                  trial = {\n                      'parameters': [\n                          parameter_name_and_value_to_dict(parameter_name, parameter_value)\n                          for parameter_name, parameter_value in parameter_set.items()\n                      ],\n                      'finalMeasurement': {\n                          'metrics': [\n                              {\n                                  'metric': metric_name,\n                                  'value': metric_value,\n                              }\n                              for metric_name, metric_value in metrics_set.items()\n                          ],\n                      },\n                      'state': 'COMPLETED',\n                  }\n                  create_trial_response = trials_api.create(\n                      parent=fix_resource_name(study_name),\n                      body=trial,\n                  ).execute()\n                  trial_name = create_trial_response[\"name\"]\n                  logging.info(f'Added trial \"{trial_name}\" to the study.')\n\n              logging.info(f'Requesting suggestions.')\n              suggest_trials_request = trials_api.suggest(\n                  parent=fix_resource_name(study_name),\n                  body=dict(\n                      suggestionCount=suggestion_count,\n                      clientId=client_id,\n                  ),\n              )\n              suggest_trials_response = suggest_trials_request.execute()\n              operation_name = suggest_trials_response['name']\n              while True:\n                  get_operation_response = operations_api.get(\n                      name=fix_resource_name(operation_name),\n                  ).execute()\n                  # Knowledge: The \"done\" key is just missing until the result is available\n                  if get_operation_response.get('done'):\n                      break\n                  logging.info('Operation not finished yet: ' + str(get_operation_response))\n                  time.sleep(10)\n              operation_response = get_operation_response['response']\n              suggested_trials = operation_response['trials']\n\n              suggested_parameter_sets = [\n                  {\n                      parameter['parameter']: parameter.get('floatValue') or parameter.get('intValue') or parameter.get('stringValue') or 0.0\n                      for parameter in trial['parameters']\n                  }\n                  for trial in suggested_trials\n              ]\n              return (suggested_parameter_sets,)\n          finally:\n              logging.info(f'Deleting study: \"{study_name}\"')\n              studies_api.delete(name=fix_resource_name(study_name))\n\n      import json\n      def _serialize_json(obj) -> str:\n          if isinstance(obj, str):\n              return obj\n          import json\n          def default_serializer(obj):\n              if hasattr(obj, 'to_struct'):\n                  return obj.to_struct()\n              else:\n                  raise TypeError(\"Object of type '%s' is not JSON serializable and does not have .to_struct() method.\" % obj.__class__.__name__)\n          return json.dumps(obj, default=default_serializer, sort_keys=True)\n\n      def _deserialize_bool(s) -> bool:\n          from distutils.util import strtobool\n          return strtobool(s) == 1\n\n      import argparse\n      _parser = argparse.ArgumentParser(prog='Suggest parameter sets from measurements using gcp ai platform optimizer', description='Suggests trials (parameter sets) to evaluate.')\n      _parser.add_argument(\"--parameter-specs\", dest=\"parameter_specs\", type=json.loads, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--metrics-for-parameter-sets\", dest=\"metrics_for_parameter_sets\", type=json.loads, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--suggestion-count\", dest=\"suggestion_count\", type=int, required=True, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--maximize\", dest=\"maximize\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--metric-specs\", dest=\"metric_specs\", type=json.loads, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--gcp-project-id\", dest=\"gcp_project_id\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"--gcp-region\", dest=\"gcp_region\", type=str, required=False, default=argparse.SUPPRESS)\n      _parser.add_argument(\"----output-paths\", dest=\"_output_paths\", type=str, nargs=1)\n      _parsed_args = vars(_parser.parse_args())\n      _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n      _outputs = suggest_parameter_sets_from_measurements_using_gcp_ai_platform_optimizer(**_parsed_args)\n\n      _output_serializers = [\n          _serialize_json,\n\n      ]\n\n      import os\n      for idx, output_file in enumerate(_output_files):\n          try:\n              os.makedirs(os.path.dirname(output_file))\n          except OSError:\n              pass\n          with open(output_file, 'w') as f:\n              f.write(_output_serializers[idx](_outputs[idx]))\n    args:\n    - --parameter-specs\n    - {inputValue: parameter_specs}\n    - --metrics-for-parameter-sets\n    - {inputValue: metrics_for_parameter_sets}\n    - --suggestion-count\n    - {inputValue: suggestion_count}\n    - if:\n        cond: {isPresent: maximize}\n        then:\n        - --maximize\n        - {inputValue: maximize}\n    - if:\n        cond: {isPresent: metric_specs}\n        then:\n        - --metric-specs\n        - {inputValue: metric_specs}\n    - if:\n        cond: {isPresent: gcp_project_id}\n        then:\n        - --gcp-project-id\n        - {inputValue: gcp_project_id}\n    - if:\n        cond: {isPresent: gcp_region}\n        then:\n        - --gcp-region\n        - {inputValue: gcp_region}\n    - '----output-paths'\n    - {outputPath: suggested_parameter_sets}\n",
                    "url": "https://raw.githubusercontent.com/Ark-kun/pipeline_components/1b87c0bdfde5d7ec039401af8561783432731402/components/google-cloud/Optimizer/Suggest_parameter_sets_based_on_measurements/component.yaml"
                  }
                ],
                "name": "Optimizer"
              }
            ],
            "name": "AI Platform (legacy)"
          }
        ],
        "name": "Google Cloud"
      }
    ]
  }
}
